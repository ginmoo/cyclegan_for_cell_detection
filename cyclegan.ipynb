{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/nvidia/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/nvidia/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/nvidia/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/nvidia/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:522: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/nvidia/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/nvidia/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "import scipy\n",
    "import cv2\n",
    "from keras.datasets import mnist\n",
    "from keras_contrib.layers.normalization.instancenormalization import InstanceNormalization\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, Concatenate\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import numpy as np\n",
    "import scipy\n",
    "from glob import glob\n",
    "import os\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg19 import VGG19\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import hamming_loss\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0' #use GPU with ID=0\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.5 # maximun alloc gpu50% of MEM\n",
    "config.gpu_options.allow_growth = True #allocate dynamically\n",
    "sess = tf.Session(config = config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    def __init__(self, dataset_name, img_res=(320, 320)):\n",
    "        self.dataset_name = dataset_name\n",
    "        self.img_res = img_res\n",
    "\n",
    "    def load_data(self, domain, batch_size=1, is_testing=False):\n",
    "        data_type = \"train%s\" % domain if not is_testing else \"test%s\" % domain\n",
    "        if domain==\"A\":\n",
    "            path = glob('PatchImage/*')\n",
    "        if domain==\"B\":\n",
    "            path = glob('RealImage/*')\n",
    "\n",
    "        batch_images = np.random.choice(path, size=batch_size)\n",
    "\n",
    "        imgs = []\n",
    "        for img_path in batch_images:\n",
    "            img = self.imread(img_path)\n",
    "            if not is_testing:\n",
    "                img = scipy.misc.imresize(img, self.img_res)\n",
    "\n",
    "                if np.random.random() > 0.5:\n",
    "                    img = np.fliplr(img)\n",
    "            else:\n",
    "                img = scipy.misc.imresize(img, self.img_res)\n",
    "            imgs.append(img)\n",
    "\n",
    "        imgs = np.array(imgs)/127.5 - 1.\n",
    "\n",
    "        return imgs\n",
    "\n",
    "    def load_batch(self, batch_size=1, is_testing=False):\n",
    "        data_type = \"train\" if not is_testing else \"val\"\n",
    "        path_A = glob('PatchImage/*')\n",
    "        path_B = glob('RealImage/*')\n",
    "\n",
    "        self.n_batches = int(min(len(path_A), len(path_B)) / batch_size)\n",
    "        total_samples = self.n_batches * batch_size\n",
    "\n",
    "        # Sample n_batches * batch_size from each path list so that model sees all\n",
    "        # samples from both domains\n",
    "        path_A = np.random.choice(path_A, total_samples, replace=False)\n",
    "        path_B = np.random.choice(path_B, total_samples, replace=False)\n",
    "\n",
    "        for i in range(self.n_batches-1):\n",
    "            batch_A = path_A[i*batch_size:(i+1)*batch_size]\n",
    "            batch_B = path_B[i*batch_size:(i+1)*batch_size]\n",
    "            imgs_A, imgs_B = [], []\n",
    "            for img_A, img_B in zip(batch_A, batch_B):\n",
    "                img_A = self.imread(img_A)\n",
    "                img_B = self.imread(img_B)\n",
    "\n",
    "                img_A = scipy.misc.imresize(img_A, self.img_res)\n",
    "                img_B = scipy.misc.imresize(img_B, self.img_res)\n",
    "\n",
    "                if not is_testing and np.random.random() > 0.5:\n",
    "                        img_A = np.fliplr(img_A)\n",
    "                        img_B = np.fliplr(img_B)\n",
    "\n",
    "                imgs_A.append(img_A)\n",
    "                imgs_B.append(img_B)\n",
    "\n",
    "            imgs_A = np.array(imgs_A)/127.5 - 1.\n",
    "            imgs_B = np.array(imgs_B)/127.5 - 1.\n",
    "\n",
    "            yield imgs_A, imgs_B\n",
    "\n",
    "    def load_img(self, path):\n",
    "        img = self.imread(path)\n",
    "        img = scipy.misc.imresize(img, self.img_res)\n",
    "        img = img/127.5 - 1.\n",
    "        return img[np.newaxis, :, :, :]\n",
    "\n",
    "    def imread(self, path):\n",
    "        return scipy.misc.imread(path, mode='RGB').astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CycleGAN():\n",
    "    \n",
    "    def customLoss_a(self,gen_a):\n",
    "        \n",
    "        def lossFunction(y_true,y_pred):  \n",
    "            one_a = tf.ones_like(y_true)\n",
    "            zero_a = tf.zeros_like(y_true) \n",
    "            l_a = tf.where(y_true ==-1., x=zero_a, y=one_a)\n",
    "            \n",
    "            one_b = tf.ones_like(y_pred)\n",
    "            zero_b = tf.zeros_like(y_pred) \n",
    "            l_b = tf.where(y_pred ==-1., x=zero_b, y=one_b)\n",
    "            \n",
    "            loss_a = K.mean(K.abs(l_a * gen_a - y_true), axis=-1)\n",
    "            loss_b = K.mean(K.abs(l_b * gen_a - y_pred), axis=-1)\n",
    "            \n",
    "            loss_mae = K.mean(K.abs(y_pred - y_true), axis=-1)\n",
    "            \n",
    "            return 0.05*loss_a +0.9*loss_mae + 0.05*loss_b\n",
    "        \n",
    "        return lossFunction\n",
    "    \n",
    "    def customLoss_b(self,gen_b):\n",
    "        \n",
    "        def lossFunction(y_true,y_pred):  \n",
    "            one = tf.ones_like(gen_b)\n",
    "            zero = tf.zeros_like(gen_b) \n",
    "            l = tf.where(gen_b ==-1., x=zero, y=one)\n",
    "            \n",
    "            loss_a = K.mean(K.abs(l * y_true - gen_b), axis=-1)\n",
    "            loss_b = K.mean(K.abs(l * y_pred - gen_b), axis=-1)\n",
    "            loss_mae = K.mean(K.abs(y_pred - y_true), axis=-1)\n",
    "            \n",
    "            return 0.05*loss_a + 0.9*loss_mae + 0.05*loss_b\n",
    "        \n",
    "        return lossFunction \n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        # Input shape\n",
    "        self.img_rows = 320\n",
    "        self.img_cols = 320\n",
    "        self.channels = 3\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "\n",
    "        # Configure data loader\n",
    "        self.dataset_name = 'test'\n",
    "        self.data_loader = DataLoader(dataset_name=self.dataset_name,\n",
    "                                      img_res=(self.img_rows, self.img_cols))\n",
    "\n",
    "\n",
    "        # Calculate output shape of D (PatchGAN)\n",
    "        patch = int(self.img_rows / 2**4)\n",
    "        self.disc_patch = (patch, patch, 1)\n",
    "\n",
    "        # Number of filters in the first layer of G and D\n",
    "        self.gf = 16\n",
    "        self.df = 32\n",
    "\n",
    "        # Loss weights\n",
    "        self.lambda_cycle = 10.0                    # Cycle-consistency loss\n",
    "        self.lambda_id = 0.1 * self.lambda_cycle    # Identity loss\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.05)\n",
    "\n",
    "        # Build and compile the discriminators\n",
    "        self.d_p = self.build_discriminator()\n",
    "        self.d_r = self.build_discriminator()\n",
    "        self.d_p.compile(loss='mse',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])  \n",
    "        self.d_r.compile(loss='mse',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "        #-------------------------\n",
    "        # Construct Computational\n",
    "        #   Graph of Generators\n",
    "        #-------------------------\n",
    "\n",
    "        # Build the generators\n",
    "        self.g_AB = self.build_generator()\n",
    "        self.g_BA = self.build_generator()\n",
    "\n",
    "        # Input images from both domains\n",
    "        patch_A = Input(shape=self.img_shape)\n",
    "        real_B = Input(shape=self.img_shape)\n",
    "\n",
    "        # Translate images to the other domain\n",
    "        gen_A = self.g_AB(patch_A)\n",
    "        gen_B = self.g_BA(real_B)\n",
    "        # Translate images back to original domain\n",
    "        rebulid_A = self.g_BA(gen_A)\n",
    "        rebulid_B = self.g_AB(gen_B)\n",
    "        #Identity mapping of images\n",
    "        img_A_id = self.g_BA(patch_A)\n",
    "        img_B_id = self.g_AB(real_B)\n",
    "\n",
    "        # For the combined model we will only train the generators\n",
    "        self.d_r.trainable = False\n",
    "        self.d_p.trainable = False\n",
    "\n",
    "        # Discriminators determines validity of translated images\n",
    "        valid_A = self.d_r(gen_A)\n",
    "        valid_B = self.d_p(gen_B)\n",
    "        \n",
    "        # Combined model trains generators to fool discriminators\n",
    "        self.combined = Model(inputs=[patch_A, real_B],\n",
    "                              outputs=[ valid_A, valid_B,\n",
    "                                        rebulid_A, rebulid_B,\n",
    "                                        img_A_id, img_B_id\n",
    "                                      ])\n",
    "        self.combined.compile(loss=['mse', 'mse',\n",
    "                                    self.customLoss_a(gen_A), self.customLoss_b(gen_B),\n",
    "                                    'mae', 'mae'\n",
    "                                   ],\n",
    "                            loss_weights=[  1, 1,\n",
    "                                            self.lambda_cycle, self.lambda_cycle,\n",
    "                                            self.lambda_id, self.lambda_id \n",
    "                                            ],\n",
    "                            optimizer=optimizer)\n",
    "\n",
    "    def build_generator(self):\n",
    "        \"\"\"U-Net Generator\"\"\"\n",
    "\n",
    "        def conv2d(layer_input, filters, f_size=4):\n",
    "            \"\"\"Layers used during downsampling\"\"\"\n",
    "            d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
    "            d = LeakyReLU(alpha=0.2)(d)\n",
    "            d = InstanceNormalization()(d)\n",
    "            return d\n",
    "\n",
    "        def deconv2d(layer_input, skip_input, filters, f_size=4, dropout_rate=0):\n",
    "            \"\"\"Layers used during upsampling\"\"\"\n",
    "            u = UpSampling2D(size=2)(layer_input)\n",
    "            u = Conv2D(filters, kernel_size=f_size, strides=1, padding='same', activation='relu')(u)\n",
    "            if dropout_rate:\n",
    "                u = Dropout(dropout_rate)(u)\n",
    "            u = InstanceNormalization()(u)\n",
    "            u = Concatenate()([u, skip_input])\n",
    "            return u\n",
    "\n",
    "        # Image input\n",
    "        d0 = Input(shape=self.img_shape)\n",
    "\n",
    "        # Downsampling\n",
    "        d1 = conv2d(d0, self.gf)\n",
    "        d2 = conv2d(d1, self.gf*2)\n",
    "        d3 = conv2d(d2, self.gf*4)\n",
    "        d4 = conv2d(d3, self.gf*8)\n",
    "        d5 = conv2d(d4, self.gf*16)\n",
    "\n",
    "        # Upsampling\n",
    "        u1 = deconv2d(d5, d4, self.gf*4)\n",
    "        u2 = deconv2d(u1, d3, self.gf*4)\n",
    "        u3 = deconv2d(u2, d2, self.gf*2)\n",
    "        u4 = deconv2d(u3, d1, self.gf)\n",
    "\n",
    "        u5 = UpSampling2D(size=2)(u4)\n",
    "        output_img = Conv2D(self.channels, kernel_size=4, strides=1, padding='same', activation='tanh')(u5)\n",
    "\n",
    "        return Model(d0, output_img)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        def d_layer(layer_input, filters, f_size=4, normalization=True):\n",
    "            \"\"\"Discriminator layer\"\"\"\n",
    "            d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
    "            d = LeakyReLU(alpha=0.2)(d)\n",
    "            if normalization:\n",
    "                d = InstanceNormalization()(d)\n",
    "            return d\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "\n",
    "        d1 = d_layer(img, self.df, normalization=False)\n",
    "        d2 = d_layer(d1, self.df*2)\n",
    "        d3 = d_layer(d2, self.df*4)\n",
    "        d4 = d_layer(d3, self.df*8)\n",
    "\n",
    "        validity = Conv2D(1, kernel_size=4, strides=1, padding='same')(d4)\n",
    "\n",
    "        return Model(img, validity)\n",
    "\n",
    "    def train(self, epochs, batch_size=1, sample_interval=50):\n",
    "\n",
    "        start_time = datetime.datetime.now()\n",
    "\n",
    "        # Adversarial loss ground truths\n",
    "        valid = np.ones((batch_size,) + self.disc_patch)\n",
    "        fake = np.zeros((batch_size,) + self.disc_patch)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for batch_i, (patch_A, real_B) in enumerate(self.data_loader.load_batch(batch_size)):\n",
    "                \n",
    "                # ----------------------\n",
    "                #  Train Discriminators\n",
    "                # ----------------------\n",
    "\n",
    "                # Translate images to opposite domain\n",
    "                gen_A = self.g_AB.predict(patch_A)\n",
    "                gen_B = self.g_BA.predict(real_B)\n",
    "\n",
    "                # Train the discriminators (original images = real / translated = Fake)\n",
    "                dr_loss_real = self.d_r.train_on_batch(real_B, valid)\n",
    "                dr_loss_fake = self.d_r.train_on_batch(gen_A, fake)\n",
    "                dr_loss = 0.5 * np.add(dr_loss_real, dr_loss_fake)\n",
    "\n",
    "                dp_loss_real = self.d_p.train_on_batch(patch_A, valid)\n",
    "                dp_loss_fake = self.d_p.train_on_batch(gen_B, fake)\n",
    "                dp_loss = 0.5 * np.add(dp_loss_real, dp_loss_fake)\n",
    "\n",
    "                # Total disciminator loss\n",
    "                d_loss = 0.5 * np.add(dr_loss, dp_loss)\n",
    "\n",
    "\n",
    "                # ------------------\n",
    "                #  Train Generators\n",
    "                # ------------------\n",
    "\n",
    "                # Train the generators\n",
    "                g_loss = self.combined.train_on_batch([patch_A, real_B],\n",
    "                                                        [valid, valid,\n",
    "                                                        patch_A, real_B,\n",
    "                                                        patch_A, real_B\n",
    "                                                        ])\n",
    "\n",
    "                elapsed_time = datetime.datetime.now() - start_time\n",
    "\n",
    "                # Plot the progress\n",
    "                print (\"[Epoch %d/%d] [Batch %d/%d] [D loss: %f, acc: %3d%%] [G loss: %05f, adv: %05f, recon: %05f, id: %05f] time: %s \" \\\n",
    "                                                                        % ( epoch, epochs,\n",
    "                                                                            batch_i, self.data_loader.n_batches,\n",
    "                                                                            d_loss[0], 100*d_loss[1],\n",
    "                                                                            g_loss[0],\n",
    "                                                                            np.mean(g_loss[1:3]),\n",
    "                                                                            np.mean(g_loss[3:5]),\n",
    "                                                                            np.mean(g_loss[5:6]),\n",
    "                                                                            elapsed_time))\n",
    "\n",
    "                # If at save interval => save generated image samples\n",
    "                if batch_i % sample_interval == 0:\n",
    "                    self.sample_images(epoch, batch_i)\n",
    "                    self.g_AB.save(\"gen_model/%d_%d_model.h5\" % (epoch, batch_i))\n",
    "\n",
    "    def sample_images(self, epoch, batch_i):\n",
    "        r, c = 2, 3\n",
    "\n",
    "        patch_A = self.data_loader.load_data(domain=\"A\", batch_size=1, is_testing=True)\n",
    "        real_B = self.data_loader.load_data(domain=\"B\", batch_size=1, is_testing=True)\n",
    "\n",
    "        # Demo (for GIF)\n",
    "        #imgs_A = self.data_loader.load_img('datasets/apple2orange/testA/n07740461_1541.jpg')\n",
    "        #imgs_B = self.data_loader.load_img('datasets/apple2orange/testB/n07749192_4241.jpg')\n",
    "\n",
    "        # Translate images to the other domain\n",
    "        gen_A = self.g_AB.predict(patch_A)\n",
    "        gen_B = self.g_BA.predict(real_B)\n",
    "        # Translate back to original domain\n",
    "        rebulid_A = self.g_BA.predict(gen_A)\n",
    "        rebulid_B = self.g_AB.predict(gen_B)\n",
    "        \n",
    "        gen_imgs = np.concatenate([patch_A, gen_A, rebulid_A, real_B, gen_B, rebulid_B])\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        titles = ['Original', 'Generate', 'Rebuild']\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(gen_imgs[cnt])\n",
    "                axs[i, j].set_title(titles[j])\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        fig.savefig(\"saved_img/%d_%d.png\" % (epoch, batch_i))\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nvidia/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:74: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/home/nvidia/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:52: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "/home/nvidia/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:53: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "/home/nvidia/anaconda3/lib/python3.6/site-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/300] [Batch 0/292] [D loss: 10.378310, acc:   9%] [G loss: 67.073784, adv: 23.702950, recon: 0.912731, id: 0.933302] time: 0:00:24.216964 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nvidia/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:24: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/300] [Batch 1/292] [D loss: 5.951986, acc:  20%] [G loss: 33.002968, adv: 7.645895, recon: 0.795202, id: 1.242858] time: 0:00:26.124370 \n",
      "[Epoch 0/300] [Batch 2/292] [D loss: 11.773167, acc:  11%] [G loss: 31.958248, adv: 7.666613, recon: 0.746874, id: 1.185756] time: 0:00:26.309116 \n",
      "[Epoch 0/300] [Batch 3/292] [D loss: 3.389850, acc:  36%] [G loss: 21.997864, adv: 4.151436, recon: 0.601163, id: 1.116565] time: 0:00:26.471696 \n",
      "[Epoch 0/300] [Batch 4/292] [D loss: 1.124404, acc:  31%] [G loss: 16.159056, adv: 2.311414, recon: 0.494730, id: 1.117024] time: 0:00:26.639646 \n",
      "[Epoch 0/300] [Batch 5/292] [D loss: 0.768715, acc:  46%] [G loss: 14.526141, adv: 2.155033, recon: 0.434683, id: 0.889967] time: 0:00:26.806413 \n",
      "[Epoch 0/300] [Batch 6/292] [D loss: 0.521330, acc:  39%] [G loss: 16.762691, adv: 1.704906, recon: 0.587488, id: 1.058823] time: 0:00:26.989351 \n",
      "[Epoch 0/300] [Batch 7/292] [D loss: 0.530204, acc:  40%] [G loss: 15.570714, adv: 1.801103, recon: 0.527575, id: 0.920464] time: 0:00:27.192272 \n",
      "[Epoch 0/300] [Batch 8/292] [D loss: 0.533662, acc:  40%] [G loss: 15.083319, adv: 2.060865, recon: 0.481727, id: 0.896161] time: 0:00:27.428684 \n",
      "[Epoch 0/300] [Batch 9/292] [D loss: 0.719197, acc:  32%] [G loss: 16.991417, adv: 3.727268, recon: 0.409097, id: 0.880710] time: 0:00:27.655804 \n",
      "[Epoch 0/300] [Batch 10/292] [D loss: 0.755666, acc:  39%] [G loss: 14.485530, adv: 1.863493, recon: 0.476922, id: 0.821976] time: 0:00:27.853721 \n",
      "[Epoch 0/300] [Batch 11/292] [D loss: 0.723701, acc:  40%] [G loss: 18.961432, adv: 4.571689, recon: 0.442912, id: 0.649814] time: 0:00:28.047945 \n",
      "[Epoch 0/300] [Batch 12/292] [D loss: 0.887708, acc:  19%] [G loss: 23.752083, adv: 4.477853, recon: 0.666051, id: 1.056513] time: 0:00:28.252086 \n",
      "[Epoch 0/300] [Batch 13/292] [D loss: 1.395624, acc:  10%] [G loss: 23.788046, adv: 4.789368, recon: 0.639391, id: 0.986445] time: 0:00:28.471308 \n",
      "[Epoch 0/300] [Batch 14/292] [D loss: 2.138695, acc:  14%] [G loss: 23.737175, adv: 5.793651, recon: 0.542503, id: 0.973202] time: 0:00:28.655816 \n",
      "[Epoch 0/300] [Batch 15/292] [D loss: 2.619046, acc:  21%] [G loss: 14.448591, adv: 2.156763, recon: 0.446900, id: 0.869492] time: 0:00:28.844070 \n",
      "[Epoch 0/300] [Batch 16/292] [D loss: 1.014751, acc:  21%] [G loss: 12.065539, adv: 1.672507, recon: 0.381392, id: 0.733434] time: 0:00:29.040143 \n",
      "[Epoch 0/300] [Batch 17/292] [D loss: 1.176267, acc:  11%] [G loss: 12.379412, adv: 1.560394, recon: 0.409018, id: 0.633266] time: 0:00:29.241310 \n",
      "[Epoch 0/300] [Batch 18/292] [D loss: 0.644200, acc:  38%] [G loss: 10.439381, adv: 1.079335, recon: 0.359927, id: 0.795705] time: 0:00:29.411835 \n",
      "[Epoch 0/300] [Batch 19/292] [D loss: 0.400894, acc:  32%] [G loss: 10.000569, adv: 1.256844, recon: 0.326640, id: 0.653546] time: 0:00:29.619617 \n",
      "[Epoch 0/300] [Batch 20/292] [D loss: 0.423599, acc:  34%] [G loss: 10.110819, adv: 1.124489, recon: 0.339924, id: 0.627601] time: 0:00:29.837068 \n",
      "[Epoch 0/300] [Batch 21/292] [D loss: 0.663885, acc:  18%] [G loss: 10.075938, adv: 1.390344, recon: 0.320144, id: 0.528552] time: 0:00:30.044235 \n",
      "[Epoch 0/300] [Batch 22/292] [D loss: 0.493101, acc:  28%] [G loss: 8.798423, adv: 1.180043, recon: 0.278861, id: 0.576405] time: 0:00:30.226228 \n",
      "[Epoch 0/300] [Batch 23/292] [D loss: 0.528098, acc:  32%] [G loss: 9.201889, adv: 1.189247, recon: 0.296718, id: 0.554003] time: 0:00:30.424277 \n",
      "[Epoch 0/300] [Batch 24/292] [D loss: 0.768054, acc:  18%] [G loss: 9.227013, adv: 1.500237, recon: 0.272892, id: 0.456618] time: 0:00:30.633440 \n",
      "[Epoch 0/300] [Batch 25/292] [D loss: 0.563275, acc:  27%] [G loss: 9.258888, adv: 1.244802, recon: 0.301091, id: 0.457613] time: 0:00:30.829446 \n",
      "[Epoch 0/300] [Batch 26/292] [D loss: 0.594271, acc:  25%] [G loss: 9.711576, adv: 1.256012, recon: 0.318678, id: 0.483870] time: 0:00:31.027521 \n",
      "[Epoch 0/300] [Batch 27/292] [D loss: 0.607539, acc:  29%] [G loss: 10.260979, adv: 1.386639, recon: 0.335483, id: 0.377580] time: 0:00:31.228437 \n",
      "[Epoch 0/300] [Batch 28/292] [D loss: 0.467895, acc:  31%] [G loss: 10.490552, adv: 1.273880, recon: 0.344148, id: 0.704233] time: 0:00:31.388801 \n",
      "[Epoch 0/300] [Batch 29/292] [D loss: 0.582830, acc:  38%] [G loss: 9.098113, adv: 1.305200, recon: 0.272567, id: 0.593295] time: 0:00:31.567868 \n",
      "[Epoch 0/300] [Batch 30/292] [D loss: 0.660054, acc:  25%] [G loss: 10.871467, adv: 1.624577, recon: 0.342660, id: 0.452674] time: 0:00:31.761406 \n",
      "[Epoch 0/300] [Batch 31/292] [D loss: 0.301811, acc:  56%] [G loss: 10.112625, adv: 1.016418, recon: 0.345513, id: 0.616123] time: 0:00:31.970969 \n",
      "[Epoch 0/300] [Batch 32/292] [D loss: 0.223481, acc:  67%] [G loss: 9.845058, adv: 1.209190, recon: 0.324411, id: 0.431383] time: 0:00:32.194680 \n",
      "[Epoch 0/300] [Batch 33/292] [D loss: 0.579395, acc:  33%] [G loss: 10.005804, adv: 1.320261, recon: 0.323608, id: 0.352373] time: 0:00:32.410885 \n",
      "[Epoch 0/300] [Batch 34/292] [D loss: 0.666106, acc:  27%] [G loss: 9.114050, adv: 1.250298, recon: 0.291738, id: 0.312518] time: 0:00:32.598239 \n",
      "[Epoch 0/300] [Batch 35/292] [D loss: 0.649536, acc:  18%] [G loss: 8.776624, adv: 1.304263, recon: 0.278259, id: 0.289892] time: 0:00:32.770184 \n",
      "[Epoch 0/300] [Batch 36/292] [D loss: 0.510436, acc:  28%] [G loss: 9.482592, adv: 1.083008, recon: 0.326807, id: 0.319800] time: 0:00:32.957152 \n",
      "[Epoch 0/300] [Batch 37/292] [D loss: 0.506556, acc:  26%] [G loss: 8.627702, adv: 1.576561, recon: 0.246681, id: 0.284350] time: 0:00:33.129921 \n",
      "[Epoch 0/300] [Batch 38/292] [D loss: 0.665351, acc:  14%] [G loss: 11.201216, adv: 2.019010, recon: 0.328164, id: 0.322462] time: 0:00:33.307902 \n",
      "[Epoch 0/300] [Batch 39/292] [D loss: 0.741286, acc:  23%] [G loss: 9.136515, adv: 1.035408, recon: 0.309410, id: 0.480325] time: 0:00:33.471872 \n",
      "[Epoch 0/300] [Batch 40/292] [D loss: 0.542503, acc:  22%] [G loss: 8.552465, adv: 1.419841, recon: 0.259592, id: 0.283901] time: 0:00:33.664890 \n",
      "[Epoch 0/300] [Batch 41/292] [D loss: 0.474157, acc:  29%] [G loss: 8.250937, adv: 1.135806, recon: 0.272499, id: 0.280611] time: 0:00:33.845055 \n",
      "[Epoch 0/300] [Batch 42/292] [D loss: 0.492844, acc:  30%] [G loss: 8.779636, adv: 1.119428, recon: 0.290158, id: 0.305004] time: 0:00:34.023953 \n",
      "[Epoch 0/300] [Batch 43/292] [D loss: 0.364429, acc:  40%] [G loss: 8.104855, adv: 1.211977, recon: 0.257851, id: 0.271931] time: 0:00:34.212394 \n",
      "[Epoch 0/300] [Batch 44/292] [D loss: 0.646633, acc:  24%] [G loss: 9.191286, adv: 1.389582, recon: 0.289831, id: 0.232205] time: 0:00:34.422807 \n",
      "[Epoch 0/300] [Batch 45/292] [D loss: 0.595369, acc:  26%] [G loss: 8.246356, adv: 1.050275, recon: 0.273537, id: 0.281658] time: 0:00:34.649654 \n",
      "[Epoch 0/300] [Batch 46/292] [D loss: 0.479620, acc:  39%] [G loss: 8.340163, adv: 1.062656, recon: 0.274980, id: 0.251721] time: 0:00:34.857145 \n",
      "[Epoch 0/300] [Batch 47/292] [D loss: 0.435726, acc:  46%] [G loss: 8.551977, adv: 0.820265, recon: 0.306492, id: 0.274621] time: 0:00:35.080744 \n",
      "[Epoch 0/300] [Batch 48/292] [D loss: 0.347469, acc:  49%] [G loss: 8.144750, adv: 1.186282, recon: 0.252820, id: 0.287443] time: 0:00:35.286839 \n",
      "[Epoch 0/300] [Batch 49/292] [D loss: 0.677559, acc:  17%] [G loss: 8.290448, adv: 1.162180, recon: 0.272085, id: 0.285138] time: 0:00:35.469052 \n",
      "[Epoch 0/300] [Batch 50/292] [D loss: 0.621720, acc:  27%] [G loss: 7.601997, adv: 1.064579, recon: 0.247948, id: 0.266327] time: 0:00:35.667038 \n",
      "[Epoch 0/300] [Batch 51/292] [D loss: 0.454662, acc:  28%] [G loss: 8.413406, adv: 1.087903, recon: 0.284300, id: 0.239077] time: 0:00:35.865760 \n",
      "[Epoch 0/300] [Batch 52/292] [D loss: 0.494749, acc:  29%] [G loss: 7.423609, adv: 0.978681, recon: 0.245300, id: 0.234925] time: 0:00:36.054519 \n",
      "[Epoch 0/300] [Batch 53/292] [D loss: 0.407929, acc:  45%] [G loss: 7.266318, adv: 0.965066, recon: 0.234264, id: 0.227693] time: 0:00:36.263689 \n",
      "[Epoch 0/300] [Batch 54/292] [D loss: 0.420350, acc:  40%] [G loss: 8.016628, adv: 0.974629, recon: 0.271468, id: 0.227155] time: 0:00:36.475044 \n",
      "[Epoch 0/300] [Batch 55/292] [D loss: 0.602057, acc:  23%] [G loss: 7.018672, adv: 0.936863, recon: 0.230436, id: 0.250513] time: 0:00:36.687229 \n",
      "[Epoch 0/300] [Batch 56/292] [D loss: 0.466958, acc:  52%] [G loss: 7.041096, adv: 0.915500, recon: 0.225669, id: 0.227707] time: 0:00:36.889922 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/300] [Batch 57/292] [D loss: 0.448335, acc:  27%] [G loss: 6.978753, adv: 0.904620, recon: 0.235204, id: 0.234660] time: 0:00:37.105702 \n",
      "[Epoch 0/300] [Batch 58/292] [D loss: 0.436639, acc:  26%] [G loss: 6.918821, adv: 1.017983, recon: 0.219776, id: 0.240632] time: 0:00:37.342106 \n",
      "[Epoch 0/300] [Batch 59/292] [D loss: 0.398607, acc:  37%] [G loss: 7.018600, adv: 0.906778, recon: 0.229172, id: 0.233554] time: 0:00:37.526661 \n",
      "[Epoch 0/300] [Batch 60/292] [D loss: 0.482586, acc:  27%] [G loss: 6.252982, adv: 0.988965, recon: 0.190432, id: 0.254949] time: 0:00:37.739399 \n",
      "[Epoch 0/300] [Batch 61/292] [D loss: 0.453716, acc:  34%] [G loss: 6.575969, adv: 0.985309, recon: 0.203840, id: 0.228586] time: 0:00:37.952270 \n",
      "[Epoch 0/300] [Batch 62/292] [D loss: 0.452093, acc:  35%] [G loss: 6.146145, adv: 1.022521, recon: 0.184667, id: 0.203943] time: 0:00:38.168059 \n",
      "[Epoch 0/300] [Batch 63/292] [D loss: 0.368402, acc:  50%] [G loss: 7.072746, adv: 1.033762, recon: 0.220890, id: 0.193369] time: 0:00:38.365776 \n",
      "[Epoch 0/300] [Batch 64/292] [D loss: 0.397352, acc:  49%] [G loss: 7.211318, adv: 0.985291, recon: 0.234678, id: 0.203046] time: 0:00:38.590566 \n",
      "[Epoch 0/300] [Batch 65/292] [D loss: 0.406670, acc:  32%] [G loss: 7.027964, adv: 1.079788, recon: 0.213925, id: 0.220743] time: 0:00:38.793092 \n",
      "[Epoch 0/300] [Batch 66/292] [D loss: 0.516100, acc:  21%] [G loss: 6.500738, adv: 1.050788, recon: 0.198446, id: 0.211542] time: 0:00:39.026787 \n",
      "[Epoch 0/300] [Batch 67/292] [D loss: 0.484783, acc:  22%] [G loss: 6.311720, adv: 0.958867, recon: 0.199954, id: 0.181819] time: 0:00:39.244220 \n",
      "[Epoch 0/300] [Batch 68/292] [D loss: 0.426145, acc:  34%] [G loss: 6.408610, adv: 0.907989, recon: 0.205900, id: 0.221418] time: 0:00:39.451903 \n",
      "[Epoch 0/300] [Batch 69/292] [D loss: 0.385195, acc:  33%] [G loss: 6.204107, adv: 0.967428, recon: 0.194431, id: 0.189678] time: 0:00:39.657593 \n",
      "[Epoch 0/300] [Batch 70/292] [D loss: 0.469467, acc:  25%] [G loss: 6.789954, adv: 1.147606, recon: 0.202966, id: 0.179596] time: 0:00:39.856281 \n",
      "[Epoch 0/300] [Batch 71/292] [D loss: 0.567132, acc:  27%] [G loss: 6.761832, adv: 1.020160, recon: 0.213426, id: 0.189216] time: 0:00:40.052525 \n",
      "[Epoch 0/300] [Batch 72/292] [D loss: 0.405986, acc:  34%] [G loss: 6.718777, adv: 1.000005, recon: 0.213515, id: 0.257899] time: 0:00:40.257071 \n",
      "[Epoch 0/300] [Batch 73/292] [D loss: 0.272233, acc:  69%] [G loss: 7.611122, adv: 1.041896, recon: 0.249107, id: 0.190288] time: 0:00:40.454725 \n",
      "[Epoch 0/300] [Batch 74/292] [D loss: 0.292781, acc:  52%] [G loss: 6.913618, adv: 0.933954, recon: 0.228667, id: 0.259360] time: 0:00:40.663386 \n",
      "[Epoch 0/300] [Batch 75/292] [D loss: 0.246394, acc:  64%] [G loss: 6.401482, adv: 0.850612, recon: 0.208445, id: 0.216243] time: 0:00:40.860500 \n",
      "[Epoch 0/300] [Batch 76/292] [D loss: 0.277087, acc:  57%] [G loss: 6.302827, adv: 0.971121, recon: 0.198838, id: 0.195196] time: 0:00:41.058078 \n",
      "[Epoch 0/300] [Batch 77/292] [D loss: 0.487701, acc:  28%] [G loss: 7.193089, adv: 1.176875, recon: 0.219045, id: 0.184482] time: 0:00:41.272521 \n",
      "[Epoch 0/300] [Batch 78/292] [D loss: 0.568964, acc:  14%] [G loss: 6.303481, adv: 1.080893, recon: 0.189780, id: 0.188404] time: 0:00:41.509337 \n",
      "[Epoch 0/300] [Batch 79/292] [D loss: 0.438054, acc:  38%] [G loss: 6.219912, adv: 1.034866, recon: 0.187500, id: 0.188884] time: 0:00:41.716368 \n",
      "[Epoch 0/300] [Batch 80/292] [D loss: 0.329980, acc:  61%] [G loss: 6.747923, adv: 0.956339, recon: 0.215311, id: 0.161388] time: 0:00:41.932096 \n",
      "[Epoch 0/300] [Batch 81/292] [D loss: 0.525992, acc:  27%] [G loss: 7.341373, adv: 1.594167, recon: 0.188621, id: 0.165388] time: 0:00:42.138247 \n",
      "[Epoch 0/300] [Batch 82/292] [D loss: 0.294490, acc:  55%] [G loss: 7.333020, adv: 0.880273, recon: 0.247823, id: 0.224880] time: 0:00:42.357321 \n",
      "[Epoch 0/300] [Batch 83/292] [D loss: 0.291375, acc:  62%] [G loss: 6.828985, adv: 0.948888, recon: 0.222281, id: 0.180769] time: 0:00:42.547089 \n",
      "[Epoch 0/300] [Batch 84/292] [D loss: 0.739866, acc:  43%] [G loss: 6.181206, adv: 0.928235, recon: 0.193294, id: 0.189505] time: 0:00:42.755748 \n",
      "[Epoch 0/300] [Batch 85/292] [D loss: 0.501732, acc:  23%] [G loss: 5.952207, adv: 0.936824, recon: 0.184618, id: 0.183294] time: 0:00:42.975941 \n",
      "[Epoch 0/300] [Batch 86/292] [D loss: 0.539244, acc:  16%] [G loss: 5.579937, adv: 0.950609, recon: 0.167534, id: 0.171476] time: 0:00:43.142233 \n",
      "[Epoch 0/300] [Batch 87/292] [D loss: 0.419548, acc:  33%] [G loss: 6.081836, adv: 0.838404, recon: 0.199569, id: 0.172147] time: 0:00:43.324658 \n",
      "[Epoch 0/300] [Batch 88/292] [D loss: 0.322544, acc:  44%] [G loss: 5.519546, adv: 0.920546, recon: 0.162347, id: 0.178074] time: 0:00:43.535893 \n",
      "[Epoch 0/300] [Batch 89/292] [D loss: 0.396678, acc:  41%] [G loss: 5.909598, adv: 0.949495, recon: 0.178470, id: 0.143413] time: 0:00:43.766070 \n",
      "[Epoch 0/300] [Batch 90/292] [D loss: 0.472014, acc:  27%] [G loss: 5.782136, adv: 1.001724, recon: 0.171926, id: 0.169723] time: 0:00:43.955270 \n",
      "[Epoch 0/300] [Batch 91/292] [D loss: 0.393157, acc:  43%] [G loss: 5.776311, adv: 0.869480, recon: 0.180435, id: 0.163803] time: 0:00:44.179326 \n",
      "[Epoch 0/300] [Batch 92/292] [D loss: 0.492548, acc:  22%] [G loss: 5.836919, adv: 0.985133, recon: 0.176481, id: 0.144724] time: 0:00:44.399177 \n",
      "[Epoch 0/300] [Batch 93/292] [D loss: 0.308076, acc:  51%] [G loss: 6.357729, adv: 0.871369, recon: 0.209115, id: 0.175075] time: 0:00:44.598039 \n",
      "[Epoch 0/300] [Batch 94/292] [D loss: 0.426630, acc:  22%] [G loss: 5.890445, adv: 1.029910, recon: 0.172554, id: 0.157460] time: 0:00:44.788871 \n",
      "[Epoch 0/300] [Batch 95/292] [D loss: 0.383208, acc:  33%] [G loss: 6.216788, adv: 0.952331, recon: 0.197757, id: 0.156310] time: 0:00:44.976740 \n",
      "[Epoch 0/300] [Batch 96/292] [D loss: 0.295619, acc:  52%] [G loss: 5.546424, adv: 0.923175, recon: 0.165064, id: 0.165152] time: 0:00:45.148898 \n",
      "[Epoch 0/300] [Batch 97/292] [D loss: 0.228251, acc:  67%] [G loss: 6.246761, adv: 0.923630, recon: 0.196650, id: 0.161914] time: 0:00:45.344511 \n",
      "[Epoch 0/300] [Batch 98/292] [D loss: 0.248954, acc:  60%] [G loss: 5.782169, adv: 0.979722, recon: 0.172050, id: 0.164748] time: 0:00:45.520828 \n",
      "[Epoch 0/300] [Batch 99/292] [D loss: 0.511465, acc:  24%] [G loss: 6.195039, adv: 1.197421, recon: 0.172472, id: 0.139889] time: 0:00:45.715261 \n",
      "[Epoch 0/300] [Batch 100/292] [D loss: 0.396975, acc:  39%] [G loss: 6.290239, adv: 1.067462, recon: 0.190929, id: 0.155728] time: 0:00:45.920606 \n",
      "[Epoch 0/300] [Batch 101/292] [D loss: 0.317855, acc:  55%] [G loss: 6.414935, adv: 1.104859, recon: 0.192235, id: 0.141545] time: 0:00:46.127120 \n",
      "[Epoch 0/300] [Batch 102/292] [D loss: 0.286147, acc:  57%] [G loss: 6.782072, adv: 0.970536, recon: 0.219530, id: 0.148487] time: 0:00:46.350781 \n",
      "[Epoch 0/300] [Batch 103/292] [D loss: 0.299427, acc:  55%] [G loss: 6.171618, adv: 0.999954, recon: 0.188349, id: 0.182857] time: 0:00:46.535328 \n",
      "[Epoch 0/300] [Batch 104/292] [D loss: 0.331804, acc:  43%] [G loss: 6.212076, adv: 1.134808, recon: 0.182168, id: 0.151714] time: 0:00:46.728746 \n",
      "[Epoch 0/300] [Batch 105/292] [D loss: 0.416659, acc:  23%] [G loss: 8.246438, adv: 1.525904, recon: 0.239153, id: 0.251527] time: 0:00:46.925913 \n",
      "[Epoch 0/300] [Batch 106/292] [D loss: 0.587693, acc:  29%] [G loss: 6.286246, adv: 1.131828, recon: 0.184509, id: 0.145965] time: 0:00:47.143722 \n",
      "[Epoch 0/300] [Batch 107/292] [D loss: 0.196443, acc:  71%] [G loss: 6.744368, adv: 0.939763, recon: 0.220766, id: 0.196418] time: 0:00:47.340874 \n",
      "[Epoch 0/300] [Batch 108/292] [D loss: 0.408535, acc:  50%] [G loss: 5.907819, adv: 1.055184, recon: 0.173054, id: 0.137879] time: 0:00:47.522199 \n",
      "[Epoch 0/300] [Batch 109/292] [D loss: 0.832319, acc:  63%] [G loss: 7.012697, adv: 0.872880, recon: 0.237491, id: 0.208573] time: 0:00:47.720489 \n",
      "[Epoch 0/300] [Batch 110/292] [D loss: 0.950944, acc:  33%] [G loss: 6.000443, adv: 0.801099, recon: 0.200642, id: 0.148359] time: 0:00:47.933781 \n",
      "[Epoch 0/300] [Batch 111/292] [D loss: 0.534247, acc:  20%] [G loss: 5.409642, adv: 0.859255, recon: 0.167565, id: 0.154590] time: 0:00:48.121728 \n",
      "[Epoch 0/300] [Batch 112/292] [D loss: 0.465970, acc:  30%] [G loss: 5.774126, adv: 0.735877, recon: 0.193087, id: 0.144226] time: 0:00:48.345763 \n",
      "[Epoch 0/300] [Batch 113/292] [D loss: 0.361740, acc:  41%] [G loss: 5.703063, adv: 0.746639, recon: 0.186810, id: 0.167218] time: 0:00:48.547694 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/300] [Batch 114/292] [D loss: 0.319465, acc:  52%] [G loss: 5.719390, adv: 0.874319, recon: 0.175334, id: 0.147626] time: 0:00:48.749429 \n",
      "[Epoch 0/300] [Batch 115/292] [D loss: 0.399237, acc:  35%] [G loss: 5.294689, adv: 0.808997, recon: 0.166007, id: 0.142116] time: 0:00:48.978355 \n",
      "[Epoch 0/300] [Batch 116/292] [D loss: 0.368159, acc:  44%] [G loss: 5.482327, adv: 0.850399, recon: 0.167801, id: 0.130765] time: 0:00:49.180554 \n",
      "[Epoch 0/300] [Batch 117/292] [D loss: 0.448659, acc:  25%] [G loss: 5.338871, adv: 0.763821, recon: 0.170829, id: 0.167274] time: 0:00:49.375083 \n",
      "[Epoch 0/300] [Batch 118/292] [D loss: 0.500862, acc:  20%] [G loss: 5.363954, adv: 0.751809, recon: 0.176704, id: 0.143673] time: 0:00:49.576344 \n",
      "[Epoch 0/300] [Batch 119/292] [D loss: 0.468617, acc:  21%] [G loss: 4.936969, adv: 0.771440, recon: 0.154656, id: 0.143247] time: 0:00:49.786696 \n",
      "[Epoch 0/300] [Batch 120/292] [D loss: 0.396899, acc:  29%] [G loss: 4.995850, adv: 0.737603, recon: 0.158141, id: 0.162416] time: 0:00:49.994290 \n",
      "[Epoch 0/300] [Batch 121/292] [D loss: 0.434389, acc:  32%] [G loss: 5.381298, adv: 0.750505, recon: 0.175662, id: 0.129549] time: 0:00:50.199391 \n",
      "[Epoch 0/300] [Batch 122/292] [D loss: 0.436822, acc:  21%] [G loss: 4.891322, adv: 0.735677, recon: 0.154846, id: 0.157904] time: 0:00:50.408111 \n",
      "[Epoch 0/300] [Batch 123/292] [D loss: 0.242741, acc:  67%] [G loss: 7.086926, adv: 0.867455, recon: 0.237476, id: 0.165649] time: 0:00:50.614221 \n",
      "[Epoch 0/300] [Batch 124/292] [D loss: 0.424034, acc:  30%] [G loss: 5.403683, adv: 0.786341, recon: 0.174827, id: 0.172009] time: 0:00:50.827061 \n",
      "[Epoch 0/300] [Batch 125/292] [D loss: 0.560236, acc:  25%] [G loss: 5.022556, adv: 0.779862, recon: 0.156372, id: 0.144133] time: 0:00:51.009363 \n",
      "[Epoch 0/300] [Batch 126/292] [D loss: 0.324394, acc:  42%] [G loss: 5.097201, adv: 0.752878, recon: 0.160376, id: 0.150258] time: 0:00:51.214062 \n",
      "[Epoch 0/300] [Batch 127/292] [D loss: 0.361308, acc:  32%] [G loss: 5.170813, adv: 0.784148, recon: 0.164220, id: 0.154915] time: 0:00:51.419820 \n",
      "[Epoch 0/300] [Batch 128/292] [D loss: 0.388462, acc:  30%] [G loss: 5.393951, adv: 0.722833, recon: 0.180746, id: 0.147850] time: 0:00:51.613127 \n",
      "[Epoch 0/300] [Batch 129/292] [D loss: 0.324756, acc:  46%] [G loss: 5.184347, adv: 0.783311, recon: 0.159970, id: 0.146894] time: 0:00:51.803591 \n",
      "[Epoch 0/300] [Batch 130/292] [D loss: 0.201049, acc:  68%] [G loss: 5.737411, adv: 0.798456, recon: 0.184699, id: 0.165787] time: 0:00:52.011732 \n",
      "[Epoch 0/300] [Batch 131/292] [D loss: 0.388233, acc:  31%] [G loss: 5.387129, adv: 0.763646, recon: 0.175289, id: 0.129380] time: 0:00:52.228007 \n",
      "[Epoch 0/300] [Batch 132/292] [D loss: 0.372324, acc:  35%] [G loss: 5.289606, adv: 0.737153, recon: 0.171833, id: 0.157466] time: 0:00:52.445640 \n",
      "[Epoch 0/300] [Batch 133/292] [D loss: 0.249187, acc:  55%] [G loss: 5.248226, adv: 0.809668, recon: 0.161197, id: 0.154835] time: 0:00:52.639488 \n",
      "[Epoch 0/300] [Batch 134/292] [D loss: 0.354314, acc:  39%] [G loss: 5.046986, adv: 0.790839, recon: 0.156789, id: 0.121937] time: 0:00:52.856482 \n",
      "[Epoch 0/300] [Batch 135/292] [D loss: 0.349901, acc:  36%] [G loss: 5.237299, adv: 0.822207, recon: 0.162386, id: 0.153756] time: 0:00:53.064822 \n",
      "[Epoch 0/300] [Batch 136/292] [D loss: 0.229381, acc:  64%] [G loss: 5.580287, adv: 0.855382, recon: 0.172448, id: 0.140545] time: 0:00:53.278633 \n",
      "[Epoch 0/300] [Batch 137/292] [D loss: 0.313045, acc:  42%] [G loss: 5.362624, adv: 0.797996, recon: 0.169131, id: 0.168419] time: 0:00:53.498448 \n",
      "[Epoch 0/300] [Batch 138/292] [D loss: 0.334095, acc:  40%] [G loss: 5.536079, adv: 0.767806, recon: 0.180353, id: 0.177480] time: 0:00:53.711891 \n",
      "[Epoch 0/300] [Batch 139/292] [D loss: 0.272692, acc:  52%] [G loss: 5.646728, adv: 0.893879, recon: 0.174192, id: 0.166422] time: 0:00:53.896327 \n",
      "[Epoch 0/300] [Batch 140/292] [D loss: 0.334982, acc:  38%] [G loss: 5.452797, adv: 0.834490, recon: 0.174488, id: 0.122588] time: 0:00:54.085902 \n",
      "[Epoch 0/300] [Batch 141/292] [D loss: 0.168249, acc:  76%] [G loss: 6.357263, adv: 0.942316, recon: 0.198271, id: 0.157788] time: 0:00:54.263779 \n",
      "[Epoch 0/300] [Batch 142/292] [D loss: 0.242998, acc:  60%] [G loss: 5.505349, adv: 0.817266, recon: 0.171745, id: 0.162341] time: 0:00:54.443064 \n",
      "[Epoch 0/300] [Batch 143/292] [D loss: 0.327117, acc:  42%] [G loss: 5.405726, adv: 0.834902, recon: 0.168060, id: 0.160369] time: 0:00:54.643578 \n",
      "[Epoch 0/300] [Batch 144/292] [D loss: 0.290916, acc:  49%] [G loss: 6.198033, adv: 0.783322, recon: 0.209855, id: 0.190071] time: 0:00:54.862449 \n",
      "[Epoch 0/300] [Batch 145/292] [D loss: 0.236721, acc:  66%] [G loss: 5.610035, adv: 0.968295, recon: 0.166406, id: 0.154737] time: 0:00:55.067903 \n",
      "[Epoch 0/300] [Batch 146/292] [D loss: 0.393434, acc:  35%] [G loss: 4.979455, adv: 0.917392, recon: 0.142536, id: 0.135597] time: 0:00:55.269950 \n",
      "[Epoch 0/300] [Batch 147/292] [D loss: 0.350372, acc:  43%] [G loss: 5.526773, adv: 0.900165, recon: 0.164835, id: 0.163614] time: 0:00:55.509310 \n",
      "[Epoch 0/300] [Batch 148/292] [D loss: 0.344765, acc:  40%] [G loss: 5.573461, adv: 0.965732, recon: 0.165052, id: 0.142926] time: 0:00:55.704054 \n",
      "[Epoch 0/300] [Batch 149/292] [D loss: 0.320382, acc:  49%] [G loss: 5.626288, adv: 0.992290, recon: 0.161878, id: 0.138567] time: 0:00:55.874783 \n",
      "[Epoch 0/300] [Batch 150/292] [D loss: 0.242998, acc:  59%] [G loss: 5.840132, adv: 0.835629, recon: 0.188180, id: 0.186953] time: 0:00:56.035428 \n",
      "[Epoch 0/300] [Batch 151/292] [D loss: 0.304317, acc:  54%] [G loss: 5.370259, adv: 0.947257, recon: 0.157660, id: 0.127181] time: 0:00:56.215896 \n",
      "[Epoch 0/300] [Batch 152/292] [D loss: 0.273082, acc:  57%] [G loss: 5.669660, adv: 0.993465, recon: 0.163857, id: 0.159838] time: 0:00:56.439570 \n",
      "[Epoch 0/300] [Batch 153/292] [D loss: 0.229405, acc:  63%] [G loss: 5.752579, adv: 0.935585, recon: 0.178010, id: 0.140452] time: 0:00:56.666466 \n",
      "[Epoch 0/300] [Batch 154/292] [D loss: 0.219337, acc:  69%] [G loss: 6.901495, adv: 1.403139, recon: 0.181932, id: 0.173912] time: 0:00:56.889006 \n",
      "[Epoch 0/300] [Batch 155/292] [D loss: 0.270493, acc:  58%] [G loss: 6.344193, adv: 1.104388, recon: 0.189608, id: 0.152179] time: 0:00:57.089885 \n",
      "[Epoch 0/300] [Batch 156/292] [D loss: 0.407936, acc:  51%] [G loss: 5.993842, adv: 1.019859, recon: 0.178059, id: 0.153818] time: 0:00:57.268403 \n",
      "[Epoch 0/300] [Batch 157/292] [D loss: 0.322858, acc:  51%] [G loss: 6.727511, adv: 1.171711, recon: 0.197112, id: 0.173228] time: 0:00:57.457717 \n",
      "[Epoch 0/300] [Batch 158/292] [D loss: 0.146408, acc:  79%] [G loss: 5.672042, adv: 1.005503, recon: 0.165854, id: 0.162705] time: 0:00:57.654387 \n",
      "[Epoch 0/300] [Batch 159/292] [D loss: 0.242618, acc:  62%] [G loss: 5.616340, adv: 0.946025, recon: 0.166791, id: 0.147051] time: 0:00:57.862854 \n",
      "[Epoch 0/300] [Batch 160/292] [D loss: 0.231410, acc:  64%] [G loss: 5.814290, adv: 1.018785, recon: 0.172433, id: 0.173456] time: 0:00:58.074416 \n",
      "[Epoch 0/300] [Batch 161/292] [D loss: 0.282748, acc:  59%] [G loss: 5.778190, adv: 1.080869, recon: 0.164387, id: 0.139841] time: 0:00:58.283886 \n",
      "[Epoch 0/300] [Batch 162/292] [D loss: 0.259465, acc:  62%] [G loss: 5.683145, adv: 1.109570, recon: 0.157402, id: 0.137474] time: 0:00:58.473265 \n",
      "[Epoch 0/300] [Batch 163/292] [D loss: 0.366259, acc:  42%] [G loss: 5.560407, adv: 0.965940, recon: 0.165926, id: 0.145382] time: 0:00:58.650659 \n",
      "[Epoch 0/300] [Batch 164/292] [D loss: 0.229672, acc:  66%] [G loss: 5.551517, adv: 1.006111, recon: 0.160328, id: 0.154366] time: 0:00:58.848474 \n",
      "[Epoch 0/300] [Batch 165/292] [D loss: 0.182891, acc:  73%] [G loss: 5.817174, adv: 1.003619, recon: 0.172400, id: 0.155821] time: 0:00:59.051078 \n",
      "[Epoch 0/300] [Batch 166/292] [D loss: 0.288140, acc:  54%] [G loss: 5.730199, adv: 1.023388, recon: 0.164455, id: 0.148019] time: 0:00:59.259665 \n",
      "[Epoch 0/300] [Batch 167/292] [D loss: 0.447668, acc:  30%] [G loss: 5.842979, adv: 1.201097, recon: 0.155299, id: 0.135876] time: 0:00:59.457799 \n",
      "[Epoch 0/300] [Batch 168/292] [D loss: 0.316161, acc:  49%] [G loss: 7.363271, adv: 1.587930, recon: 0.190966, id: 0.166281] time: 0:00:59.639820 \n",
      "[Epoch 0/300] [Batch 169/292] [D loss: 0.263949, acc:  58%] [G loss: 6.742170, adv: 1.243154, recon: 0.195962, id: 0.121548] time: 0:00:59.828722 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/300] [Batch 170/292] [D loss: 0.233694, acc:  63%] [G loss: 6.642616, adv: 0.979604, recon: 0.210681, id: 0.139209] time: 0:01:00.034916 \n",
      "[Epoch 0/300] [Batch 171/292] [D loss: 0.307172, acc:  59%] [G loss: 6.212924, adv: 1.107935, recon: 0.183237, id: 0.125887] time: 0:01:00.235595 \n",
      "[Epoch 0/300] [Batch 172/292] [D loss: 0.271177, acc:  77%] [G loss: 6.304059, adv: 1.010965, recon: 0.190914, id: 0.126530] time: 0:01:00.457273 \n",
      "[Epoch 0/300] [Batch 173/292] [D loss: 0.733837, acc:  81%] [G loss: 6.012824, adv: 0.940714, recon: 0.184071, id: 0.139592] time: 0:01:00.668126 \n",
      "[Epoch 0/300] [Batch 174/292] [D loss: 0.607936, acc:  36%] [G loss: 5.056299, adv: 0.873596, recon: 0.150885, id: 0.142940] time: 0:01:00.865838 \n",
      "[Epoch 0/300] [Batch 175/292] [D loss: 0.511277, acc:  30%] [G loss: 5.072094, adv: 0.829675, recon: 0.154419, id: 0.134284] time: 0:01:01.091728 \n",
      "[Epoch 0/300] [Batch 176/292] [D loss: 0.367820, acc:  42%] [G loss: 5.238912, adv: 0.784999, recon: 0.163141, id: 0.121928] time: 0:01:01.310078 \n",
      "[Epoch 0/300] [Batch 177/292] [D loss: 0.386949, acc:  36%] [G loss: 4.997749, adv: 0.855604, recon: 0.150383, id: 0.136145] time: 0:01:01.509975 \n",
      "[Epoch 0/300] [Batch 178/292] [D loss: 0.405562, acc:  32%] [G loss: 5.152465, adv: 0.802931, recon: 0.160265, id: 0.131635] time: 0:01:01.701881 \n",
      "[Epoch 0/300] [Batch 179/292] [D loss: 0.321950, acc:  45%] [G loss: 5.613691, adv: 0.821519, recon: 0.181323, id: 0.160592] time: 0:01:01.889099 \n",
      "[Epoch 0/300] [Batch 180/292] [D loss: 0.375273, acc:  34%] [G loss: 5.918230, adv: 0.810715, recon: 0.197169, id: 0.146415] time: 0:01:02.077518 \n",
      "[Epoch 0/300] [Batch 181/292] [D loss: 0.293299, acc:  53%] [G loss: 5.492780, adv: 0.813247, recon: 0.175709, id: 0.156289] time: 0:01:02.274734 \n",
      "[Epoch 0/300] [Batch 182/292] [D loss: 0.385968, acc:  30%] [G loss: 4.568429, adv: 0.793499, recon: 0.134543, id: 0.130290] time: 0:01:02.474948 \n",
      "[Epoch 0/300] [Batch 183/292] [D loss: 0.273557, acc:  48%] [G loss: 4.641516, adv: 0.795457, recon: 0.138121, id: 0.117719] time: 0:01:02.676403 \n",
      "[Epoch 0/300] [Batch 184/292] [D loss: 0.275219, acc:  49%] [G loss: 4.497676, adv: 0.805523, recon: 0.129552, id: 0.124122] time: 0:01:02.865753 \n",
      "[Epoch 0/300] [Batch 185/292] [D loss: 0.307178, acc:  44%] [G loss: 4.807819, adv: 0.816786, recon: 0.143324, id: 0.118310] time: 0:01:03.095167 \n",
      "[Epoch 0/300] [Batch 186/292] [D loss: 0.338851, acc:  34%] [G loss: 5.434796, adv: 0.855026, recon: 0.170846, id: 0.131921] time: 0:01:03.288105 \n",
      "[Epoch 0/300] [Batch 187/292] [D loss: 0.321194, acc:  41%] [G loss: 5.398214, adv: 0.845853, recon: 0.169644, id: 0.149777] time: 0:01:03.492142 \n",
      "[Epoch 0/300] [Batch 188/292] [D loss: 0.270723, acc:  57%] [G loss: 5.528823, adv: 0.865099, recon: 0.171365, id: 0.127596] time: 0:01:03.714978 \n",
      "[Epoch 0/300] [Batch 189/292] [D loss: 0.356455, acc:  40%] [G loss: 4.776883, adv: 0.821157, recon: 0.140779, id: 0.116373] time: 0:01:03.917353 \n",
      "[Epoch 0/300] [Batch 190/292] [D loss: 0.332250, acc:  38%] [G loss: 4.669077, adv: 0.861257, recon: 0.134334, id: 0.117225] time: 0:01:04.124769 \n",
      "[Epoch 0/300] [Batch 191/292] [D loss: 0.302523, acc:  42%] [G loss: 5.024792, adv: 0.880909, recon: 0.148510, id: 0.122384] time: 0:01:04.346237 \n",
      "[Epoch 0/300] [Batch 192/292] [D loss: 0.319189, acc:  41%] [G loss: 4.798744, adv: 0.850726, recon: 0.138901, id: 0.119546] time: 0:01:04.555389 \n",
      "[Epoch 0/300] [Batch 193/292] [D loss: 0.205310, acc:  65%] [G loss: 6.245003, adv: 0.976702, recon: 0.191781, id: 0.122223] time: 0:01:04.735542 \n",
      "[Epoch 0/300] [Batch 194/292] [D loss: 0.307588, acc:  47%] [G loss: 5.376142, adv: 0.985174, recon: 0.155382, id: 0.118929] time: 0:01:04.931564 \n",
      "[Epoch 0/300] [Batch 195/292] [D loss: 0.222809, acc:  63%] [G loss: 5.813266, adv: 0.888202, recon: 0.183159, id: 0.145116] time: 0:01:05.130747 \n",
      "[Epoch 0/300] [Batch 196/292] [D loss: 0.326942, acc:  41%] [G loss: 4.593945, adv: 0.831675, recon: 0.134561, id: 0.120840] time: 0:01:05.336054 \n",
      "[Epoch 0/300] [Batch 197/292] [D loss: 0.270634, acc:  47%] [G loss: 5.165384, adv: 0.933405, recon: 0.150557, id: 0.127927] time: 0:01:05.552282 \n",
      "[Epoch 0/300] [Batch 198/292] [D loss: 0.293335, acc:  48%] [G loss: 5.473632, adv: 0.909848, recon: 0.169048, id: 0.142894] time: 0:01:05.760253 \n",
      "[Epoch 0/300] [Batch 199/292] [D loss: 0.314502, acc:  41%] [G loss: 5.657489, adv: 1.056094, recon: 0.163318, id: 0.122741] time: 0:01:05.965308 \n",
      "[Epoch 0/300] [Batch 200/292] [D loss: 0.318514, acc:  48%] [G loss: 5.278163, adv: 1.000435, recon: 0.147140, id: 0.123650] time: 0:01:06.152061 \n",
      "[Epoch 0/300] [Batch 201/292] [D loss: 0.246514, acc:  60%] [G loss: 5.160198, adv: 0.945661, recon: 0.147388, id: 0.126222] time: 0:01:06.806387 \n",
      "[Epoch 0/300] [Batch 202/292] [D loss: 0.260946, acc:  52%] [G loss: 5.397745, adv: 1.029181, recon: 0.152283, id: 0.112288] time: 0:01:06.987258 \n",
      "[Epoch 0/300] [Batch 203/292] [D loss: 0.275967, acc:  48%] [G loss: 4.857720, adv: 0.975995, recon: 0.131112, id: 0.116018] time: 0:01:07.175588 \n",
      "[Epoch 0/300] [Batch 204/292] [D loss: 0.254103, acc:  60%] [G loss: 5.344999, adv: 1.056642, recon: 0.145242, id: 0.110603] time: 0:01:07.355381 \n",
      "[Epoch 0/300] [Batch 205/292] [D loss: 0.273527, acc:  53%] [G loss: 5.027289, adv: 0.959938, recon: 0.138530, id: 0.143804] time: 0:01:07.576623 \n",
      "[Epoch 0/300] [Batch 206/292] [D loss: 0.215803, acc:  68%] [G loss: 4.984754, adv: 0.892719, recon: 0.142637, id: 0.116879] time: 0:01:07.809069 \n",
      "[Epoch 0/300] [Batch 207/292] [D loss: 0.345912, acc:  42%] [G loss: 5.104152, adv: 1.004448, recon: 0.138772, id: 0.123386] time: 0:01:08.023947 \n",
      "[Epoch 0/300] [Batch 208/292] [D loss: 0.356744, acc:  34%] [G loss: 6.466800, adv: 1.273882, recon: 0.179634, id: 0.119233] time: 0:01:08.231526 \n",
      "[Epoch 0/300] [Batch 209/292] [D loss: 0.233670, acc:  60%] [G loss: 6.387550, adv: 1.088937, recon: 0.190758, id: 0.174905] time: 0:01:08.428893 \n",
      "[Epoch 0/300] [Batch 210/292] [D loss: 0.307872, acc:  59%] [G loss: 7.155619, adv: 1.107304, recon: 0.223342, id: 0.137749] time: 0:01:08.617420 \n",
      "[Epoch 0/300] [Batch 211/292] [D loss: 0.275549, acc:  50%] [G loss: 5.774732, adv: 1.008434, recon: 0.172362, id: 0.116011] time: 0:01:08.803936 \n",
      "[Epoch 0/300] [Batch 212/292] [D loss: 0.282532, acc:  49%] [G loss: 5.212632, adv: 0.906886, recon: 0.151494, id: 0.134267] time: 0:01:08.999362 \n",
      "[Epoch 0/300] [Batch 213/292] [D loss: 0.196930, acc:  67%] [G loss: 5.600708, adv: 1.019096, recon: 0.162170, id: 0.162466] time: 0:01:09.208222 \n",
      "[Epoch 0/300] [Batch 214/292] [D loss: 0.192983, acc:  71%] [G loss: 5.568132, adv: 1.030245, recon: 0.160236, id: 0.131256] time: 0:01:09.419541 \n",
      "[Epoch 0/300] [Batch 215/292] [D loss: 0.159066, acc:  78%] [G loss: 5.278965, adv: 0.952150, recon: 0.152369, id: 0.140258] time: 0:01:09.629104 \n",
      "[Epoch 0/300] [Batch 216/292] [D loss: 0.256553, acc:  58%] [G loss: 5.848744, adv: 1.120251, recon: 0.166001, id: 0.120465] time: 0:01:09.829854 \n",
      "[Epoch 0/300] [Batch 217/292] [D loss: 0.182277, acc:  73%] [G loss: 6.362596, adv: 1.219211, recon: 0.176190, id: 0.165233] time: 0:01:10.019721 \n",
      "[Epoch 0/300] [Batch 218/292] [D loss: 0.171038, acc:  78%] [G loss: 5.917711, adv: 0.828645, recon: 0.189106, id: 0.119165] time: 0:01:10.211056 \n",
      "[Epoch 0/300] [Batch 219/292] [D loss: 0.232940, acc:  62%] [G loss: 5.251621, adv: 1.040319, recon: 0.141451, id: 0.157086] time: 0:01:10.398538 \n",
      "[Epoch 0/300] [Batch 220/292] [D loss: 0.213554, acc:  68%] [G loss: 5.706782, adv: 1.257834, recon: 0.144974, id: 0.155967] time: 0:01:10.627818 \n",
      "[Epoch 0/300] [Batch 221/292] [D loss: 0.252586, acc:  64%] [G loss: 6.708799, adv: 1.339734, recon: 0.180862, id: 0.126335] time: 0:01:10.872420 \n",
      "[Epoch 0/300] [Batch 222/292] [D loss: 0.321021, acc:  53%] [G loss: 5.551712, adv: 0.937002, recon: 0.166647, id: 0.155402] time: 0:01:11.082947 \n",
      "[Epoch 0/300] [Batch 223/292] [D loss: 0.198685, acc:  70%] [G loss: 5.277885, adv: 1.031689, recon: 0.145069, id: 0.128080] time: 0:01:11.278521 \n",
      "[Epoch 0/300] [Batch 224/292] [D loss: 0.130061, acc:  83%] [G loss: 6.026713, adv: 1.122807, recon: 0.172646, id: 0.126866] time: 0:01:11.444237 \n",
      "[Epoch 0/300] [Batch 225/292] [D loss: 0.164667, acc:  78%] [G loss: 5.605081, adv: 1.035800, recon: 0.160472, id: 0.139990] time: 0:01:11.621111 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/300] [Batch 226/292] [D loss: 0.230398, acc:  62%] [G loss: 5.824739, adv: 1.097697, recon: 0.164352, id: 0.110263] time: 0:01:11.825023 \n",
      "[Epoch 0/300] [Batch 227/292] [D loss: 0.301917, acc:  55%] [G loss: 7.166147, adv: 1.456696, recon: 0.194837, id: 0.167159] time: 0:01:12.028042 \n",
      "[Epoch 0/300] [Batch 228/292] [D loss: 0.219715, acc:  70%] [G loss: 6.825238, adv: 1.357366, recon: 0.184433, id: 0.120023] time: 0:01:12.210606 \n",
      "[Epoch 0/300] [Batch 229/292] [D loss: 0.159717, acc:  78%] [G loss: 6.205528, adv: 1.060497, recon: 0.184700, id: 0.126616] time: 0:01:12.408099 \n",
      "[Epoch 0/300] [Batch 230/292] [D loss: 0.174447, acc:  81%] [G loss: 5.836124, adv: 0.872228, recon: 0.184375, id: 0.129324] time: 0:01:12.607390 \n",
      "[Epoch 0/300] [Batch 231/292] [D loss: 0.231844, acc:  63%] [G loss: 5.649191, adv: 1.169519, recon: 0.149953, id: 0.127142] time: 0:01:12.784106 \n",
      "[Epoch 0/300] [Batch 232/292] [D loss: 0.230201, acc:  77%] [G loss: 6.382762, adv: 1.080569, recon: 0.191099, id: 0.112212] time: 0:01:12.960838 \n",
      "[Epoch 0/300] [Batch 233/292] [D loss: 0.320283, acc:  46%] [G loss: 5.561257, adv: 1.079198, recon: 0.154764, id: 0.126894] time: 0:01:13.163151 \n",
      "[Epoch 0/300] [Batch 234/292] [D loss: 0.286667, acc:  47%] [G loss: 5.660927, adv: 1.034473, recon: 0.164369, id: 0.137984] time: 0:01:13.370238 \n",
      "[Epoch 0/300] [Batch 235/292] [D loss: 0.271808, acc:  56%] [G loss: 5.253493, adv: 0.939620, recon: 0.151102, id: 0.129994] time: 0:01:13.569501 \n",
      "[Epoch 0/300] [Batch 236/292] [D loss: 0.234206, acc:  57%] [G loss: 5.172938, adv: 1.039788, recon: 0.140880, id: 0.136302] time: 0:01:13.812428 \n",
      "[Epoch 0/300] [Batch 237/292] [D loss: 0.159758, acc:  77%] [G loss: 5.832597, adv: 1.148329, recon: 0.160050, id: 0.115089] time: 0:01:14.031527 \n",
      "[Epoch 0/300] [Batch 238/292] [D loss: 0.177664, acc:  76%] [G loss: 5.325475, adv: 1.140334, recon: 0.137745, id: 0.120948] time: 0:01:14.262856 \n",
      "[Epoch 0/300] [Batch 239/292] [D loss: 0.183389, acc:  72%] [G loss: 5.939702, adv: 1.197771, recon: 0.164338, id: 0.111989] time: 0:01:14.475629 \n",
      "[Epoch 0/300] [Batch 240/292] [D loss: 0.218997, acc:  65%] [G loss: 6.424271, adv: 1.503428, recon: 0.156761, id: 0.143489] time: 0:01:14.682888 \n",
      "[Epoch 0/300] [Batch 241/292] [D loss: 0.333733, acc:  58%] [G loss: 6.952145, adv: 1.520294, recon: 0.176254, id: 0.123316] time: 0:01:14.860040 \n",
      "[Epoch 0/300] [Batch 242/292] [D loss: 0.225222, acc:  67%] [G loss: 5.975164, adv: 1.067491, recon: 0.175767, id: 0.139082] time: 0:01:15.038392 \n",
      "[Epoch 0/300] [Batch 243/292] [D loss: 0.181863, acc:  74%] [G loss: 6.945888, adv: 1.360315, recon: 0.195940, id: 0.129907] time: 0:01:15.246384 \n",
      "[Epoch 0/300] [Batch 244/292] [D loss: 0.081461, acc:  91%] [G loss: 6.250532, adv: 1.031911, recon: 0.191407, id: 0.123981] time: 0:01:15.462807 \n",
      "[Epoch 0/300] [Batch 245/292] [D loss: 0.164105, acc:  77%] [G loss: 6.357412, adv: 1.356899, recon: 0.168481, id: 0.132004] time: 0:01:15.667497 \n",
      "[Epoch 0/300] [Batch 246/292] [D loss: 0.277523, acc:  49%] [G loss: 6.316242, adv: 1.125177, recon: 0.187730, id: 0.119608] time: 0:01:15.856269 \n",
      "[Epoch 0/300] [Batch 247/292] [D loss: 0.139712, acc:  83%] [G loss: 5.875663, adv: 1.150293, recon: 0.163353, id: 0.123865] time: 0:01:16.054817 \n",
      "[Epoch 0/300] [Batch 248/292] [D loss: 0.293436, acc:  57%] [G loss: 6.114978, adv: 1.206590, recon: 0.168404, id: 0.181470] time: 0:01:16.250237 \n",
      "[Epoch 0/300] [Batch 249/292] [D loss: 0.119692, acc:  87%] [G loss: 5.792185, adv: 1.148130, recon: 0.149716, id: 0.124963] time: 0:01:16.456731 \n",
      "[Epoch 0/300] [Batch 250/292] [D loss: 0.210259, acc:  73%] [G loss: 5.391841, adv: 1.134436, recon: 0.140308, id: 0.138989] time: 0:01:16.648218 \n",
      "[Epoch 0/300] [Batch 251/292] [D loss: 0.163384, acc:  76%] [G loss: 5.566429, adv: 0.982518, recon: 0.161237, id: 0.118943] time: 0:01:16.869586 \n",
      "[Epoch 0/300] [Batch 252/292] [D loss: 0.205276, acc:  69%] [G loss: 5.722606, adv: 1.039820, recon: 0.166743, id: 0.136429] time: 0:01:17.079535 \n",
      "[Epoch 0/300] [Batch 253/292] [D loss: 0.314936, acc:  47%] [G loss: 6.147092, adv: 1.205578, recon: 0.170169, id: 0.120040] time: 0:01:17.290314 \n",
      "[Epoch 0/300] [Batch 254/292] [D loss: 0.222442, acc:  72%] [G loss: 6.103696, adv: 1.330912, recon: 0.156365, id: 0.119819] time: 0:01:17.485837 \n",
      "[Epoch 0/300] [Batch 255/292] [D loss: 0.244021, acc:  67%] [G loss: 5.387100, adv: 0.982090, recon: 0.156625, id: 0.131579] time: 0:01:17.683643 \n",
      "[Epoch 0/300] [Batch 256/292] [D loss: 0.389523, acc:  43%] [G loss: 10.381454, adv: 1.520221, recon: 0.343822, id: 0.104406] time: 0:01:17.901480 \n",
      "[Epoch 0/300] [Batch 257/292] [D loss: 0.191565, acc:  71%] [G loss: 8.280000, adv: 1.113225, recon: 0.242626, id: 1.013155] time: 0:01:18.143796 \n",
      "[Epoch 0/300] [Batch 258/292] [D loss: 0.129978, acc:  87%] [G loss: 7.531842, adv: 1.201282, recon: 0.205128, id: 0.834167] time: 0:01:18.365137 \n",
      "[Epoch 0/300] [Batch 259/292] [D loss: 0.287417, acc:  64%] [G loss: 7.238502, adv: 0.993281, recon: 0.213733, id: 0.626621] time: 0:01:18.583217 \n",
      "[Epoch 0/300] [Batch 260/292] [D loss: 0.390713, acc:  79%] [G loss: 7.022479, adv: 1.032345, recon: 0.203663, id: 0.534252] time: 0:01:18.769526 \n",
      "[Epoch 0/300] [Batch 261/292] [D loss: 0.177866, acc:  77%] [G loss: 6.712089, adv: 0.985855, recon: 0.188824, id: 0.682030] time: 0:01:18.956289 \n",
      "[Epoch 0/300] [Batch 262/292] [D loss: 0.187075, acc:  75%] [G loss: 6.391978, adv: 1.093768, recon: 0.178852, id: 0.289580] time: 0:01:19.157894 \n",
      "[Epoch 0/300] [Batch 263/292] [D loss: 0.234497, acc:  57%] [G loss: 6.484689, adv: 1.091247, recon: 0.178315, id: 0.557432] time: 0:01:19.389528 \n",
      "[Epoch 0/300] [Batch 264/292] [D loss: 0.231549, acc:  73%] [G loss: 6.883984, adv: 1.187081, recon: 0.191773, id: 0.312512] time: 0:01:19.612205 \n",
      "[Epoch 0/300] [Batch 265/292] [D loss: 0.280584, acc:  53%] [G loss: 5.856624, adv: 1.143005, recon: 0.149399, id: 0.390590] time: 0:01:19.823291 \n",
      "[Epoch 0/300] [Batch 266/292] [D loss: 0.241413, acc:  59%] [G loss: 5.135593, adv: 0.897976, recon: 0.141456, id: 0.298557] time: 0:01:20.006998 \n",
      "[Epoch 0/300] [Batch 267/292] [D loss: 0.179531, acc:  68%] [G loss: 5.621724, adv: 0.981432, recon: 0.160474, id: 0.299990] time: 0:01:20.221238 \n",
      "[Epoch 0/300] [Batch 268/292] [D loss: 0.086420, acc:  94%] [G loss: 5.669022, adv: 1.037141, recon: 0.157029, id: 0.293197] time: 0:01:20.427299 \n",
      "[Epoch 0/300] [Batch 269/292] [D loss: 0.109258, acc:  90%] [G loss: 5.211602, adv: 0.977666, recon: 0.145038, id: 0.213695] time: 0:01:20.646558 \n",
      "[Epoch 0/300] [Batch 270/292] [D loss: 0.117605, acc:  87%] [G loss: 5.932770, adv: 1.102358, recon: 0.168362, id: 0.161418] time: 0:01:20.889314 \n",
      "[Epoch 0/300] [Batch 271/292] [D loss: 0.239531, acc:  62%] [G loss: 5.542697, adv: 1.056210, recon: 0.151902, id: 0.233149] time: 0:01:21.121855 \n",
      "[Epoch 0/300] [Batch 272/292] [D loss: 0.177656, acc:  76%] [G loss: 5.845510, adv: 1.174343, recon: 0.156681, id: 0.240375] time: 0:01:21.310726 \n",
      "[Epoch 0/300] [Batch 273/292] [D loss: 0.144682, acc:  80%] [G loss: 6.120715, adv: 1.129910, recon: 0.171952, id: 0.196521] time: 0:01:21.514131 \n",
      "[Epoch 0/300] [Batch 274/292] [D loss: 0.108891, acc:  87%] [G loss: 6.055770, adv: 1.300499, recon: 0.158931, id: 0.127216] time: 0:01:21.718441 \n",
      "[Epoch 0/300] [Batch 275/292] [D loss: 0.119923, acc:  86%] [G loss: 7.214255, adv: 1.270472, recon: 0.207522, id: 0.308701] time: 0:01:21.938436 \n",
      "[Epoch 0/300] [Batch 276/292] [D loss: 0.112099, acc:  87%] [G loss: 6.715236, adv: 1.272496, recon: 0.189457, id: 0.215087] time: 0:01:22.131777 \n",
      "[Epoch 0/300] [Batch 277/292] [D loss: 0.080279, acc:  93%] [G loss: 4.880191, adv: 0.708470, recon: 0.154305, id: 0.222342] time: 0:01:22.349491 \n",
      "[Epoch 0/300] [Batch 278/292] [D loss: 0.120426, acc:  83%] [G loss: 5.429012, adv: 1.192964, recon: 0.135625, id: 0.144339] time: 0:01:22.557909 \n",
      "[Epoch 0/300] [Batch 279/292] [D loss: 0.093288, acc:  90%] [G loss: 5.245220, adv: 0.590719, recon: 0.181370, id: 0.267978] time: 0:01:22.755205 \n",
      "[Epoch 0/300] [Batch 280/292] [D loss: 0.174204, acc:  77%] [G loss: 6.093173, adv: 1.178254, recon: 0.171313, id: 0.139569] time: 0:01:22.950167 \n",
      "[Epoch 0/300] [Batch 281/292] [D loss: 0.363938, acc:  53%] [G loss: 6.636505, adv: 1.260666, recon: 0.190451, id: 0.140233] time: 0:01:23.127328 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/300] [Batch 282/292] [D loss: 0.130509, acc:  77%] [G loss: 5.962893, adv: 1.022532, recon: 0.176975, id: 0.136261] time: 0:01:23.303142 \n",
      "[Epoch 0/300] [Batch 283/292] [D loss: 0.085262, acc:  94%] [G loss: 5.640471, adv: 1.012132, recon: 0.164432, id: 0.138765] time: 0:01:23.476139 \n",
      "[Epoch 0/300] [Batch 284/292] [D loss: 0.092557, acc:  94%] [G loss: 5.330188, adv: 1.007268, recon: 0.148551, id: 0.212198] time: 0:01:23.694501 \n",
      "[Epoch 0/300] [Batch 285/292] [D loss: 0.053514, acc:  96%] [G loss: 5.497935, adv: 0.972916, recon: 0.161841, id: 0.147992] time: 0:01:23.878299 \n",
      "[Epoch 0/300] [Batch 286/292] [D loss: 0.062517, acc:  94%] [G loss: 5.233222, adv: 0.977251, recon: 0.145617, id: 0.143743] time: 0:01:24.055068 \n",
      "[Epoch 0/300] [Batch 287/292] [D loss: 0.226257, acc:  59%] [G loss: 5.941681, adv: 1.057329, recon: 0.176151, id: 0.149484] time: 0:01:24.271726 \n",
      "[Epoch 0/300] [Batch 288/292] [D loss: 0.089874, acc:  92%] [G loss: 5.732372, adv: 1.008259, recon: 0.168365, id: 0.128406] time: 0:01:24.486486 \n",
      "[Epoch 0/300] [Batch 289/292] [D loss: 0.092606, acc:  88%] [G loss: 5.327754, adv: 1.039719, recon: 0.145762, id: 0.122759] time: 0:01:24.678174 \n",
      "[Epoch 0/300] [Batch 290/292] [D loss: 0.233699, acc:  63%] [G loss: 5.306350, adv: 0.981349, recon: 0.154610, id: 0.129364] time: 0:01:24.856540 \n",
      "[Epoch 1/300] [Batch 0/292] [D loss: 0.184373, acc:  70%] [G loss: 5.919737, adv: 1.122171, recon: 0.169788, id: 0.153978] time: 0:01:25.055391 \n",
      "[Epoch 1/300] [Batch 1/292] [D loss: 0.114621, acc:  87%] [G loss: 6.466465, adv: 1.091522, recon: 0.197518, id: 0.121869] time: 0:01:25.548964 \n",
      "[Epoch 1/300] [Batch 2/292] [D loss: 0.193104, acc:  65%] [G loss: 5.175657, adv: 1.046123, recon: 0.140174, id: 0.144638] time: 0:01:25.751878 \n",
      "[Epoch 1/300] [Batch 3/292] [D loss: 0.121830, acc:  84%] [G loss: 5.827341, adv: 1.214768, recon: 0.153433, id: 0.127409] time: 0:01:25.948420 \n",
      "[Epoch 1/300] [Batch 4/292] [D loss: 0.200300, acc:  68%] [G loss: 6.492722, adv: 1.400388, recon: 0.169057, id: 0.129543] time: 0:01:26.172812 \n",
      "[Epoch 1/300] [Batch 5/292] [D loss: 0.244556, acc:  66%] [G loss: 6.875753, adv: 1.701290, recon: 0.158274, id: 0.133374] time: 0:01:26.401124 \n",
      "[Epoch 1/300] [Batch 6/292] [D loss: 0.180683, acc:  75%] [G loss: 6.904295, adv: 1.211426, recon: 0.204711, id: 0.151535] time: 0:01:26.592507 \n",
      "[Epoch 1/300] [Batch 7/292] [D loss: 0.252179, acc:  61%] [G loss: 5.553061, adv: 1.119832, recon: 0.149774, id: 0.153831] time: 0:01:26.790184 \n",
      "[Epoch 1/300] [Batch 8/292] [D loss: 0.213389, acc:  67%] [G loss: 6.119423, adv: 1.164342, recon: 0.172831, id: 0.128021] time: 0:01:27.007399 \n",
      "[Epoch 1/300] [Batch 9/292] [D loss: 0.159787, acc:  75%] [G loss: 5.578299, adv: 1.016540, recon: 0.159406, id: 0.162086] time: 0:01:27.213379 \n",
      "[Epoch 1/300] [Batch 10/292] [D loss: 0.252626, acc:  58%] [G loss: 6.460607, adv: 1.469558, recon: 0.160909, id: 0.143932] time: 0:01:27.412333 \n",
      "[Epoch 1/300] [Batch 11/292] [D loss: 0.121019, acc:  86%] [G loss: 6.614661, adv: 1.215813, recon: 0.185348, id: 0.194048] time: 0:01:27.605911 \n",
      "[Epoch 1/300] [Batch 12/292] [D loss: 0.200315, acc:  72%] [G loss: 5.644033, adv: 1.046775, recon: 0.157090, id: 0.178824] time: 0:01:27.804754 \n",
      "[Epoch 1/300] [Batch 13/292] [D loss: 0.206615, acc:  70%] [G loss: 6.061253, adv: 1.266704, recon: 0.162639, id: 0.136904] time: 0:01:28.003449 \n",
      "[Epoch 1/300] [Batch 14/292] [D loss: 0.245595, acc:  59%] [G loss: 5.708269, adv: 1.122475, recon: 0.158317, id: 0.145131] time: 0:01:28.195346 \n",
      "[Epoch 1/300] [Batch 15/292] [D loss: 0.186637, acc:  70%] [G loss: 5.748652, adv: 0.998907, recon: 0.171067, id: 0.169132] time: 0:01:28.392641 \n",
      "[Epoch 1/300] [Batch 16/292] [D loss: 0.155362, acc:  79%] [G loss: 5.849922, adv: 0.967872, recon: 0.172441, id: 0.234776] time: 0:01:28.584538 \n",
      "[Epoch 1/300] [Batch 17/292] [D loss: 0.227539, acc:  62%] [G loss: 5.689081, adv: 1.202058, recon: 0.148063, id: 0.137779] time: 0:01:28.789163 \n",
      "[Epoch 1/300] [Batch 18/292] [D loss: 0.199683, acc:  68%] [G loss: 5.674373, adv: 1.166536, recon: 0.151195, id: 0.147595] time: 0:01:28.984853 \n",
      "[Epoch 1/300] [Batch 19/292] [D loss: 0.100026, acc:  90%] [G loss: 6.124255, adv: 1.111937, recon: 0.175087, id: 0.143085] time: 0:01:29.185238 \n",
      "[Epoch 1/300] [Batch 20/292] [D loss: 0.192245, acc:  73%] [G loss: 5.512573, adv: 1.111139, recon: 0.150782, id: 0.131936] time: 0:01:29.376596 \n",
      "[Epoch 1/300] [Batch 21/292] [D loss: 0.148882, acc:  80%] [G loss: 5.221446, adv: 0.873148, recon: 0.157116, id: 0.154004] time: 0:01:29.595138 \n",
      "[Epoch 1/300] [Batch 22/292] [D loss: 0.153699, acc:  78%] [G loss: 5.674894, adv: 1.289466, recon: 0.138841, id: 0.139630] time: 0:01:29.806282 \n",
      "[Epoch 1/300] [Batch 23/292] [D loss: 0.089577, acc:  89%] [G loss: 6.102964, adv: 1.071782, recon: 0.179373, id: 0.123328] time: 0:01:30.004868 \n",
      "[Epoch 1/300] [Batch 24/292] [D loss: 0.108478, acc:  88%] [G loss: 6.033724, adv: 1.185285, recon: 0.166837, id: 0.141861] time: 0:01:30.195519 \n",
      "[Epoch 1/300] [Batch 25/292] [D loss: 0.133854, acc:  81%] [G loss: 5.954572, adv: 1.148520, recon: 0.165505, id: 0.143002] time: 0:01:30.390806 \n",
      "[Epoch 1/300] [Batch 26/292] [D loss: 0.203529, acc:  67%] [G loss: 6.023664, adv: 1.463120, recon: 0.139514, id: 0.143849] time: 0:01:30.593636 \n",
      "[Epoch 1/300] [Batch 27/292] [D loss: 0.260221, acc:  61%] [G loss: 7.234033, adv: 1.589265, recon: 0.185700, id: 0.127695] time: 0:01:30.783070 \n",
      "[Epoch 1/300] [Batch 28/292] [D loss: 0.208964, acc:  72%] [G loss: 7.323562, adv: 1.586114, recon: 0.192984, id: 0.127664] time: 0:01:30.986882 \n",
      "[Epoch 1/300] [Batch 29/292] [D loss: 0.285050, acc:  65%] [G loss: 6.948301, adv: 1.446621, recon: 0.183547, id: 0.153104] time: 0:01:31.203885 \n",
      "[Epoch 1/300] [Batch 30/292] [D loss: 0.551851, acc:  38%] [G loss: 6.157476, adv: 1.424419, recon: 0.150431, id: 0.151491] time: 0:01:31.416874 \n",
      "[Epoch 1/300] [Batch 31/292] [D loss: 0.296749, acc:  63%] [G loss: 6.064353, adv: 1.172466, recon: 0.170551, id: 0.149760] time: 0:01:31.630561 \n",
      "[Epoch 1/300] [Batch 32/292] [D loss: 0.218068, acc:  63%] [G loss: 5.575232, adv: 0.995332, recon: 0.162698, id: 0.152787] time: 0:01:31.849567 \n",
      "[Epoch 1/300] [Batch 33/292] [D loss: 0.265043, acc:  57%] [G loss: 5.056007, adv: 0.934037, recon: 0.141102, id: 0.152029] time: 0:01:32.055191 \n",
      "[Epoch 1/300] [Batch 34/292] [D loss: 0.255484, acc:  57%] [G loss: 5.366791, adv: 1.016558, recon: 0.151229, id: 0.126512] time: 0:01:32.280088 \n",
      "[Epoch 1/300] [Batch 35/292] [D loss: 0.168875, acc:  73%] [G loss: 5.397241, adv: 0.984299, recon: 0.155835, id: 0.122816] time: 0:01:32.484371 \n",
      "[Epoch 1/300] [Batch 36/292] [D loss: 0.177738, acc:  70%] [G loss: 5.259580, adv: 1.080217, recon: 0.140958, id: 0.134170] time: 0:01:32.710469 \n",
      "[Epoch 1/300] [Batch 37/292] [D loss: 0.154187, acc:  74%] [G loss: 4.943285, adv: 0.925907, recon: 0.139651, id: 0.138745] time: 0:01:32.919386 \n",
      "[Epoch 1/300] [Batch 38/292] [D loss: 0.172530, acc:  71%] [G loss: 5.642996, adv: 1.256413, recon: 0.142525, id: 0.142899] time: 0:01:33.111586 \n",
      "[Epoch 1/300] [Batch 39/292] [D loss: 0.259770, acc:  55%] [G loss: 11.636798, adv: 4.038341, recon: 0.163026, id: 0.114845] time: 0:01:33.279513 \n",
      "[Epoch 1/300] [Batch 40/292] [D loss: 0.254347, acc:  64%] [G loss: 7.841121, adv: 1.338089, recon: 0.238590, id: 0.113303] time: 0:01:33.475447 \n",
      "[Epoch 1/300] [Batch 41/292] [D loss: 0.221603, acc:  74%] [G loss: 8.360381, adv: 1.508202, recon: 0.247774, id: 0.130226] time: 0:01:33.700951 \n",
      "[Epoch 1/300] [Batch 42/292] [D loss: 0.162710, acc:  82%] [G loss: 7.233282, adv: 1.277666, recon: 0.218216, id: 0.129100] time: 0:01:33.886269 \n",
      "[Epoch 1/300] [Batch 43/292] [D loss: 0.293859, acc:  52%] [G loss: 5.758396, adv: 0.934418, recon: 0.180937, id: 0.128412] time: 0:01:34.077837 \n",
      "[Epoch 1/300] [Batch 44/292] [D loss: 0.140991, acc:  82%] [G loss: 6.783456, adv: 1.066109, recon: 0.212131, id: 0.137564] time: 0:01:34.289247 \n",
      "[Epoch 1/300] [Batch 45/292] [D loss: 0.198278, acc:  63%] [G loss: 6.511940, adv: 0.870707, recon: 0.221582, id: 0.133062] time: 0:01:34.484336 \n",
      "[Epoch 1/300] [Batch 46/292] [D loss: 0.136378, acc:  84%] [G loss: 6.387247, adv: 1.412707, recon: 0.165566, id: 0.110530] time: 0:01:34.709033 \n",
      "[Epoch 1/300] [Batch 47/292] [D loss: 0.207140, acc:  74%] [G loss: 7.439351, adv: 1.418789, recon: 0.210852, id: 0.138795] time: 0:01:34.898811 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/300] [Batch 48/292] [D loss: 0.201859, acc:  69%] [G loss: 5.915864, adv: 1.109132, recon: 0.170819, id: 0.130539] time: 0:01:35.109234 \n",
      "[Epoch 1/300] [Batch 49/292] [D loss: 0.174615, acc:  72%] [G loss: 6.706832, adv: 1.254799, recon: 0.193829, id: 0.148110] time: 0:01:35.329319 \n",
      "[Epoch 1/300] [Batch 50/292] [D loss: 0.172651, acc:  76%] [G loss: 7.085110, adv: 1.454084, recon: 0.193075, id: 0.120435] time: 0:01:35.551053 \n",
      "[Epoch 1/300] [Batch 51/292] [D loss: 0.184704, acc:  77%] [G loss: 5.747579, adv: 1.148013, recon: 0.158940, id: 0.125450] time: 0:01:35.752807 \n",
      "[Epoch 1/300] [Batch 52/292] [D loss: 0.083135, acc:  90%] [G loss: 6.058927, adv: 1.061830, recon: 0.180907, id: 0.125878] time: 0:01:35.968074 \n",
      "[Epoch 1/300] [Batch 53/292] [D loss: 0.104776, acc:  89%] [G loss: 5.737554, adv: 0.968016, recon: 0.173943, id: 0.163647] time: 0:01:36.188478 \n",
      "[Epoch 1/300] [Batch 54/292] [D loss: 0.162376, acc:  77%] [G loss: 5.574038, adv: 1.206882, recon: 0.141208, id: 0.148999] time: 0:01:36.373903 \n",
      "[Epoch 1/300] [Batch 55/292] [D loss: 0.099738, acc:  90%] [G loss: 5.655589, adv: 1.006916, recon: 0.162557, id: 0.152166] time: 0:01:36.584131 \n",
      "[Epoch 1/300] [Batch 56/292] [D loss: 0.299709, acc:  58%] [G loss: 5.656289, adv: 1.131506, recon: 0.154559, id: 0.148820] time: 0:01:36.785664 \n",
      "[Epoch 1/300] [Batch 57/292] [D loss: 0.311331, acc:  54%] [G loss: 6.234162, adv: 1.139019, recon: 0.180527, id: 0.135258] time: 0:01:36.966059 \n",
      "[Epoch 1/300] [Batch 58/292] [D loss: 0.280092, acc:  57%] [G loss: 4.944464, adv: 0.763471, recon: 0.155721, id: 0.158309] time: 0:01:37.167321 \n",
      "[Epoch 1/300] [Batch 59/292] [D loss: 0.318152, acc:  64%] [G loss: 7.901980, adv: 1.885894, recon: 0.189442, id: 0.132458] time: 0:01:37.389181 \n",
      "[Epoch 1/300] [Batch 60/292] [D loss: 0.356139, acc:  66%] [G loss: 6.431174, adv: 1.112531, recon: 0.192372, id: 0.150489] time: 0:01:37.594059 \n",
      "[Epoch 1/300] [Batch 61/292] [D loss: 0.172600, acc:  70%] [G loss: 5.359163, adv: 1.036397, recon: 0.150189, id: 0.114690] time: 0:01:37.790323 \n",
      "[Epoch 1/300] [Batch 62/292] [D loss: 0.103524, acc:  91%] [G loss: 5.642615, adv: 0.940919, recon: 0.171267, id: 0.119266] time: 0:01:38.010015 \n",
      "[Epoch 1/300] [Batch 63/292] [D loss: 0.274785, acc:  55%] [G loss: 4.673550, adv: 0.907115, recon: 0.129938, id: 0.117103] time: 0:01:38.251121 \n",
      "[Epoch 1/300] [Batch 64/292] [D loss: 0.222995, acc:  61%] [G loss: 4.946851, adv: 0.934489, recon: 0.140695, id: 0.109175] time: 0:01:38.457926 \n",
      "[Epoch 1/300] [Batch 65/292] [D loss: 0.186543, acc:  66%] [G loss: 4.482031, adv: 0.762743, recon: 0.134094, id: 0.127178] time: 0:01:38.666645 \n",
      "[Epoch 1/300] [Batch 66/292] [D loss: 0.191878, acc:  67%] [G loss: 4.752117, adv: 0.843793, recon: 0.139401, id: 0.133559] time: 0:01:38.883242 \n",
      "[Epoch 1/300] [Batch 67/292] [D loss: 0.194147, acc:  67%] [G loss: 5.161992, adv: 0.831946, recon: 0.159859, id: 0.141680] time: 0:01:39.075242 \n",
      "[Epoch 1/300] [Batch 68/292] [D loss: 0.183350, acc:  68%] [G loss: 5.161485, adv: 0.914299, recon: 0.154245, id: 0.124769] time: 0:01:39.262282 \n",
      "[Epoch 1/300] [Batch 69/292] [D loss: 0.253825, acc:  53%] [G loss: 5.043240, adv: 0.908461, recon: 0.147837, id: 0.123473] time: 0:01:39.463195 \n",
      "[Epoch 1/300] [Batch 70/292] [D loss: 0.177104, acc:  68%] [G loss: 5.127332, adv: 0.935124, recon: 0.150484, id: 0.131291] time: 0:01:39.645303 \n",
      "[Epoch 1/300] [Batch 71/292] [D loss: 0.210830, acc:  70%] [G loss: 4.718745, adv: 0.819143, recon: 0.140357, id: 0.154267] time: 0:01:39.832115 \n",
      "[Epoch 1/300] [Batch 72/292] [D loss: 0.252229, acc:  58%] [G loss: 5.946756, adv: 1.160089, recon: 0.166439, id: 0.135176] time: 0:01:40.028519 \n",
      "[Epoch 1/300] [Batch 73/292] [D loss: 0.269920, acc:  56%] [G loss: 5.981056, adv: 1.173392, recon: 0.166454, id: 0.141567] time: 0:01:40.251422 \n",
      "[Epoch 1/300] [Batch 74/292] [D loss: 0.154358, acc:  75%] [G loss: 5.403640, adv: 0.819134, recon: 0.171653, id: 0.159451] time: 0:01:40.454269 \n",
      "[Epoch 1/300] [Batch 75/292] [D loss: 0.172617, acc:  72%] [G loss: 4.944027, adv: 0.911290, recon: 0.142508, id: 0.127537] time: 0:01:40.656989 \n",
      "[Epoch 1/300] [Batch 76/292] [D loss: 0.264659, acc:  56%] [G loss: 5.037488, adv: 0.847980, recon: 0.152106, id: 0.133215] time: 0:01:40.873964 \n",
      "[Epoch 1/300] [Batch 77/292] [D loss: 0.195943, acc:  67%] [G loss: 4.962543, adv: 0.871412, recon: 0.145482, id: 0.169482] time: 0:01:41.060931 \n",
      "[Epoch 1/300] [Batch 78/292] [D loss: 0.134798, acc:  82%] [G loss: 5.334459, adv: 0.886883, recon: 0.161546, id: 0.131008] time: 0:01:41.275369 \n",
      "[Epoch 1/300] [Batch 79/292] [D loss: 0.171427, acc:  72%] [G loss: 4.684387, adv: 0.904030, recon: 0.129762, id: 0.119160] time: 0:01:41.480142 \n",
      "[Epoch 1/300] [Batch 80/292] [D loss: 0.236270, acc:  57%] [G loss: 4.522935, adv: 0.952596, recon: 0.119246, id: 0.107851] time: 0:01:41.687806 \n",
      "[Epoch 1/300] [Batch 81/292] [D loss: 0.202609, acc:  67%] [G loss: 5.307336, adv: 1.188685, recon: 0.133227, id: 0.118861] time: 0:01:41.904977 \n",
      "[Epoch 1/300] [Batch 82/292] [D loss: 0.229439, acc:  59%] [G loss: 5.315997, adv: 1.129597, recon: 0.141002, id: 0.113323] time: 0:01:42.110726 \n",
      "[Epoch 1/300] [Batch 83/292] [D loss: 0.219045, acc:  69%] [G loss: 6.976082, adv: 1.509513, recon: 0.182687, id: 0.131785] time: 0:01:42.327034 \n",
      "[Epoch 1/300] [Batch 84/292] [D loss: 0.181076, acc:  73%] [G loss: 5.955402, adv: 1.104342, recon: 0.171753, id: 0.106748] time: 0:01:42.533635 \n",
      "[Epoch 1/300] [Batch 85/292] [D loss: 0.316808, acc:  48%] [G loss: 5.102030, adv: 1.045352, recon: 0.137942, id: 0.110982] time: 0:01:42.726789 \n",
      "[Epoch 1/300] [Batch 86/292] [D loss: 0.200652, acc:  64%] [G loss: 4.859962, adv: 0.920768, recon: 0.138355, id: 0.133573] time: 0:01:42.907890 \n",
      "[Epoch 1/300] [Batch 87/292] [D loss: 0.205434, acc:  62%] [G loss: 4.591905, adv: 0.822213, recon: 0.134674, id: 0.123261] time: 0:01:43.093278 \n",
      "[Epoch 1/300] [Batch 88/292] [D loss: 0.249692, acc:  57%] [G loss: 4.492564, adv: 0.667627, recon: 0.146441, id: 0.113720] time: 0:01:43.273535 \n",
      "[Epoch 1/300] [Batch 89/292] [D loss: 0.239348, acc:  59%] [G loss: 4.889671, adv: 0.992935, recon: 0.133420, id: 0.112079] time: 0:01:43.480277 \n",
      "[Epoch 1/300] [Batch 90/292] [D loss: 0.089621, acc:  91%] [G loss: 5.300767, adv: 0.795833, recon: 0.169669, id: 0.125455] time: 0:01:43.694176 \n",
      "[Epoch 1/300] [Batch 91/292] [D loss: 0.242604, acc:  58%] [G loss: 6.010061, adv: 1.432132, recon: 0.143042, id: 0.128000] time: 0:01:43.904577 \n",
      "[Epoch 1/300] [Batch 92/292] [D loss: 0.322750, acc:  46%] [G loss: 5.698421, adv: 0.904269, recon: 0.179366, id: 0.137993] time: 0:01:44.104819 \n",
      "[Epoch 1/300] [Batch 93/292] [D loss: 0.213800, acc:  62%] [G loss: 4.374564, adv: 0.750607, recon: 0.131027, id: 0.114893] time: 0:01:44.301708 \n",
      "[Epoch 1/300] [Batch 94/292] [D loss: 0.200796, acc:  65%] [G loss: 4.740993, adv: 0.820622, recon: 0.141419, id: 0.121645] time: 0:01:44.491955 \n",
      "[Epoch 1/300] [Batch 95/292] [D loss: 0.205265, acc:  65%] [G loss: 4.793066, adv: 0.907066, recon: 0.137680, id: 0.121083] time: 0:01:44.706111 \n",
      "[Epoch 1/300] [Batch 96/292] [D loss: 0.152275, acc:  77%] [G loss: 5.385257, adv: 0.805497, recon: 0.173164, id: 0.137930] time: 0:01:44.923787 \n",
      "[Epoch 1/300] [Batch 97/292] [D loss: 0.175270, acc:  69%] [G loss: 5.147522, adv: 0.960774, recon: 0.149959, id: 0.129321] time: 0:01:45.145658 \n",
      "[Epoch 1/300] [Batch 98/292] [D loss: 0.182135, acc:  72%] [G loss: 5.411294, adv: 1.062880, recon: 0.149965, id: 0.109825] time: 0:01:45.348022 \n",
      "[Epoch 1/300] [Batch 99/292] [D loss: 0.123677, acc:  83%] [G loss: 4.889723, adv: 0.698454, recon: 0.159148, id: 0.128573] time: 0:01:45.558112 \n",
      "[Epoch 1/300] [Batch 100/292] [D loss: 0.290150, acc:  57%] [G loss: 5.002587, adv: 0.991394, recon: 0.138245, id: 0.113815] time: 0:01:45.803295 \n",
      "[Epoch 1/300] [Batch 101/292] [D loss: 0.326118, acc:  52%] [G loss: 5.426524, adv: 1.066201, recon: 0.152732, id: 0.118351] time: 0:01:46.037955 \n",
      "[Epoch 1/300] [Batch 102/292] [D loss: 0.251312, acc:  58%] [G loss: 6.743227, adv: 1.681408, recon: 0.154825, id: 0.119988] time: 0:01:46.261872 \n",
      "[Epoch 1/300] [Batch 103/292] [D loss: 0.237229, acc:  71%] [G loss: 5.386649, adv: 0.848222, recon: 0.170740, id: 0.120643] time: 0:01:46.470076 \n",
      "[Epoch 1/300] [Batch 104/292] [D loss: 0.145902, acc:  79%] [G loss: 4.840688, adv: 0.888602, recon: 0.139699, id: 0.120149] time: 0:01:46.699986 \n",
      "[Epoch 1/300] [Batch 105/292] [D loss: 0.184697, acc:  71%] [G loss: 5.152635, adv: 0.768167, recon: 0.162346, id: 0.176280] time: 0:01:46.900042 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/300] [Batch 106/292] [D loss: 0.191328, acc:  70%] [G loss: 4.828078, adv: 0.935523, recon: 0.135241, id: 0.115695] time: 0:01:47.111243 \n",
      "[Epoch 1/300] [Batch 107/292] [D loss: 0.264610, acc:  50%] [G loss: 5.299912, adv: 1.104184, recon: 0.143258, id: 0.103131] time: 0:01:47.345054 \n",
      "[Epoch 1/300] [Batch 108/292] [D loss: 0.250104, acc:  52%] [G loss: 5.288158, adv: 0.927977, recon: 0.159552, id: 0.118735] time: 0:01:47.563119 \n",
      "[Epoch 1/300] [Batch 109/292] [D loss: 0.206717, acc:  65%] [G loss: 5.590505, adv: 0.968615, recon: 0.166871, id: 0.131206] time: 0:01:47.734756 \n",
      "[Epoch 1/300] [Batch 110/292] [D loss: 0.193834, acc:  65%] [G loss: 4.699661, adv: 0.831800, recon: 0.139724, id: 0.107497] time: 0:01:47.907768 \n",
      "[Epoch 1/300] [Batch 111/292] [D loss: 0.212726, acc:  63%] [G loss: 5.206713, adv: 0.875534, recon: 0.157792, id: 0.128444] time: 0:01:48.069985 \n",
      "[Epoch 1/300] [Batch 112/292] [D loss: 0.202783, acc:  62%] [G loss: 4.543928, adv: 0.829980, recon: 0.131961, id: 0.124289] time: 0:01:48.271464 \n",
      "[Epoch 1/300] [Batch 113/292] [D loss: 0.185516, acc:  71%] [G loss: 5.573992, adv: 1.221424, recon: 0.143338, id: 0.132565] time: 0:01:48.459817 \n",
      "[Epoch 1/300] [Batch 114/292] [D loss: 0.273431, acc:  55%] [G loss: 11.053476, adv: 3.696362, recon: 0.168864, id: 0.108219] time: 0:01:48.635451 \n",
      "[Epoch 1/300] [Batch 115/292] [D loss: 0.601542, acc:  46%] [G loss: 10.660443, adv: 3.222349, recon: 0.198489, id: 0.121043] time: 0:01:48.812345 \n",
      "[Epoch 1/300] [Batch 116/292] [D loss: 2.951019, acc:  50%] [G loss: 9.346598, adv: 2.754099, recon: 0.176698, id: 0.125721] time: 0:01:49.001395 \n",
      "[Epoch 1/300] [Batch 117/292] [D loss: 1.161019, acc:  50%] [G loss: 5.171747, adv: 0.843554, recon: 0.157619, id: 0.124000] time: 0:01:49.184746 \n",
      "[Epoch 1/300] [Batch 118/292] [D loss: 0.485124, acc:  32%] [G loss: 4.605051, adv: 0.759458, recon: 0.141245, id: 0.115401] time: 0:01:49.370401 \n",
      "[Epoch 1/300] [Batch 119/292] [D loss: 0.562852, acc:  15%] [G loss: 4.985579, adv: 0.730516, recon: 0.162566, id: 0.151760] time: 0:01:49.571441 \n",
      "[Epoch 1/300] [Batch 120/292] [D loss: 0.405549, acc:  47%] [G loss: 4.495472, adv: 0.686870, recon: 0.140285, id: 0.134775] time: 0:01:49.772928 \n",
      "[Epoch 1/300] [Batch 121/292] [D loss: 0.414187, acc:  29%] [G loss: 4.492117, adv: 0.724267, recon: 0.138291, id: 0.122507] time: 0:01:49.972467 \n",
      "[Epoch 1/300] [Batch 122/292] [D loss: 0.518387, acc:  15%] [G loss: 4.575509, adv: 0.663459, recon: 0.149696, id: 0.120931] time: 0:01:50.204152 \n",
      "[Epoch 1/300] [Batch 123/292] [D loss: 0.442179, acc:  23%] [G loss: 4.028712, adv: 0.652741, recon: 0.124258, id: 0.123637] time: 0:01:50.427099 \n",
      "[Epoch 1/300] [Batch 124/292] [D loss: 0.412187, acc:  29%] [G loss: 4.073621, adv: 0.621745, recon: 0.127958, id: 0.117673] time: 0:01:50.617246 \n",
      "[Epoch 1/300] [Batch 125/292] [D loss: 0.416250, acc:  31%] [G loss: 4.035842, adv: 0.651916, recon: 0.122897, id: 0.137059] time: 0:01:50.812190 \n",
      "[Epoch 1/300] [Batch 126/292] [D loss: 0.388690, acc:  31%] [G loss: 4.074419, adv: 0.601398, recon: 0.129845, id: 0.111803] time: 0:01:50.993249 \n",
      "[Epoch 1/300] [Batch 127/292] [D loss: 0.430396, acc:  25%] [G loss: 4.465791, adv: 0.665818, recon: 0.144318, id: 0.119054] time: 0:01:51.208044 \n",
      "[Epoch 1/300] [Batch 128/292] [D loss: 0.309290, acc:  44%] [G loss: 4.603013, adv: 0.697357, recon: 0.144318, id: 0.166586] time: 0:01:51.414488 \n",
      "[Epoch 1/300] [Batch 129/292] [D loss: 0.367959, acc:  39%] [G loss: 4.729363, adv: 0.666906, recon: 0.149376, id: 0.118995] time: 0:01:51.615607 \n",
      "[Epoch 1/300] [Batch 130/292] [D loss: 0.435346, acc:  31%] [G loss: 3.923644, adv: 0.579392, recon: 0.124698, id: 0.127767] time: 0:01:51.832262 \n",
      "[Epoch 1/300] [Batch 131/292] [D loss: 0.394685, acc:  28%] [G loss: 4.065039, adv: 0.666642, recon: 0.124710, id: 0.118688] time: 0:01:52.016996 \n",
      "[Epoch 1/300] [Batch 132/292] [D loss: 0.381243, acc:  33%] [G loss: 3.888248, adv: 0.623863, recon: 0.119509, id: 0.116910] time: 0:01:52.252014 \n",
      "[Epoch 1/300] [Batch 133/292] [D loss: 0.369102, acc:  32%] [G loss: 4.506010, adv: 0.659905, recon: 0.146511, id: 0.116108] time: 0:01:52.483791 \n",
      "[Epoch 1/300] [Batch 134/292] [D loss: 0.383732, acc:  29%] [G loss: 4.846658, adv: 0.617031, recon: 0.164162, id: 0.150906] time: 0:01:52.697521 \n",
      "[Epoch 1/300] [Batch 135/292] [D loss: 0.460070, acc:  21%] [G loss: 3.912808, adv: 0.559361, recon: 0.127140, id: 0.119089] time: 0:01:52.908379 \n",
      "[Epoch 1/300] [Batch 136/292] [D loss: 0.353090, acc:  31%] [G loss: 3.790714, adv: 0.605988, recon: 0.116260, id: 0.129748] time: 0:01:53.121809 \n",
      "[Epoch 1/300] [Batch 137/292] [D loss: 0.365638, acc:  28%] [G loss: 3.822424, adv: 0.574314, recon: 0.121378, id: 0.112138] time: 0:01:53.362833 \n",
      "[Epoch 1/300] [Batch 138/292] [D loss: 0.377592, acc:  25%] [G loss: 3.624156, adv: 0.600777, recon: 0.110373, id: 0.106319] time: 0:01:53.583169 \n",
      "[Epoch 1/300] [Batch 139/292] [D loss: 0.340819, acc:  36%] [G loss: 4.180694, adv: 0.703425, recon: 0.125969, id: 0.136810] time: 0:01:53.795105 \n",
      "[Epoch 1/300] [Batch 140/292] [D loss: 0.366696, acc:  29%] [G loss: 3.813944, adv: 0.660271, recon: 0.113799, id: 0.112162] time: 0:01:54.013484 \n",
      "[Epoch 1/300] [Batch 141/292] [D loss: 0.409762, acc:  20%] [G loss: 3.883079, adv: 0.601480, recon: 0.123486, id: 0.098211] time: 0:01:54.253145 \n",
      "[Epoch 1/300] [Batch 142/292] [D loss: 0.357535, acc:  34%] [G loss: 4.209466, adv: 0.634198, recon: 0.134607, id: 0.126633] time: 0:01:54.489135 \n",
      "[Epoch 1/300] [Batch 143/292] [D loss: 0.365418, acc:  29%] [G loss: 4.113599, adv: 0.626317, recon: 0.130063, id: 0.126208] time: 0:01:54.703853 \n",
      "[Epoch 1/300] [Batch 144/292] [D loss: 0.330744, acc:  40%] [G loss: 3.885693, adv: 0.596829, recon: 0.122110, id: 0.106308] time: 0:01:54.917788 \n",
      "[Epoch 1/300] [Batch 145/292] [D loss: 0.313319, acc:  44%] [G loss: 4.091592, adv: 0.660190, recon: 0.123931, id: 0.110092] time: 0:01:55.125843 \n",
      "[Epoch 1/300] [Batch 146/292] [D loss: 0.342046, acc:  35%] [G loss: 4.342462, adv: 0.790182, recon: 0.125866, id: 0.130910] time: 0:01:55.339370 \n",
      "[Epoch 1/300] [Batch 147/292] [D loss: 0.272699, acc:  49%] [G loss: 4.727488, adv: 0.936700, recon: 0.128661, id: 0.113194] time: 0:01:55.514713 \n",
      "[Epoch 1/300] [Batch 148/292] [D loss: 0.346737, acc:  35%] [G loss: 6.395755, adv: 1.540761, recon: 0.150867, id: 0.141283] time: 0:01:55.685076 \n",
      "[Epoch 1/300] [Batch 149/292] [D loss: 0.248740, acc:  60%] [G loss: 5.952922, adv: 0.685536, recon: 0.212371, id: 0.112550] time: 0:01:55.888290 \n",
      "[Epoch 1/300] [Batch 150/292] [D loss: 0.243472, acc:  59%] [G loss: 4.639973, adv: 0.763625, recon: 0.141843, id: 0.127421] time: 0:01:56.083605 \n",
      "[Epoch 1/300] [Batch 151/292] [D loss: 0.299152, acc:  43%] [G loss: 3.995271, adv: 0.710774, recon: 0.118231, id: 0.114572] time: 0:01:56.284869 \n",
      "[Epoch 1/300] [Batch 152/292] [D loss: 0.197131, acc:  65%] [G loss: 5.142775, adv: 0.785561, recon: 0.163177, id: 0.121533] time: 0:01:56.478725 \n",
      "[Epoch 1/300] [Batch 153/292] [D loss: 0.382657, acc:  28%] [G loss: 3.926439, adv: 0.621179, recon: 0.123202, id: 0.106129] time: 0:01:56.665093 \n",
      "[Epoch 1/300] [Batch 154/292] [D loss: 0.374259, acc:  29%] [G loss: 3.876083, adv: 0.617768, recon: 0.121332, id: 0.118195] time: 0:01:56.843146 \n",
      "[Epoch 1/300] [Batch 155/292] [D loss: 0.298030, acc:  51%] [G loss: 4.511642, adv: 0.634901, recon: 0.148893, id: 0.134405] time: 0:01:57.030846 \n",
      "[Epoch 1/300] [Batch 156/292] [D loss: 0.238429, acc:  58%] [G loss: 4.560833, adv: 0.747564, recon: 0.138771, id: 0.129360] time: 0:01:57.247377 \n",
      "[Epoch 1/300] [Batch 157/292] [D loss: 0.313225, acc:  40%] [G loss: 4.601750, adv: 0.877243, recon: 0.130566, id: 0.124836] time: 0:01:57.457074 \n",
      "[Epoch 1/300] [Batch 158/292] [D loss: 0.220710, acc:  64%] [G loss: 5.378652, adv: 0.783049, recon: 0.172706, id: 0.105828] time: 0:01:57.684447 \n",
      "[Epoch 1/300] [Batch 159/292] [D loss: 0.343341, acc:  35%] [G loss: 4.082942, adv: 0.572663, recon: 0.130912, id: 0.171279] time: 0:01:57.899524 \n",
      "[Epoch 1/300] [Batch 160/292] [D loss: 0.276829, acc:  51%] [G loss: 4.435462, adv: 0.705517, recon: 0.137859, id: 0.103896] time: 0:01:58.106203 \n",
      "[Epoch 1/300] [Batch 161/292] [D loss: 0.311620, acc:  38%] [G loss: 3.905333, adv: 0.569519, recon: 0.127292, id: 0.100235] time: 0:01:58.291465 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/300] [Batch 162/292] [D loss: 0.267997, acc:  52%] [G loss: 4.337101, adv: 0.647574, recon: 0.139207, id: 0.115876] time: 0:01:58.494721 \n",
      "[Epoch 1/300] [Batch 163/292] [D loss: 0.346048, acc:  35%] [G loss: 4.580362, adv: 0.772304, recon: 0.139296, id: 0.140368] time: 0:01:58.661551 \n",
      "[Epoch 1/300] [Batch 164/292] [D loss: 0.236666, acc:  55%] [G loss: 6.520549, adv: 0.716399, recon: 0.226724, id: 0.101177] time: 0:01:58.832242 \n",
      "[Epoch 1/300] [Batch 165/292] [D loss: 0.231864, acc:  63%] [G loss: 5.705154, adv: 0.789754, recon: 0.187792, id: 0.132785] time: 0:01:59.007539 \n",
      "[Epoch 1/300] [Batch 166/292] [D loss: 0.296853, acc:  52%] [G loss: 4.897995, adv: 0.783241, recon: 0.151384, id: 0.150358] time: 0:01:59.226009 \n",
      "[Epoch 1/300] [Batch 167/292] [D loss: 0.268215, acc:  56%] [G loss: 4.921472, adv: 0.702560, recon: 0.161400, id: 0.119709] time: 0:01:59.434254 \n",
      "[Epoch 1/300] [Batch 168/292] [D loss: 0.204244, acc:  64%] [G loss: 4.994655, adv: 0.744635, recon: 0.159801, id: 0.121195] time: 0:01:59.628547 \n",
      "[Epoch 1/300] [Batch 169/292] [D loss: 0.173518, acc:  72%] [G loss: 4.988863, adv: 0.830003, recon: 0.150307, id: 0.096671] time: 0:01:59.828823 \n",
      "[Epoch 1/300] [Batch 170/292] [D loss: 0.336124, acc:  42%] [G loss: 4.139274, adv: 0.730229, recon: 0.122222, id: 0.114169] time: 0:02:00.025671 \n",
      "[Epoch 1/300] [Batch 171/292] [D loss: 0.315490, acc:  44%] [G loss: 3.819386, adv: 0.634314, recon: 0.115361, id: 0.102526] time: 0:02:00.221419 \n",
      "[Epoch 1/300] [Batch 172/292] [D loss: 0.249990, acc:  56%] [G loss: 4.605824, adv: 0.674067, recon: 0.147447, id: 0.110459] time: 0:02:00.445479 \n",
      "[Epoch 1/300] [Batch 173/292] [D loss: 0.172410, acc:  74%] [G loss: 4.560819, adv: 0.790017, recon: 0.134168, id: 0.129335] time: 0:02:00.663270 \n",
      "[Epoch 1/300] [Batch 174/292] [D loss: 0.301413, acc:  45%] [G loss: 4.354743, adv: 0.840740, recon: 0.123504, id: 0.095040] time: 0:02:00.856829 \n",
      "[Epoch 1/300] [Batch 175/292] [D loss: 0.233738, acc:  56%] [G loss: 4.987370, adv: 0.880538, recon: 0.147995, id: 0.101357] time: 0:02:01.053777 \n",
      "[Epoch 1/300] [Batch 176/292] [D loss: 0.346256, acc:  38%] [G loss: 4.325936, adv: 0.703936, recon: 0.132830, id: 0.109290] time: 0:02:01.243827 \n",
      "[Epoch 1/300] [Batch 177/292] [D loss: 0.286957, acc:  47%] [G loss: 4.470483, adv: 0.822346, recon: 0.128237, id: 0.130930] time: 0:02:01.443101 \n",
      "[Epoch 1/300] [Batch 178/292] [D loss: 0.277594, acc:  51%] [G loss: 5.738311, adv: 1.202583, recon: 0.152165, id: 0.132048] time: 0:02:01.638336 \n",
      "[Epoch 1/300] [Batch 179/292] [D loss: 0.321609, acc:  44%] [G loss: 4.762922, adv: 0.818666, recon: 0.141915, id: 0.133935] time: 0:02:01.839489 \n",
      "[Epoch 1/300] [Batch 180/292] [D loss: 0.238119, acc:  59%] [G loss: 4.626614, adv: 0.727818, recon: 0.145733, id: 0.112589] time: 0:02:02.012736 \n",
      "[Epoch 1/300] [Batch 181/292] [D loss: 0.174799, acc:  73%] [G loss: 4.826846, adv: 0.782804, recon: 0.147738, id: 0.144606] time: 0:02:02.210257 \n",
      "[Epoch 1/300] [Batch 182/292] [D loss: 0.240717, acc:  65%] [G loss: 5.741121, adv: 1.125989, recon: 0.158835, id: 0.131593] time: 0:02:02.429499 \n",
      "[Epoch 1/300] [Batch 183/292] [D loss: 0.259218, acc:  56%] [G loss: 4.647598, adv: 0.773391, recon: 0.137201, id: 0.129206] time: 0:02:02.620942 \n",
      "[Epoch 1/300] [Batch 184/292] [D loss: 0.297745, acc:  50%] [G loss: 5.047756, adv: 0.945975, recon: 0.140679, id: 0.155190] time: 0:02:02.857649 \n",
      "[Epoch 1/300] [Batch 185/292] [D loss: 0.246573, acc:  65%] [G loss: 4.189238, adv: 0.736941, recon: 0.122118, id: 0.138411] time: 0:02:03.067978 \n",
      "[Epoch 1/300] [Batch 186/292] [D loss: 0.320490, acc:  39%] [G loss: 4.328671, adv: 0.722843, recon: 0.132274, id: 0.119020] time: 0:02:03.246836 \n",
      "[Epoch 1/300] [Batch 187/292] [D loss: 0.302108, acc:  47%] [G loss: 4.228186, adv: 0.744469, recon: 0.125130, id: 0.108764] time: 0:02:03.426420 \n",
      "[Epoch 1/300] [Batch 188/292] [D loss: 0.309808, acc:  51%] [G loss: 4.504359, adv: 0.679020, recon: 0.142528, id: 0.119485] time: 0:02:03.606398 \n",
      "[Epoch 1/300] [Batch 189/292] [D loss: 0.281263, acc:  49%] [G loss: 3.906375, adv: 0.678300, recon: 0.114669, id: 0.100181] time: 0:02:03.781275 \n",
      "[Epoch 1/300] [Batch 190/292] [D loss: 0.244571, acc:  55%] [G loss: 4.231257, adv: 0.724324, recon: 0.126167, id: 0.111408] time: 0:02:03.944309 \n",
      "[Epoch 1/300] [Batch 191/292] [D loss: 0.194279, acc:  70%] [G loss: 4.515762, adv: 0.817971, recon: 0.129691, id: 0.134239] time: 0:02:04.143465 \n",
      "[Epoch 1/300] [Batch 192/292] [D loss: 0.183933, acc:  74%] [G loss: 4.733240, adv: 0.883856, recon: 0.134093, id: 0.121804] time: 0:02:04.356024 \n",
      "[Epoch 1/300] [Batch 193/292] [D loss: 0.146210, acc:  81%] [G loss: 5.180948, adv: 1.076183, recon: 0.137180, id: 0.146470] time: 0:02:04.539826 \n",
      "[Epoch 1/300] [Batch 194/292] [D loss: 0.197950, acc:  70%] [G loss: 5.354550, adv: 1.320888, recon: 0.124262, id: 0.095123] time: 0:02:04.755908 \n",
      "[Epoch 1/300] [Batch 195/292] [D loss: 0.183349, acc:  73%] [G loss: 9.035193, adv: 2.950610, recon: 0.143418, id: 0.109975] time: 0:02:04.957407 \n",
      "[Epoch 1/300] [Batch 196/292] [D loss: 0.185020, acc:  74%] [G loss: 5.950114, adv: 0.782922, recon: 0.202777, id: 0.137002] time: 0:02:05.139162 \n",
      "[Epoch 1/300] [Batch 197/292] [D loss: 0.241741, acc:  62%] [G loss: 6.283385, adv: 1.201545, recon: 0.181444, id: 0.112109] time: 0:02:05.328770 \n",
      "[Epoch 1/300] [Batch 198/292] [D loss: 0.212840, acc:  69%] [G loss: 5.926339, adv: 1.118076, recon: 0.172618, id: 0.123892] time: 0:02:05.529914 \n",
      "[Epoch 1/300] [Batch 199/292] [D loss: 0.206453, acc:  68%] [G loss: 6.483467, adv: 1.090117, recon: 0.200201, id: 0.164147] time: 0:02:05.720300 \n",
      "[Epoch 1/300] [Batch 200/292] [D loss: 0.145885, acc:  81%] [G loss: 7.730574, adv: 1.537687, recon: 0.213879, id: 0.112405] time: 0:02:05.906523 \n",
      "[Epoch 1/300] [Batch 201/292] [D loss: 0.139113, acc:  88%] [G loss: 6.896125, adv: 1.101998, recon: 0.218492, id: 0.120282] time: 0:02:06.373387 \n",
      "[Epoch 1/300] [Batch 202/292] [D loss: 0.104974, acc:  89%] [G loss: 6.363531, adv: 1.058072, recon: 0.195587, id: 0.111054] time: 0:02:06.554193 \n",
      "[Epoch 1/300] [Batch 203/292] [D loss: 0.210388, acc:  65%] [G loss: 6.231545, adv: 1.034127, recon: 0.194517, id: 0.119147] time: 0:02:06.728605 \n",
      "[Epoch 1/300] [Batch 204/292] [D loss: 0.238760, acc:  59%] [G loss: 5.378042, adv: 1.014218, recon: 0.156158, id: 0.109629] time: 0:02:06.914415 \n",
      "[Epoch 1/300] [Batch 205/292] [D loss: 0.180047, acc:  73%] [G loss: 6.060352, adv: 0.973188, recon: 0.189715, id: 0.121434] time: 0:02:07.093425 \n",
      "[Epoch 1/300] [Batch 206/292] [D loss: 0.183030, acc:  72%] [G loss: 5.456520, adv: 1.011969, recon: 0.159295, id: 0.103178] time: 0:02:07.267560 \n",
      "[Epoch 1/300] [Batch 207/292] [D loss: 0.167717, acc:  74%] [G loss: 4.663270, adv: 0.958915, recon: 0.125774, id: 0.107333] time: 0:02:07.445738 \n",
      "[Epoch 1/300] [Batch 208/292] [D loss: 0.074143, acc:  93%] [G loss: 5.241967, adv: 1.101536, recon: 0.138760, id: 0.101966] time: 0:02:07.657577 \n",
      "[Epoch 1/300] [Batch 209/292] [D loss: 0.096571, acc:  89%] [G loss: 4.925703, adv: 0.586826, recon: 0.172979, id: 0.102058] time: 0:02:07.858414 \n",
      "[Epoch 1/300] [Batch 210/292] [D loss: 0.111726, acc:  85%] [G loss: 4.834683, adv: 0.947536, recon: 0.135951, id: 0.106580] time: 0:02:08.042374 \n",
      "[Epoch 1/300] [Batch 211/292] [D loss: 0.082374, acc:  94%] [G loss: 5.220791, adv: 0.941895, recon: 0.151878, id: 0.139911] time: 0:02:08.245432 \n",
      "[Epoch 1/300] [Batch 212/292] [D loss: 0.178670, acc:  74%] [G loss: 6.525748, adv: 1.339441, recon: 0.179725, id: 0.117121] time: 0:02:08.453438 \n",
      "[Epoch 1/300] [Batch 213/292] [D loss: 0.262175, acc:  56%] [G loss: 7.530034, adv: 1.977884, recon: 0.162511, id: 0.133364] time: 0:02:08.688996 \n",
      "[Epoch 1/300] [Batch 214/292] [D loss: 0.281982, acc:  62%] [G loss: 9.658916, adv: 2.037877, recon: 0.260294, id: 0.217101] time: 0:02:08.918656 \n",
      "[Epoch 1/300] [Batch 215/292] [D loss: 0.202334, acc:  71%] [G loss: 6.400840, adv: 0.999704, recon: 0.200874, id: 0.189601] time: 0:02:09.151174 \n",
      "[Epoch 1/300] [Batch 216/292] [D loss: 0.067377, acc:  94%] [G loss: 6.462422, adv: 1.003380, recon: 0.202976, id: 0.140291] time: 0:02:09.341852 \n",
      "[Epoch 1/300] [Batch 217/292] [D loss: 0.194323, acc:  70%] [G loss: 5.157045, adv: 0.946801, recon: 0.149502, id: 0.131763] time: 0:02:09.551302 \n",
      "[Epoch 1/300] [Batch 218/292] [D loss: 0.152004, acc:  80%] [G loss: 4.767246, adv: 0.971868, recon: 0.128085, id: 0.108651] time: 0:02:09.743591 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/300] [Batch 219/292] [D loss: 0.227866, acc:  61%] [G loss: 5.055833, adv: 1.077453, recon: 0.134132, id: 0.101818] time: 0:02:09.921969 \n",
      "[Epoch 1/300] [Batch 220/292] [D loss: 0.178613, acc:  71%] [G loss: 4.938539, adv: 1.074035, recon: 0.126968, id: 0.125667] time: 0:02:10.123402 \n",
      "[Epoch 1/300] [Batch 221/292] [D loss: 0.198366, acc:  70%] [G loss: 4.951293, adv: 0.836709, recon: 0.151181, id: 0.113142] time: 0:02:10.344761 \n",
      "[Epoch 1/300] [Batch 222/292] [D loss: 0.068003, acc:  94%] [G loss: 5.644023, adv: 0.991099, recon: 0.167893, id: 0.143043] time: 0:02:10.587604 \n",
      "[Epoch 1/300] [Batch 223/292] [D loss: 0.193167, acc:  69%] [G loss: 5.943541, adv: 1.128646, recon: 0.173144, id: 0.126319] time: 0:02:10.787777 \n",
      "[Epoch 1/300] [Batch 224/292] [D loss: 0.179761, acc:  75%] [G loss: 5.396333, adv: 1.092372, recon: 0.149722, id: 0.090680] time: 0:02:10.981510 \n",
      "[Epoch 1/300] [Batch 225/292] [D loss: 0.197964, acc:  67%] [G loss: 4.976152, adv: 0.891323, recon: 0.146094, id: 0.123598] time: 0:02:11.173198 \n",
      "[Epoch 1/300] [Batch 226/292] [D loss: 0.098238, acc:  89%] [G loss: 5.193886, adv: 0.941537, recon: 0.150910, id: 0.119980] time: 0:02:11.386132 \n",
      "[Epoch 1/300] [Batch 227/292] [D loss: 0.148978, acc:  78%] [G loss: 5.463977, adv: 1.193362, recon: 0.140938, id: 0.106400] time: 0:02:11.560275 \n",
      "[Epoch 1/300] [Batch 228/292] [D loss: 0.091163, acc:  89%] [G loss: 4.858471, adv: 1.074090, recon: 0.123581, id: 0.090862] time: 0:02:11.739171 \n",
      "[Epoch 1/300] [Batch 229/292] [D loss: 0.169518, acc:  76%] [G loss: 6.311378, adv: 1.609754, recon: 0.142491, id: 0.106122] time: 0:02:11.920454 \n",
      "[Epoch 1/300] [Batch 230/292] [D loss: 0.180045, acc:  74%] [G loss: 6.100204, adv: 1.239252, recon: 0.168006, id: 0.104635] time: 0:02:12.085357 \n",
      "[Epoch 1/300] [Batch 231/292] [D loss: 0.138358, acc:  81%] [G loss: 5.447657, adv: 1.019375, recon: 0.156615, id: 0.108523] time: 0:02:12.285063 \n",
      "[Epoch 1/300] [Batch 232/292] [D loss: 0.253983, acc:  57%] [G loss: 4.988769, adv: 1.018052, recon: 0.137173, id: 0.104625] time: 0:02:12.490381 \n",
      "[Epoch 1/300] [Batch 233/292] [D loss: 0.194054, acc:  69%] [G loss: 5.263347, adv: 0.962650, recon: 0.152947, id: 0.129594] time: 0:02:12.696224 \n",
      "[Epoch 1/300] [Batch 234/292] [D loss: 0.169920, acc:  75%] [G loss: 4.188352, adv: 0.773473, recon: 0.121378, id: 0.098396] time: 0:02:12.887696 \n",
      "[Epoch 1/300] [Batch 235/292] [D loss: 0.176995, acc:  74%] [G loss: 5.187365, adv: 1.169196, recon: 0.130364, id: 0.096529] time: 0:02:13.089589 \n",
      "[Epoch 1/300] [Batch 236/292] [D loss: 0.211330, acc:  65%] [G loss: 4.578291, adv: 0.981230, recon: 0.120552, id: 0.089819] time: 0:02:13.325300 \n",
      "[Epoch 1/300] [Batch 237/292] [D loss: 0.140959, acc:  81%] [G loss: 5.630643, adv: 1.086589, recon: 0.159730, id: 0.114557] time: 0:02:13.552008 \n",
      "[Epoch 1/300] [Batch 238/292] [D loss: 0.149476, acc:  82%] [G loss: 5.229859, adv: 0.646071, recon: 0.179099, id: 0.105669] time: 0:02:13.767225 \n",
      "[Epoch 1/300] [Batch 239/292] [D loss: 0.272679, acc:  60%] [G loss: 4.959504, adv: 0.788647, recon: 0.143501, id: 0.357870] time: 0:02:13.968120 \n",
      "[Epoch 1/300] [Batch 240/292] [D loss: 0.304159, acc:  46%] [G loss: 4.869632, adv: 1.055693, recon: 0.127254, id: 0.101742] time: 0:02:14.174681 \n",
      "[Epoch 1/300] [Batch 241/292] [D loss: 0.148966, acc:  79%] [G loss: 5.724690, adv: 1.068015, recon: 0.165660, id: 0.126133] time: 0:02:14.386618 \n",
      "[Epoch 1/300] [Batch 242/292] [D loss: 0.206768, acc:  68%] [G loss: 4.993609, adv: 1.116958, recon: 0.126767, id: 0.107600] time: 0:02:14.576110 \n",
      "[Epoch 1/300] [Batch 243/292] [D loss: 0.375701, acc:  37%] [G loss: 6.567554, adv: 1.492374, recon: 0.166217, id: 0.124393] time: 0:02:14.789783 \n",
      "[Epoch 1/300] [Batch 244/292] [D loss: 0.450149, acc:  29%] [G loss: 4.978418, adv: 0.694580, recon: 0.167834, id: 0.121307] time: 0:02:15.015755 \n",
      "[Epoch 1/300] [Batch 245/292] [D loss: 0.140190, acc:  84%] [G loss: 5.134092, adv: 0.812621, recon: 0.159176, id: 0.115931] time: 0:02:15.248805 \n",
      "[Epoch 1/300] [Batch 246/292] [D loss: 0.090068, acc:  93%] [G loss: 5.166014, adv: 0.897420, recon: 0.155744, id: 0.104784] time: 0:02:15.438810 \n",
      "[Epoch 1/300] [Batch 247/292] [D loss: 0.190531, acc:  71%] [G loss: 4.554490, adv: 0.799841, recon: 0.133639, id: 0.108758] time: 0:02:15.625092 \n",
      "[Epoch 1/300] [Batch 248/292] [D loss: 0.194514, acc:  68%] [G loss: 4.819006, adv: 0.953484, recon: 0.132931, id: 0.124897] time: 0:02:15.823955 \n",
      "[Epoch 1/300] [Batch 249/292] [D loss: 0.190841, acc:  70%] [G loss: 4.936952, adv: 1.012073, recon: 0.134732, id: 0.091389] time: 0:02:16.013549 \n",
      "[Epoch 1/300] [Batch 250/292] [D loss: 0.194516, acc:  64%] [G loss: 4.537846, adv: 0.900575, recon: 0.126595, id: 0.096614] time: 0:02:16.199520 \n",
      "[Epoch 1/300] [Batch 251/292] [D loss: 0.169596, acc:  73%] [G loss: 4.724132, adv: 0.884477, recon: 0.135227, id: 0.127002] time: 0:02:16.396331 \n",
      "[Epoch 1/300] [Batch 252/292] [D loss: 0.196321, acc:  68%] [G loss: 4.896073, adv: 0.975642, recon: 0.131860, id: 0.138762] time: 0:02:16.591796 \n",
      "[Epoch 1/300] [Batch 253/292] [D loss: 0.156344, acc:  78%] [G loss: 5.060358, adv: 1.091672, recon: 0.132950, id: 0.091155] time: 0:02:16.781845 \n",
      "[Epoch 1/300] [Batch 254/292] [D loss: 0.166654, acc:  77%] [G loss: 5.300793, adv: 0.788950, recon: 0.174228, id: 0.128077] time: 0:02:16.971993 \n",
      "[Epoch 1/300] [Batch 255/292] [D loss: 0.201767, acc:  75%] [G loss: 5.073140, adv: 0.900037, recon: 0.153253, id: 0.087448] time: 0:02:17.149982 \n",
      "[Epoch 1/300] [Batch 256/292] [D loss: 0.157650, acc:  75%] [G loss: 4.492276, adv: 0.751159, recon: 0.137560, id: 0.127175] time: 0:02:17.345807 \n",
      "[Epoch 1/300] [Batch 257/292] [D loss: 0.128489, acc:  84%] [G loss: 4.870234, adv: 0.932489, recon: 0.138974, id: 0.117503] time: 0:02:17.520812 \n",
      "[Epoch 1/300] [Batch 258/292] [D loss: 0.130180, acc:  82%] [G loss: 4.545613, adv: 0.841927, recon: 0.132160, id: 0.111401] time: 0:02:17.715110 \n",
      "[Epoch 1/300] [Batch 259/292] [D loss: 0.085524, acc:  91%] [G loss: 5.844745, adv: 1.063252, recon: 0.171682, id: 0.106258] time: 0:02:17.902396 \n",
      "[Epoch 1/300] [Batch 260/292] [D loss: 0.161476, acc:  77%] [G loss: 5.231655, adv: 1.006470, recon: 0.137617, id: 0.334887] time: 0:02:18.100428 \n",
      "[Epoch 1/300] [Batch 261/292] [D loss: 0.094126, acc:  90%] [G loss: 4.638268, adv: 0.931044, recon: 0.127002, id: 0.111272] time: 0:02:18.285244 \n",
      "[Epoch 1/300] [Batch 262/292] [D loss: 0.075431, acc:  93%] [G loss: 5.396622, adv: 1.132633, recon: 0.143794, id: 0.129909] time: 0:02:18.490043 \n",
      "[Epoch 1/300] [Batch 263/292] [D loss: 0.081121, acc:  91%] [G loss: 5.310675, adv: 1.052559, recon: 0.146884, id: 0.122148] time: 0:02:18.697646 \n",
      "[Epoch 1/300] [Batch 264/292] [D loss: 0.129674, acc:  84%] [G loss: 5.583323, adv: 1.180792, recon: 0.147186, id: 0.114577] time: 0:02:18.923374 \n",
      "[Epoch 1/300] [Batch 265/292] [D loss: 0.177333, acc:  75%] [G loss: 5.154900, adv: 1.041676, recon: 0.140989, id: 0.114324] time: 0:02:19.132575 \n",
      "[Epoch 1/300] [Batch 266/292] [D loss: 0.170668, acc:  72%] [G loss: 5.869496, adv: 1.193765, recon: 0.160074, id: 0.124052] time: 0:02:19.320009 \n",
      "[Epoch 1/300] [Batch 267/292] [D loss: 0.136467, acc:  82%] [G loss: 5.351278, adv: 1.131834, recon: 0.140529, id: 0.135221] time: 0:02:19.484464 \n",
      "[Epoch 1/300] [Batch 268/292] [D loss: 0.095614, acc:  88%] [G loss: 5.179991, adv: 0.921488, recon: 0.153804, id: 0.113080] time: 0:02:19.668350 \n",
      "[Epoch 1/300] [Batch 269/292] [D loss: 0.209163, acc:  66%] [G loss: 4.932261, adv: 0.980545, recon: 0.135985, id: 0.124155] time: 0:02:19.870732 \n",
      "[Epoch 1/300] [Batch 270/292] [D loss: 0.162134, acc:  78%] [G loss: 5.258213, adv: 1.124650, recon: 0.138791, id: 0.108641] time: 0:02:20.067687 \n",
      "[Epoch 1/300] [Batch 271/292] [D loss: 0.212807, acc:  65%] [G loss: 5.489333, adv: 1.147746, recon: 0.146786, id: 0.123038] time: 0:02:20.251456 \n",
      "[Epoch 1/300] [Batch 272/292] [D loss: 0.304297, acc:  51%] [G loss: 8.064052, adv: 2.191258, recon: 0.170956, id: 0.118077] time: 0:02:20.431204 \n",
      "[Epoch 1/300] [Batch 273/292] [D loss: 0.249889, acc:  66%] [G loss: 6.005938, adv: 0.876039, recon: 0.197602, id: 0.152322] time: 0:02:20.617076 \n",
      "[Epoch 1/300] [Batch 274/292] [D loss: 0.121735, acc:  83%] [G loss: 5.043647, adv: 1.055707, recon: 0.132009, id: 0.165906] time: 0:02:20.847738 \n",
      "[Epoch 1/300] [Batch 275/292] [D loss: 0.219675, acc:  62%] [G loss: 5.048959, adv: 0.882546, recon: 0.151778, id: 0.113065] time: 0:02:21.039932 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/300] [Batch 276/292] [D loss: 0.110029, acc:  87%] [G loss: 5.240051, adv: 1.065203, recon: 0.141655, id: 0.140374] time: 0:02:21.235047 \n",
      "[Epoch 1/300] [Batch 277/292] [D loss: 0.131101, acc:  81%] [G loss: 4.510755, adv: 0.897755, recon: 0.123276, id: 0.140559] time: 0:02:21.441188 \n",
      "[Epoch 1/300] [Batch 278/292] [D loss: 0.106848, acc:  87%] [G loss: 5.043936, adv: 1.187249, recon: 0.122625, id: 0.113383] time: 0:02:21.664215 \n",
      "[Epoch 1/300] [Batch 279/292] [D loss: 0.161269, acc:  74%] [G loss: 6.593578, adv: 1.388426, recon: 0.176256, id: 0.133939] time: 0:02:21.915310 \n",
      "[Epoch 1/300] [Batch 280/292] [D loss: 0.070585, acc:  94%] [G loss: 5.917535, adv: 1.115174, recon: 0.169709, id: 0.123359] time: 0:02:22.158445 \n",
      "[Epoch 1/300] [Batch 281/292] [D loss: 0.215969, acc:  65%] [G loss: 6.132265, adv: 1.452620, recon: 0.147815, id: 0.124061] time: 0:02:22.372544 \n",
      "[Epoch 1/300] [Batch 282/292] [D loss: 0.214940, acc:  64%] [G loss: 4.804059, adv: 0.900612, recon: 0.137266, id: 0.115233] time: 0:02:22.557216 \n",
      "[Epoch 1/300] [Batch 283/292] [D loss: 0.158140, acc:  75%] [G loss: 4.994346, adv: 0.915587, recon: 0.143721, id: 0.135458] time: 0:02:22.733685 \n",
      "[Epoch 1/300] [Batch 284/292] [D loss: 0.185146, acc:  70%] [G loss: 4.560400, adv: 0.876063, recon: 0.127756, id: 0.116753] time: 0:02:22.935833 \n",
      "[Epoch 1/300] [Batch 285/292] [D loss: 0.164469, acc:  75%] [G loss: 5.403841, adv: 1.027415, recon: 0.155638, id: 0.122418] time: 0:02:23.118364 \n",
      "[Epoch 1/300] [Batch 286/292] [D loss: 0.125185, acc:  84%] [G loss: 4.800109, adv: 0.930195, recon: 0.134647, id: 0.107434] time: 0:02:23.338147 \n",
      "[Epoch 1/300] [Batch 287/292] [D loss: 0.081683, acc:  92%] [G loss: 4.829758, adv: 0.979959, recon: 0.129588, id: 0.124260] time: 0:02:23.552934 \n",
      "[Epoch 1/300] [Batch 288/292] [D loss: 0.081057, acc:  92%] [G loss: 5.270478, adv: 1.155909, recon: 0.132883, id: 0.138051] time: 0:02:23.769518 \n",
      "[Epoch 1/300] [Batch 289/292] [D loss: 0.191713, acc:  73%] [G loss: 5.261263, adv: 1.152028, recon: 0.134972, id: 0.124362] time: 0:02:23.999274 \n",
      "[Epoch 1/300] [Batch 290/292] [D loss: 0.210520, acc:  73%] [G loss: 5.304089, adv: 1.222199, recon: 0.131222, id: 0.103369] time: 0:02:24.194399 \n",
      "[Epoch 2/300] [Batch 0/292] [D loss: 0.178852, acc:  73%] [G loss: 6.252752, adv: 1.529022, recon: 0.146753, id: 0.097748] time: 0:02:24.398259 \n",
      "[Epoch 2/300] [Batch 1/292] [D loss: 0.122639, acc:  83%] [G loss: 5.526371, adv: 0.986280, recon: 0.166195, id: 0.099860] time: 0:02:24.890741 \n",
      "[Epoch 2/300] [Batch 2/292] [D loss: 0.132935, acc:  81%] [G loss: 5.742015, adv: 1.146071, recon: 0.160955, id: 0.107122] time: 0:02:25.094014 \n",
      "[Epoch 2/300] [Batch 3/292] [D loss: 0.178980, acc:  75%] [G loss: 5.353631, adv: 0.978699, recon: 0.157489, id: 0.102245] time: 0:02:25.300029 \n",
      "[Epoch 2/300] [Batch 4/292] [D loss: 0.132333, acc:  82%] [G loss: 4.541877, adv: 0.838330, recon: 0.131610, id: 0.114346] time: 0:02:25.487474 \n",
      "[Epoch 2/300] [Batch 5/292] [D loss: 0.193152, acc:  67%] [G loss: 4.811406, adv: 1.012719, recon: 0.128798, id: 0.102131] time: 0:02:25.693798 \n",
      "[Epoch 2/300] [Batch 6/292] [D loss: 0.192482, acc:  69%] [G loss: 6.011971, adv: 1.187274, recon: 0.170023, id: 0.104369] time: 0:02:25.878700 \n",
      "[Epoch 2/300] [Batch 7/292] [D loss: 0.132260, acc:  84%] [G loss: 5.259244, adv: 1.179884, recon: 0.133137, id: 0.099106] time: 0:02:26.112224 \n",
      "[Epoch 2/300] [Batch 8/292] [D loss: 0.134999, acc:  82%] [G loss: 5.936889, adv: 1.517283, recon: 0.134134, id: 0.098590] time: 0:02:26.324556 \n",
      "[Epoch 2/300] [Batch 9/292] [D loss: 0.171050, acc:  75%] [G loss: 5.674457, adv: 1.313675, recon: 0.140172, id: 0.091122] time: 0:02:26.536340 \n",
      "[Epoch 2/300] [Batch 10/292] [D loss: 0.167255, acc:  75%] [G loss: 5.041147, adv: 0.996821, recon: 0.139394, id: 0.150508] time: 0:02:26.773000 \n",
      "[Epoch 2/300] [Batch 11/292] [D loss: 0.141350, acc:  78%] [G loss: 5.676241, adv: 1.147701, recon: 0.157072, id: 0.099306] time: 0:02:27.001200 \n",
      "[Epoch 2/300] [Batch 12/292] [D loss: 0.185026, acc:  70%] [G loss: 5.946225, adv: 1.339995, recon: 0.153037, id: 0.108712] time: 0:02:27.193784 \n",
      "[Epoch 2/300] [Batch 13/292] [D loss: 0.123872, acc:  84%] [G loss: 6.744222, adv: 1.460546, recon: 0.177638, id: 0.112827] time: 0:02:27.406172 \n",
      "[Epoch 2/300] [Batch 14/292] [D loss: 0.074914, acc:  94%] [G loss: 5.821836, adv: 1.248306, recon: 0.153994, id: 0.087754] time: 0:02:27.584390 \n",
      "[Epoch 2/300] [Batch 15/292] [D loss: 0.173799, acc:  84%] [G loss: 5.715480, adv: 1.376189, recon: 0.138096, id: 0.104290] time: 0:02:27.805349 \n",
      "[Epoch 2/300] [Batch 16/292] [D loss: 0.173287, acc:  77%] [G loss: 5.825188, adv: 1.353500, recon: 0.143969, id: 0.103802] time: 0:02:28.033100 \n",
      "[Epoch 2/300] [Batch 17/292] [D loss: 0.157747, acc:  78%] [G loss: 7.153301, adv: 2.001470, recon: 0.146025, id: 0.101193] time: 0:02:28.256221 \n",
      "[Epoch 2/300] [Batch 18/292] [D loss: 0.108434, acc:  87%] [G loss: 5.250469, adv: 0.773645, recon: 0.169922, id: 0.114153] time: 0:02:28.457201 \n",
      "[Epoch 2/300] [Batch 19/292] [D loss: 0.111152, acc:  86%] [G loss: 5.984254, adv: 1.320832, recon: 0.153198, id: 0.116375] time: 0:02:28.661653 \n",
      "[Epoch 2/300] [Batch 20/292] [D loss: 0.221461, acc:  62%] [G loss: 6.316845, adv: 1.326389, recon: 0.172022, id: 0.112178] time: 0:02:28.880414 \n",
      "[Epoch 2/300] [Batch 21/292] [D loss: 0.148597, acc:  81%] [G loss: 6.212607, adv: 1.397773, recon: 0.160694, id: 0.096197] time: 0:02:29.106870 \n",
      "[Epoch 2/300] [Batch 22/292] [D loss: 0.151092, acc:  81%] [G loss: 5.422798, adv: 0.620621, recon: 0.192648, id: 0.100705] time: 0:02:29.294816 \n",
      "[Epoch 2/300] [Batch 23/292] [D loss: 0.377748, acc:  48%] [G loss: 5.318661, adv: 0.847324, recon: 0.169204, id: 0.098587] time: 0:02:29.461513 \n",
      "[Epoch 2/300] [Batch 24/292] [D loss: 0.260789, acc:  53%] [G loss: 4.941748, adv: 0.825255, recon: 0.151109, id: 0.103740] time: 0:02:29.623561 \n",
      "[Epoch 2/300] [Batch 25/292] [D loss: 0.162459, acc:  78%] [G loss: 5.446206, adv: 0.989919, recon: 0.159894, id: 0.125916] time: 0:02:29.811691 \n",
      "[Epoch 2/300] [Batch 26/292] [D loss: 0.206985, acc:  63%] [G loss: 5.020736, adv: 0.994403, recon: 0.140607, id: 0.114919] time: 0:02:30.013313 \n",
      "[Epoch 2/300] [Batch 27/292] [D loss: 0.122185, acc:  87%] [G loss: 5.310696, adv: 0.976356, recon: 0.155935, id: 0.097597] time: 0:02:30.230166 \n",
      "[Epoch 2/300] [Batch 28/292] [D loss: 0.089921, acc:  90%] [G loss: 5.098740, adv: 0.933389, recon: 0.148618, id: 0.104723] time: 0:02:30.445961 \n",
      "[Epoch 2/300] [Batch 29/292] [D loss: 0.253687, acc:  54%] [G loss: 5.429197, adv: 0.815300, recon: 0.180176, id: 0.101610] time: 0:02:30.650928 \n",
      "[Epoch 2/300] [Batch 30/292] [D loss: 0.124620, acc:  86%] [G loss: 6.559331, adv: 1.079848, recon: 0.205306, id: 0.134368] time: 0:02:30.876677 \n",
      "[Epoch 2/300] [Batch 31/292] [D loss: 0.289708, acc:  50%] [G loss: 4.595736, adv: 0.899670, recon: 0.129004, id: 0.089383] time: 0:02:31.088162 \n",
      "[Epoch 2/300] [Batch 32/292] [D loss: 0.148517, acc:  81%] [G loss: 6.543337, adv: 1.464669, recon: 0.166140, id: 0.110807] time: 0:02:31.290369 \n",
      "[Epoch 2/300] [Batch 33/292] [D loss: 0.204453, acc:  67%] [G loss: 4.746565, adv: 0.896661, recon: 0.134422, id: 0.110604] time: 0:02:31.500331 \n",
      "[Epoch 2/300] [Batch 34/292] [D loss: 0.127913, acc:  83%] [G loss: 4.740255, adv: 0.948123, recon: 0.131418, id: 0.089814] time: 0:02:31.712681 \n",
      "[Epoch 2/300] [Batch 35/292] [D loss: 0.098601, acc:  91%] [G loss: 4.637985, adv: 0.836147, recon: 0.136804, id: 0.093001] time: 0:02:31.900884 \n",
      "[Epoch 2/300] [Batch 36/292] [D loss: 0.095000, acc:  87%] [G loss: 4.904129, adv: 1.113058, recon: 0.123226, id: 0.090524] time: 0:02:32.066546 \n",
      "[Epoch 2/300] [Batch 37/292] [D loss: 0.070217, acc:  94%] [G loss: 4.547334, adv: 0.829922, recon: 0.132590, id: 0.113022] time: 0:02:32.268678 \n",
      "[Epoch 2/300] [Batch 38/292] [D loss: 0.250007, acc:  67%] [G loss: 5.782926, adv: 1.382791, recon: 0.138480, id: 0.132871] time: 0:02:32.449515 \n",
      "[Epoch 2/300] [Batch 39/292] [D loss: 0.168472, acc:  76%] [G loss: 5.641810, adv: 1.242226, recon: 0.146066, id: 0.112231] time: 0:02:32.643915 \n",
      "[Epoch 2/300] [Batch 40/292] [D loss: 0.186270, acc:  71%] [G loss: 5.164292, adv: 1.119119, recon: 0.135524, id: 0.088025] time: 0:02:32.860774 \n",
      "[Epoch 2/300] [Batch 41/292] [D loss: 0.149919, acc:  79%] [G loss: 4.923623, adv: 0.984711, recon: 0.136171, id: 0.100168] time: 0:02:33.089750 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2/300] [Batch 42/292] [D loss: 0.183259, acc:  73%] [G loss: 5.882978, adv: 1.206320, recon: 0.162387, id: 0.109933] time: 0:02:33.316867 \n",
      "[Epoch 2/300] [Batch 43/292] [D loss: 0.105353, acc:  90%] [G loss: 5.838173, adv: 1.130460, recon: 0.165499, id: 0.107353] time: 0:02:33.546115 \n",
      "[Epoch 2/300] [Batch 44/292] [D loss: 0.168765, acc:  73%] [G loss: 5.659244, adv: 1.304135, recon: 0.143191, id: 0.110519] time: 0:02:33.734165 \n",
      "[Epoch 2/300] [Batch 45/292] [D loss: 0.106488, acc:  88%] [G loss: 5.374496, adv: 1.172373, recon: 0.140738, id: 0.099736] time: 0:02:33.936707 \n",
      "[Epoch 2/300] [Batch 46/292] [D loss: 0.222204, acc:  63%] [G loss: 5.219197, adv: 1.132654, recon: 0.135414, id: 0.107513] time: 0:02:34.140468 \n",
      "[Epoch 2/300] [Batch 47/292] [D loss: 0.164736, acc:  76%] [G loss: 5.495572, adv: 0.851752, recon: 0.176955, id: 0.118046] time: 0:02:34.320431 \n",
      "[Epoch 2/300] [Batch 48/292] [D loss: 0.108198, acc:  87%] [G loss: 4.929700, adv: 1.040396, recon: 0.132088, id: 0.098823] time: 0:02:34.486423 \n",
      "[Epoch 2/300] [Batch 49/292] [D loss: 0.202909, acc:  69%] [G loss: 5.936309, adv: 1.484872, recon: 0.139232, id: 0.090946] time: 0:02:34.681675 \n",
      "[Epoch 2/300] [Batch 50/292] [D loss: 0.111283, acc:  86%] [G loss: 6.899199, adv: 1.419198, recon: 0.190130, id: 0.111367] time: 0:02:34.895963 \n",
      "[Epoch 2/300] [Batch 51/292] [D loss: 0.156496, acc:  77%] [G loss: 6.052834, adv: 1.393834, recon: 0.151153, id: 0.115981] time: 0:02:35.064623 \n",
      "[Epoch 2/300] [Batch 52/292] [D loss: 0.094233, acc:  89%] [G loss: 5.855368, adv: 1.244278, recon: 0.156659, id: 0.110019] time: 0:02:35.245461 \n",
      "[Epoch 2/300] [Batch 53/292] [D loss: 0.063375, acc:  95%] [G loss: 5.724504, adv: 1.039196, recon: 0.170986, id: 0.107591] time: 0:02:35.461000 \n",
      "[Epoch 2/300] [Batch 54/292] [D loss: 0.079329, acc:  92%] [G loss: 4.521703, adv: 0.751104, recon: 0.139751, id: 0.094562] time: 0:02:35.674274 \n",
      "[Epoch 2/300] [Batch 55/292] [D loss: 0.245284, acc:  62%] [G loss: 5.247950, adv: 1.153955, recon: 0.134727, id: 0.111877] time: 0:02:35.889478 \n",
      "[Epoch 2/300] [Batch 56/292] [D loss: 0.121127, acc:  83%] [G loss: 5.306042, adv: 1.003140, recon: 0.153703, id: 0.103897] time: 0:02:36.087925 \n",
      "[Epoch 2/300] [Batch 57/292] [D loss: 0.122776, acc:  85%] [G loss: 4.811360, adv: 0.897186, recon: 0.139017, id: 0.100524] time: 0:02:36.281190 \n",
      "[Epoch 2/300] [Batch 58/292] [D loss: 0.148192, acc:  78%] [G loss: 4.612101, adv: 0.962709, recon: 0.123292, id: 0.097676] time: 0:02:36.466965 \n",
      "[Epoch 2/300] [Batch 59/292] [D loss: 0.228957, acc:  62%] [G loss: 5.721523, adv: 1.157329, recon: 0.156270, id: 0.110395] time: 0:02:36.685893 \n",
      "[Epoch 2/300] [Batch 60/292] [D loss: 0.134586, acc:  84%] [G loss: 4.721527, adv: 0.864291, recon: 0.137575, id: 0.121353] time: 0:02:36.875778 \n",
      "[Epoch 2/300] [Batch 61/292] [D loss: 0.198226, acc:  70%] [G loss: 5.972597, adv: 1.406031, recon: 0.146975, id: 0.118152] time: 0:02:37.051373 \n",
      "[Epoch 2/300] [Batch 62/292] [D loss: 0.241629, acc:  63%] [G loss: 5.720286, adv: 1.398071, recon: 0.136737, id: 0.100340] time: 0:02:37.237065 \n",
      "[Epoch 2/300] [Batch 63/292] [D loss: 0.243800, acc:  77%] [G loss: 5.509652, adv: 1.061436, recon: 0.159570, id: 0.092189] time: 0:02:37.413592 \n",
      "[Epoch 2/300] [Batch 64/292] [D loss: 0.181742, acc:  73%] [G loss: 4.190919, adv: 0.723056, recon: 0.127970, id: 0.099457] time: 0:02:37.618213 \n",
      "[Epoch 2/300] [Batch 65/292] [D loss: 0.217836, acc:  61%] [G loss: 4.940386, adv: 0.758273, recon: 0.159983, id: 0.100776] time: 0:02:37.827311 \n",
      "[Epoch 2/300] [Batch 66/292] [D loss: 0.141749, acc:  82%] [G loss: 4.710402, adv: 0.859834, recon: 0.139097, id: 0.087720] time: 0:02:38.048975 \n",
      "[Epoch 2/300] [Batch 67/292] [D loss: 0.275285, acc:  56%] [G loss: 4.537909, adv: 0.867180, recon: 0.130906, id: 0.090949] time: 0:02:38.289684 \n",
      "[Epoch 2/300] [Batch 68/292] [D loss: 0.160371, acc:  73%] [G loss: 4.717485, adv: 0.712345, recon: 0.152759, id: 0.133011] time: 0:02:38.504550 \n",
      "[Epoch 2/300] [Batch 69/292] [D loss: 0.131627, acc:  82%] [G loss: 4.644089, adv: 1.010936, recon: 0.122082, id: 0.100517] time: 0:02:38.710278 \n",
      "[Epoch 2/300] [Batch 70/292] [D loss: 0.099812, acc:  88%] [G loss: 4.803543, adv: 0.794005, recon: 0.147655, id: 0.104444] time: 0:02:38.933957 \n",
      "[Epoch 2/300] [Batch 71/292] [D loss: 0.127330, acc:  83%] [G loss: 4.693320, adv: 0.975765, recon: 0.126844, id: 0.087062] time: 0:02:39.143776 \n",
      "[Epoch 2/300] [Batch 72/292] [D loss: 0.237620, acc:  64%] [G loss: 4.934804, adv: 0.963785, recon: 0.139812, id: 0.099912] time: 0:02:39.349526 \n",
      "[Epoch 2/300] [Batch 73/292] [D loss: 0.107923, acc:  89%] [G loss: 4.653819, adv: 0.859718, recon: 0.135765, id: 0.094095] time: 0:02:39.545240 \n",
      "[Epoch 2/300] [Batch 74/292] [D loss: 0.094420, acc:  89%] [G loss: 4.508271, adv: 0.923217, recon: 0.123083, id: 0.090993] time: 0:02:39.788453 \n",
      "[Epoch 2/300] [Batch 75/292] [D loss: 0.073593, acc:  92%] [G loss: 5.171629, adv: 1.066037, recon: 0.140020, id: 0.094513] time: 0:02:40.003684 \n",
      "[Epoch 2/300] [Batch 76/292] [D loss: 0.291420, acc:  59%] [G loss: 5.202862, adv: 0.863753, recon: 0.160905, id: 0.084071] time: 0:02:40.205109 \n",
      "[Epoch 2/300] [Batch 77/292] [D loss: 0.128244, acc:  83%] [G loss: 5.603748, adv: 1.013600, recon: 0.166252, id: 0.113205] time: 0:02:40.405874 \n",
      "[Epoch 2/300] [Batch 78/292] [D loss: 0.195492, acc:  68%] [G loss: 4.653823, adv: 0.839510, recon: 0.138119, id: 0.108135] time: 0:02:40.596833 \n",
      "[Epoch 2/300] [Batch 79/292] [D loss: 0.163985, acc:  73%] [G loss: 4.906156, adv: 0.970372, recon: 0.136541, id: 0.125526] time: 0:02:40.792316 \n",
      "[Epoch 2/300] [Batch 80/292] [D loss: 0.232120, acc:  62%] [G loss: 5.524783, adv: 1.348379, recon: 0.130471, id: 0.097792] time: 0:02:40.990311 \n",
      "[Epoch 2/300] [Batch 81/292] [D loss: 0.124564, acc:  85%] [G loss: 4.880422, adv: 0.952477, recon: 0.137543, id: 0.126885] time: 0:02:41.175419 \n",
      "[Epoch 2/300] [Batch 82/292] [D loss: 0.108049, acc:  88%] [G loss: 5.238834, adv: 1.086535, recon: 0.141873, id: 0.102358] time: 0:02:41.375712 \n",
      "[Epoch 2/300] [Batch 83/292] [D loss: 0.205389, acc:  71%] [G loss: 4.973707, adv: 1.081164, recon: 0.129203, id: 0.114656] time: 0:02:41.612532 \n",
      "[Epoch 2/300] [Batch 84/292] [D loss: 0.081163, acc:  93%] [G loss: 5.101305, adv: 0.811640, recon: 0.160581, id: 0.103833] time: 0:02:41.824326 \n",
      "[Epoch 2/300] [Batch 85/292] [D loss: 0.213359, acc:  64%] [G loss: 4.561354, adv: 0.912968, recon: 0.126163, id: 0.085518] time: 0:02:42.042069 \n",
      "[Epoch 2/300] [Batch 86/292] [D loss: 0.136092, acc:  83%] [G loss: 5.675476, adv: 1.335725, recon: 0.139155, id: 0.117687] time: 0:02:42.250657 \n",
      "[Epoch 2/300] [Batch 87/292] [D loss: 0.205480, acc:  70%] [G loss: 5.468894, adv: 1.162740, recon: 0.144390, id: 0.103050] time: 0:02:42.454194 \n",
      "[Epoch 2/300] [Batch 88/292] [D loss: 0.234805, acc:  61%] [G loss: 5.066278, adv: 0.957785, recon: 0.147847, id: 0.107699] time: 0:02:42.646015 \n",
      "[Epoch 2/300] [Batch 89/292] [D loss: 0.125065, acc:  83%] [G loss: 5.226255, adv: 0.956997, recon: 0.153646, id: 0.099792] time: 0:02:42.840717 \n",
      "[Epoch 2/300] [Batch 90/292] [D loss: 0.168903, acc:  73%] [G loss: 4.951283, adv: 0.901830, recon: 0.145315, id: 0.117459] time: 0:02:43.057283 \n",
      "[Epoch 2/300] [Batch 91/292] [D loss: 0.215885, acc:  63%] [G loss: 6.200323, adv: 1.354420, recon: 0.164088, id: 0.116748] time: 0:02:43.243390 \n",
      "[Epoch 2/300] [Batch 92/292] [D loss: 0.161759, acc:  75%] [G loss: 5.986335, adv: 1.148210, recon: 0.171952, id: 0.124798] time: 0:02:43.460759 \n",
      "[Epoch 2/300] [Batch 93/292] [D loss: 0.175702, acc:  76%] [G loss: 6.377633, adv: 1.222726, recon: 0.186148, id: 0.089600] time: 0:02:43.676345 \n",
      "[Epoch 2/300] [Batch 94/292] [D loss: 0.221955, acc:  68%] [G loss: 6.765698, adv: 1.402974, recon: 0.186107, id: 0.093275] time: 0:02:43.876852 \n",
      "[Epoch 2/300] [Batch 95/292] [D loss: 0.147260, acc:  78%] [G loss: 5.860523, adv: 1.011301, recon: 0.179156, id: 0.107939] time: 0:02:44.100651 \n",
      "[Epoch 2/300] [Batch 96/292] [D loss: 0.113964, acc:  85%] [G loss: 5.451885, adv: 1.014875, recon: 0.158168, id: 0.132637] time: 0:02:44.301558 \n",
      "[Epoch 2/300] [Batch 97/292] [D loss: 0.093532, acc:  91%] [G loss: 6.067509, adv: 1.054309, recon: 0.185756, id: 0.100597] time: 0:02:44.520141 \n",
      "[Epoch 2/300] [Batch 98/292] [D loss: 0.177765, acc:  76%] [G loss: 5.715770, adv: 1.257503, recon: 0.148977, id: 0.111577] time: 0:02:44.748327 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2/300] [Batch 99/292] [D loss: 0.188782, acc:  74%] [G loss: 5.692449, adv: 1.311680, recon: 0.143648, id: 0.100054] time: 0:02:44.984903 \n",
      "[Epoch 2/300] [Batch 100/292] [D loss: 0.183196, acc:  75%] [G loss: 4.893760, adv: 1.086732, recon: 0.126537, id: 0.098970] time: 0:02:45.200192 \n",
      "[Epoch 2/300] [Batch 101/292] [D loss: 0.126236, acc:  83%] [G loss: 6.036669, adv: 1.462695, recon: 0.144507, id: 0.099573] time: 0:02:45.411148 \n",
      "[Epoch 2/300] [Batch 102/292] [D loss: 0.214917, acc:  72%] [G loss: 9.987576, adv: 2.892584, recon: 0.198221, id: 0.094537] time: 0:02:45.585019 \n",
      "[Epoch 2/300] [Batch 103/292] [D loss: 0.283459, acc:  73%] [G loss: 6.865877, adv: 1.340274, recon: 0.196915, id: 0.102708] time: 0:02:45.780972 \n",
      "[Epoch 2/300] [Batch 104/292] [D loss: 0.168987, acc:  77%] [G loss: 5.986586, adv: 0.941038, recon: 0.191353, id: 0.124159] time: 0:02:45.989525 \n",
      "[Epoch 2/300] [Batch 105/292] [D loss: 0.169660, acc:  73%] [G loss: 5.029202, adv: 0.889414, recon: 0.150786, id: 0.107854] time: 0:02:46.209773 \n",
      "[Epoch 2/300] [Batch 106/292] [D loss: 0.307005, acc:  50%] [G loss: 5.047603, adv: 0.946990, recon: 0.147494, id: 0.085890] time: 0:02:46.420640 \n",
      "[Epoch 2/300] [Batch 107/292] [D loss: 0.248033, acc:  58%] [G loss: 6.191240, adv: 1.576703, recon: 0.142307, id: 0.105535] time: 0:02:46.644917 \n",
      "[Epoch 2/300] [Batch 108/292] [D loss: 0.118894, acc:  85%] [G loss: 4.930020, adv: 0.769320, recon: 0.158410, id: 0.110860] time: 0:02:46.869801 \n",
      "[Epoch 2/300] [Batch 109/292] [D loss: 0.180872, acc:  68%] [G loss: 4.879682, adv: 0.856174, recon: 0.149052, id: 0.093862] time: 0:02:47.079732 \n",
      "[Epoch 2/300] [Batch 110/292] [D loss: 0.184575, acc:  72%] [G loss: 5.366189, adv: 0.997516, recon: 0.159132, id: 0.097985] time: 0:02:47.277348 \n",
      "[Epoch 2/300] [Batch 111/292] [D loss: 0.232204, acc:  65%] [G loss: 6.394564, adv: 1.238409, recon: 0.183372, id: 0.115818] time: 0:02:47.489705 \n",
      "[Epoch 2/300] [Batch 112/292] [D loss: 0.148874, acc:  78%] [G loss: 6.247187, adv: 1.353808, recon: 0.167379, id: 0.098143] time: 0:02:47.714246 \n",
      "[Epoch 2/300] [Batch 113/292] [D loss: 0.084480, acc:  90%] [G loss: 5.859741, adv: 1.142615, recon: 0.166235, id: 0.116190] time: 0:02:47.908522 \n",
      "[Epoch 2/300] [Batch 114/292] [D loss: 0.167681, acc:  74%] [G loss: 4.980265, adv: 1.034183, recon: 0.135150, id: 0.092423] time: 0:02:48.119383 \n",
      "[Epoch 2/300] [Batch 115/292] [D loss: 0.101885, acc:  89%] [G loss: 5.244781, adv: 0.924090, recon: 0.157653, id: 0.118790] time: 0:02:48.317348 \n",
      "[Epoch 2/300] [Batch 116/292] [D loss: 0.072282, acc:  94%] [G loss: 5.661967, adv: 1.183438, recon: 0.152817, id: 0.087034] time: 0:02:48.542304 \n",
      "[Epoch 2/300] [Batch 117/292] [D loss: 0.107186, acc:  87%] [G loss: 4.240404, adv: 0.795553, recon: 0.123130, id: 0.088315] time: 0:02:48.761713 \n",
      "[Epoch 2/300] [Batch 118/292] [D loss: 0.084881, acc:  92%] [G loss: 5.160898, adv: 0.996476, recon: 0.145819, id: 0.117327] time: 0:02:48.993448 \n",
      "[Epoch 2/300] [Batch 119/292] [D loss: 0.093129, acc:  92%] [G loss: 5.179030, adv: 1.044595, recon: 0.142108, id: 0.098051] time: 0:02:49.209792 \n",
      "[Epoch 2/300] [Batch 120/292] [D loss: 0.221974, acc:  63%] [G loss: 6.665059, adv: 1.752215, recon: 0.147128, id: 0.107050] time: 0:02:49.415481 \n",
      "[Epoch 2/300] [Batch 121/292] [D loss: 0.159972, acc:  81%] [G loss: 5.918077, adv: 1.192105, recon: 0.165071, id: 0.111762] time: 0:02:49.615175 \n",
      "[Epoch 2/300] [Batch 122/292] [D loss: 0.106191, acc:  87%] [G loss: 5.073647, adv: 1.061153, recon: 0.135557, id: 0.104510] time: 0:02:49.818484 \n",
      "[Epoch 2/300] [Batch 123/292] [D loss: 0.153144, acc:  78%] [G loss: 7.632089, adv: 2.063817, recon: 0.164577, id: 0.105204] time: 0:02:50.009698 \n",
      "[Epoch 2/300] [Batch 124/292] [D loss: 0.373962, acc:  50%] [G loss: 11.565294, adv: 3.642076, recon: 0.203568, id: 0.091809] time: 0:02:50.197057 \n",
      "[Epoch 2/300] [Batch 125/292] [D loss: 0.185542, acc:  73%] [G loss: 5.952802, adv: 1.177156, recon: 0.167779, id: 0.116620] time: 0:02:50.414365 \n",
      "[Epoch 2/300] [Batch 126/292] [D loss: 0.086622, acc:  91%] [G loss: 5.111759, adv: 0.843626, recon: 0.159999, id: 0.126421] time: 0:02:50.636256 \n",
      "[Epoch 2/300] [Batch 127/292] [D loss: 0.103673, acc:  88%] [G loss: 5.087860, adv: 1.003254, recon: 0.142499, id: 0.084404] time: 0:02:50.866464 \n",
      "[Epoch 2/300] [Batch 128/292] [D loss: 0.085603, acc:  91%] [G loss: 4.896430, adv: 0.939666, recon: 0.140413, id: 0.095282] time: 0:02:51.084798 \n",
      "[Epoch 2/300] [Batch 129/292] [D loss: 0.209881, acc:  69%] [G loss: 5.682652, adv: 1.324373, recon: 0.140210, id: 0.101864] time: 0:02:51.283695 \n",
      "[Epoch 2/300] [Batch 130/292] [D loss: 0.331599, acc:  48%] [G loss: 5.487577, adv: 1.208635, recon: 0.142246, id: 0.097622] time: 0:02:51.480180 \n",
      "[Epoch 2/300] [Batch 131/292] [D loss: 0.179474, acc:  73%] [G loss: 4.631717, adv: 0.610952, recon: 0.156953, id: 0.114108] time: 0:02:51.700359 \n",
      "[Epoch 2/300] [Batch 132/292] [D loss: 0.331577, acc:  41%] [G loss: 4.562118, adv: 0.685202, recon: 0.149830, id: 0.108455] time: 0:02:51.926551 \n",
      "[Epoch 2/300] [Batch 133/292] [D loss: 0.264286, acc:  50%] [G loss: 4.246431, adv: 0.751079, recon: 0.127055, id: 0.108943] time: 0:02:52.134069 \n",
      "[Epoch 2/300] [Batch 134/292] [D loss: 0.132679, acc:  85%] [G loss: 4.274594, adv: 0.841253, recon: 0.119231, id: 0.082425] time: 0:02:52.348858 \n",
      "[Epoch 2/300] [Batch 135/292] [D loss: 0.120445, acc:  84%] [G loss: 4.666676, adv: 0.853041, recon: 0.138111, id: 0.092410] time: 0:02:52.545419 \n",
      "[Epoch 2/300] [Batch 136/292] [D loss: 0.230238, acc:  63%] [G loss: 5.409944, adv: 0.879021, recon: 0.167321, id: 0.088102] time: 0:02:52.739017 \n",
      "[Epoch 2/300] [Batch 137/292] [D loss: 0.115769, acc:  87%] [G loss: 5.245429, adv: 1.114394, recon: 0.141243, id: 0.090414] time: 0:02:52.935421 \n",
      "[Epoch 2/300] [Batch 138/292] [D loss: 0.161013, acc:  78%] [G loss: 4.629755, adv: 0.536285, recon: 0.163462, id: 0.099247] time: 0:02:53.135321 \n",
      "[Epoch 2/300] [Batch 139/292] [D loss: 0.203608, acc:  64%] [G loss: 5.218749, adv: 0.899292, recon: 0.156284, id: 0.098043] time: 0:02:53.312567 \n",
      "[Epoch 2/300] [Batch 140/292] [D loss: 0.374870, acc:  37%] [G loss: 4.442088, adv: 0.805577, recon: 0.129537, id: 0.083607] time: 0:02:53.502386 \n",
      "[Epoch 2/300] [Batch 141/292] [D loss: 0.262658, acc:  57%] [G loss: 4.730611, adv: 0.815987, recon: 0.144618, id: 0.089990] time: 0:02:53.706905 \n",
      "[Epoch 2/300] [Batch 142/292] [D loss: 0.221675, acc:  62%] [G loss: 4.343647, adv: 0.914759, recon: 0.117333, id: 0.077817] time: 0:02:53.911136 \n",
      "[Epoch 2/300] [Batch 143/292] [D loss: 0.167345, acc:  75%] [G loss: 4.816261, adv: 0.852125, recon: 0.142911, id: 0.111264] time: 0:02:54.112714 \n",
      "[Epoch 2/300] [Batch 144/292] [D loss: 0.173962, acc:  74%] [G loss: 4.815571, adv: 1.000587, recon: 0.130234, id: 0.090503] time: 0:02:54.333537 \n",
      "[Epoch 2/300] [Batch 145/292] [D loss: 0.148487, acc:  79%] [G loss: 5.094831, adv: 0.991317, recon: 0.144031, id: 0.109663] time: 0:02:54.527668 \n",
      "[Epoch 2/300] [Batch 146/292] [D loss: 0.109457, acc:  87%] [G loss: 4.869884, adv: 0.991567, recon: 0.132882, id: 0.105377] time: 0:02:54.733485 \n",
      "[Epoch 2/300] [Batch 147/292] [D loss: 0.231138, acc:  64%] [G loss: 4.669662, adv: 0.866647, recon: 0.136294, id: 0.107109] time: 0:02:54.927023 \n",
      "[Epoch 2/300] [Batch 148/292] [D loss: 0.175843, acc:  73%] [G loss: 5.245292, adv: 1.024246, recon: 0.150134, id: 0.095835] time: 0:02:55.136915 \n",
      "[Epoch 2/300] [Batch 149/292] [D loss: 0.180798, acc:  76%] [G loss: 5.313265, adv: 1.178093, recon: 0.136925, id: 0.109621] time: 0:02:55.326904 \n",
      "[Epoch 2/300] [Batch 150/292] [D loss: 0.194747, acc:  70%] [G loss: 5.092156, adv: 1.000675, recon: 0.145715, id: 0.084227] time: 0:02:55.500739 \n",
      "[Epoch 2/300] [Batch 151/292] [D loss: 0.101134, acc:  88%] [G loss: 5.371865, adv: 1.198787, recon: 0.136089, id: 0.116026] time: 0:02:55.691698 \n",
      "[Epoch 2/300] [Batch 152/292] [D loss: 0.146183, acc:  79%] [G loss: 6.316470, adv: 1.745859, recon: 0.131016, id: 0.089546] time: 0:02:55.905431 \n",
      "[Epoch 2/300] [Batch 153/292] [D loss: 0.169310, acc:  76%] [G loss: 4.592277, adv: 0.736356, recon: 0.143651, id: 0.099291] time: 0:02:56.109873 \n",
      "[Epoch 2/300] [Batch 154/292] [D loss: 0.252819, acc:  68%] [G loss: 4.841733, adv: 0.815688, recon: 0.151176, id: 0.100789] time: 0:02:56.316065 \n",
      "[Epoch 2/300] [Batch 155/292] [D loss: 0.133040, acc:  85%] [G loss: 4.596512, adv: 0.917334, recon: 0.127492, id: 0.092522] time: 0:02:56.500365 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2/300] [Batch 156/292] [D loss: 0.134054, acc:  80%] [G loss: 4.523905, adv: 0.784058, recon: 0.138265, id: 0.096546] time: 0:02:56.698135 \n",
      "[Epoch 2/300] [Batch 157/292] [D loss: 0.120054, acc:  84%] [G loss: 4.442711, adv: 0.889229, recon: 0.124022, id: 0.089676] time: 0:02:56.923535 \n",
      "[Epoch 2/300] [Batch 158/292] [D loss: 0.164662, acc:  72%] [G loss: 5.229732, adv: 0.939477, recon: 0.157423, id: 0.103981] time: 0:02:57.137312 \n",
      "[Epoch 2/300] [Batch 159/292] [D loss: 0.123333, acc:  85%] [G loss: 5.360623, adv: 0.996691, recon: 0.156115, id: 0.102526] time: 0:02:57.335399 \n",
      "[Epoch 2/300] [Batch 160/292] [D loss: 0.150350, acc:  76%] [G loss: 4.243625, adv: 0.799039, recon: 0.123102, id: 0.083665] time: 0:02:57.542422 \n",
      "[Epoch 2/300] [Batch 161/292] [D loss: 0.242317, acc:  60%] [G loss: 4.549976, adv: 0.889164, recon: 0.130034, id: 0.086103] time: 0:02:57.736835 \n",
      "[Epoch 2/300] [Batch 162/292] [D loss: 0.108234, acc:  86%] [G loss: 4.756956, adv: 0.864587, recon: 0.141583, id: 0.095221] time: 0:02:57.941613 \n",
      "[Epoch 2/300] [Batch 163/292] [D loss: 0.132378, acc:  83%] [G loss: 4.968263, adv: 1.119664, recon: 0.126349, id: 0.092126] time: 0:02:58.145360 \n",
      "[Epoch 2/300] [Batch 164/292] [D loss: 0.138395, acc:  84%] [G loss: 4.532416, adv: 0.687844, recon: 0.147744, id: 0.098843] time: 0:02:58.342662 \n",
      "[Epoch 2/300] [Batch 165/292] [D loss: 0.167004, acc:  74%] [G loss: 4.407778, adv: 0.905873, recon: 0.120575, id: 0.092579] time: 0:02:58.538895 \n",
      "[Epoch 2/300] [Batch 166/292] [D loss: 0.134090, acc:  82%] [G loss: 4.780580, adv: 1.039589, recon: 0.126049, id: 0.099232] time: 0:02:58.735265 \n",
      "[Epoch 2/300] [Batch 167/292] [D loss: 0.071550, acc:  95%] [G loss: 4.725310, adv: 1.001197, recon: 0.126062, id: 0.077921] time: 0:02:58.912225 \n",
      "[Epoch 2/300] [Batch 168/292] [D loss: 0.121866, acc:  84%] [G loss: 5.346774, adv: 1.157916, recon: 0.141470, id: 0.081585] time: 0:02:59.083551 \n",
      "[Epoch 2/300] [Batch 169/292] [D loss: 0.173943, acc:  74%] [G loss: 5.334090, adv: 1.039227, recon: 0.151648, id: 0.110162] time: 0:02:59.270719 \n",
      "[Epoch 2/300] [Batch 170/292] [D loss: 0.076951, acc:  93%] [G loss: 5.281848, adv: 1.038211, recon: 0.149396, id: 0.123058] time: 0:02:59.466245 \n",
      "[Epoch 2/300] [Batch 171/292] [D loss: 0.224699, acc:  62%] [G loss: 5.703794, adv: 1.268955, recon: 0.149443, id: 0.100850] time: 0:02:59.684230 \n",
      "[Epoch 2/300] [Batch 172/292] [D loss: 0.161970, acc:  76%] [G loss: 5.448179, adv: 1.124995, recon: 0.149585, id: 0.114712] time: 0:02:59.899380 \n",
      "[Epoch 2/300] [Batch 173/292] [D loss: 0.171541, acc:  73%] [G loss: 5.583357, adv: 1.370749, recon: 0.132025, id: 0.086547] time: 0:03:00.110575 \n",
      "[Epoch 2/300] [Batch 174/292] [D loss: 0.380961, acc:  60%] [G loss: 28.166368, adv: 12.136306, recon: 0.186191, id: 0.086184] time: 0:03:00.332897 \n",
      "[Epoch 2/300] [Batch 175/292] [D loss: 0.622456, acc:  57%] [G loss: 9.547589, adv: 1.276271, recon: 0.321381, id: 0.415249] time: 0:03:00.542882 \n",
      "[Epoch 2/300] [Batch 176/292] [D loss: 0.235864, acc:  69%] [G loss: 7.861472, adv: 1.192126, recon: 0.245713, id: 0.388047] time: 0:03:00.737779 \n",
      "[Epoch 2/300] [Batch 177/292] [D loss: 0.411813, acc:  38%] [G loss: 8.628149, adv: 1.134825, recon: 0.288661, id: 0.391575] time: 0:03:00.937993 \n",
      "[Epoch 2/300] [Batch 178/292] [D loss: 0.469455, acc:  36%] [G loss: 7.189483, adv: 0.854691, recon: 0.247445, id: 0.322429] time: 0:03:01.114014 \n",
      "[Epoch 2/300] [Batch 179/292] [D loss: 0.327177, acc:  55%] [G loss: 6.492671, adv: 0.907543, recon: 0.203529, id: 0.429396] time: 0:03:01.317190 \n",
      "[Epoch 2/300] [Batch 180/292] [D loss: 0.165256, acc:  77%] [G loss: 6.390151, adv: 0.965764, recon: 0.196520, id: 0.354030] time: 0:03:01.521143 \n",
      "[Epoch 2/300] [Batch 181/292] [D loss: 0.129137, acc:  80%] [G loss: 5.986852, adv: 1.002912, recon: 0.176352, id: 0.303132] time: 0:03:01.710615 \n",
      "[Epoch 2/300] [Batch 182/292] [D loss: 0.210572, acc:  68%] [G loss: 6.800858, adv: 1.258510, recon: 0.189962, id: 0.319812] time: 0:03:01.906296 \n",
      "[Epoch 2/300] [Batch 183/292] [D loss: 0.284961, acc:  56%] [G loss: 6.599078, adv: 1.107822, recon: 0.192070, id: 0.342493] time: 0:03:02.079575 \n",
      "[Epoch 2/300] [Batch 184/292] [D loss: 0.162937, acc:  77%] [G loss: 6.246183, adv: 1.068723, recon: 0.184224, id: 0.298043] time: 0:03:02.270546 \n",
      "[Epoch 2/300] [Batch 185/292] [D loss: 0.146473, acc:  78%] [G loss: 6.459709, adv: 1.278104, recon: 0.176499, id: 0.243717] time: 0:03:02.477463 \n",
      "[Epoch 2/300] [Batch 186/292] [D loss: 0.246704, acc:  64%] [G loss: 6.622478, adv: 1.290167, recon: 0.176856, id: 0.373979] time: 0:03:02.705782 \n",
      "[Epoch 2/300] [Batch 187/292] [D loss: 0.294726, acc:  54%] [G loss: 6.241039, adv: 1.164008, recon: 0.178335, id: 0.204073] time: 0:03:02.919799 \n",
      "[Epoch 2/300] [Batch 188/292] [D loss: 0.138954, acc:  81%] [G loss: 6.760595, adv: 1.468975, recon: 0.174565, id: 0.208636] time: 0:03:03.114336 \n",
      "[Epoch 2/300] [Batch 189/292] [D loss: 0.161230, acc:  76%] [G loss: 6.143641, adv: 1.099963, recon: 0.179680, id: 0.167211] time: 0:03:03.308378 \n",
      "[Epoch 2/300] [Batch 190/292] [D loss: 0.178237, acc:  75%] [G loss: 5.974759, adv: 1.276522, recon: 0.155578, id: 0.172068] time: 0:03:03.521758 \n",
      "[Epoch 2/300] [Batch 191/292] [D loss: 0.113752, acc:  87%] [G loss: 5.884565, adv: 1.226018, recon: 0.155957, id: 0.183572] time: 0:03:03.737634 \n",
      "[Epoch 2/300] [Batch 192/292] [D loss: 0.129504, acc:  82%] [G loss: 6.056255, adv: 1.148445, recon: 0.172719, id: 0.131784] time: 0:03:03.908785 \n",
      "[Epoch 2/300] [Batch 193/292] [D loss: 0.054268, acc:  96%] [G loss: 5.661437, adv: 1.121797, recon: 0.157396, id: 0.129556] time: 0:03:04.094985 \n",
      "[Epoch 2/300] [Batch 194/292] [D loss: 0.273153, acc:  68%] [G loss: 6.022985, adv: 0.970687, recon: 0.187718, id: 0.161780] time: 0:03:04.301778 \n",
      "[Epoch 2/300] [Batch 195/292] [D loss: 0.271745, acc:  60%] [G loss: 5.588768, adv: 1.024667, recon: 0.164423, id: 0.139623] time: 0:03:04.512666 \n",
      "[Epoch 2/300] [Batch 196/292] [D loss: 0.275892, acc:  59%] [G loss: 5.137304, adv: 0.969262, recon: 0.147965, id: 0.121860] time: 0:03:04.709794 \n",
      "[Epoch 2/300] [Batch 197/292] [D loss: 0.140306, acc:  81%] [G loss: 4.463229, adv: 0.711616, recon: 0.138775, id: 0.129913] time: 0:03:04.917597 \n",
      "[Epoch 2/300] [Batch 198/292] [D loss: 0.173927, acc:  73%] [G loss: 5.049614, adv: 0.856508, recon: 0.150857, id: 0.161725] time: 0:03:05.144761 \n",
      "[Epoch 2/300] [Batch 199/292] [D loss: 0.220605, acc:  63%] [G loss: 4.979169, adv: 1.007931, recon: 0.134134, id: 0.125237] time: 0:03:05.374406 \n",
      "[Epoch 2/300] [Batch 200/292] [D loss: 0.242971, acc:  59%] [G loss: 5.098475, adv: 0.834384, recon: 0.157648, id: 0.128814] time: 0:03:05.589075 \n",
      "[Epoch 2/300] [Batch 201/292] [D loss: 0.234690, acc:  62%] [G loss: 5.629970, adv: 0.937278, recon: 0.172346, id: 0.204927] time: 0:03:06.087809 \n",
      "[Epoch 2/300] [Batch 202/292] [D loss: 0.180075, acc:  68%] [G loss: 5.017152, adv: 0.809024, recon: 0.154620, id: 0.115013] time: 0:03:06.297001 \n",
      "[Epoch 2/300] [Batch 203/292] [D loss: 0.113915, acc:  86%] [G loss: 4.762101, adv: 0.717278, recon: 0.153295, id: 0.139265] time: 0:03:06.503250 \n",
      "[Epoch 2/300] [Batch 204/292] [D loss: 0.097814, acc:  90%] [G loss: 4.970483, adv: 0.896780, recon: 0.145678, id: 0.143827] time: 0:03:06.725155 \n",
      "[Epoch 2/300] [Batch 205/292] [D loss: 0.113167, acc:  87%] [G loss: 5.476271, adv: 0.908887, recon: 0.167987, id: 0.184333] time: 0:03:06.938904 \n",
      "[Epoch 2/300] [Batch 206/292] [D loss: 0.142360, acc:  80%] [G loss: 5.179721, adv: 0.880277, recon: 0.156103, id: 0.130266] time: 0:03:07.147367 \n",
      "[Epoch 2/300] [Batch 207/292] [D loss: 0.091971, acc:  92%] [G loss: 4.824979, adv: 0.815225, recon: 0.145787, id: 0.147067] time: 0:03:07.354917 \n",
      "[Epoch 2/300] [Batch 208/292] [D loss: 0.197570, acc:  69%] [G loss: 4.656896, adv: 0.814936, recon: 0.139022, id: 0.129695] time: 0:03:07.552659 \n",
      "[Epoch 2/300] [Batch 209/292] [D loss: 0.178838, acc:  78%] [G loss: 5.089075, adv: 1.076373, recon: 0.134937, id: 0.121217] time: 0:03:07.751057 \n",
      "[Epoch 2/300] [Batch 210/292] [D loss: 0.227523, acc:  71%] [G loss: 5.070976, adv: 0.898320, recon: 0.150430, id: 0.166451] time: 0:03:07.934920 \n",
      "[Epoch 2/300] [Batch 211/292] [D loss: 0.115317, acc:  86%] [G loss: 4.144841, adv: 0.581976, recon: 0.137490, id: 0.115786] time: 0:03:08.119920 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2/300] [Batch 212/292] [D loss: 0.130363, acc:  84%] [G loss: 4.633904, adv: 0.907771, recon: 0.130950, id: 0.111721] time: 0:03:08.331237 \n",
      "[Epoch 2/300] [Batch 213/292] [D loss: 0.112984, acc:  90%] [G loss: 4.456904, adv: 0.876142, recon: 0.123854, id: 0.111681] time: 0:03:08.532582 \n",
      "[Epoch 2/300] [Batch 214/292] [D loss: 0.133443, acc:  80%] [G loss: 4.874435, adv: 0.966591, recon: 0.135075, id: 0.143887] time: 0:03:08.746752 \n",
      "[Epoch 2/300] [Batch 215/292] [D loss: 0.147203, acc:  79%] [G loss: 5.207821, adv: 0.946342, recon: 0.152680, id: 0.148629] time: 0:03:08.962707 \n",
      "[Epoch 2/300] [Batch 216/292] [D loss: 0.182506, acc:  72%] [G loss: 4.809268, adv: 0.909113, recon: 0.139210, id: 0.108301] time: 0:03:09.144771 \n",
      "[Epoch 2/300] [Batch 217/292] [D loss: 0.067411, acc:  93%] [G loss: 4.993320, adv: 0.934645, recon: 0.143011, id: 0.132119] time: 0:03:09.321746 \n",
      "[Epoch 2/300] [Batch 218/292] [D loss: 0.141712, acc:  80%] [G loss: 6.549908, adv: 1.118944, recon: 0.202824, id: 0.133396] time: 0:03:09.490413 \n",
      "[Epoch 2/300] [Batch 219/292] [D loss: 0.091736, acc:  88%] [G loss: 6.604091, adv: 0.966114, recon: 0.213652, id: 0.122360] time: 0:03:09.665155 \n",
      "[Epoch 2/300] [Batch 220/292] [D loss: 0.108914, acc:  85%] [G loss: 5.779442, adv: 0.984247, recon: 0.173558, id: 0.120279] time: 0:03:09.861623 \n",
      "[Epoch 2/300] [Batch 221/292] [D loss: 0.099786, acc:  90%] [G loss: 5.488518, adv: 1.012673, recon: 0.157133, id: 0.099924] time: 0:03:10.053979 \n",
      "[Epoch 2/300] [Batch 222/292] [D loss: 0.137069, acc:  80%] [G loss: 5.578375, adv: 1.031240, recon: 0.161375, id: 0.106195] time: 0:03:10.262921 \n",
      "[Epoch 2/300] [Batch 223/292] [D loss: 0.047781, acc:  97%] [G loss: 5.139463, adv: 1.034245, recon: 0.140995, id: 0.141964] time: 0:03:10.474210 \n",
      "[Epoch 2/300] [Batch 224/292] [D loss: 0.073905, acc:  96%] [G loss: 5.120859, adv: 1.053370, recon: 0.138515, id: 0.104367] time: 0:03:10.659995 \n",
      "[Epoch 2/300] [Batch 225/292] [D loss: 0.119933, acc:  84%] [G loss: 5.639233, adv: 1.122726, recon: 0.158204, id: 0.125028] time: 0:03:10.836668 \n",
      "[Epoch 2/300] [Batch 226/292] [D loss: 0.140574, acc:  83%] [G loss: 5.505894, adv: 1.097116, recon: 0.154094, id: 0.102099] time: 0:03:11.020460 \n",
      "[Epoch 2/300] [Batch 227/292] [D loss: 0.079485, acc:  92%] [G loss: 5.006770, adv: 0.856713, recon: 0.152274, id: 0.126878] time: 0:03:11.228995 \n",
      "[Epoch 2/300] [Batch 228/292] [D loss: 0.203251, acc:  71%] [G loss: 19.167706, adv: 7.968268, recon: 0.149579, id: 0.139971] time: 0:03:11.452159 \n",
      "[Epoch 2/300] [Batch 229/292] [D loss: 0.869811, acc:  57%] [G loss: 46.753208, adv: 18.612915, recon: 0.446842, id: 0.141020] time: 0:03:11.684048 \n",
      "[Epoch 2/300] [Batch 230/292] [D loss: 1.068193, acc:  87%] [G loss: 9.348009, adv: 0.872616, recon: 0.348323, id: 0.134119] time: 0:03:11.889049 \n",
      "[Epoch 2/300] [Batch 231/292] [D loss: 0.261832, acc:  64%] [G loss: 8.467448, adv: 0.882069, recon: 0.306578, id: 0.120823] time: 0:03:12.064465 \n",
      "[Epoch 2/300] [Batch 232/292] [D loss: 0.172742, acc:  70%] [G loss: 8.037498, adv: 0.812453, recon: 0.294826, id: 0.116664] time: 0:03:12.269169 \n",
      "[Epoch 2/300] [Batch 233/292] [D loss: 0.149861, acc:  77%] [G loss: 8.289934, adv: 0.867841, recon: 0.301063, id: 0.141722] time: 0:03:12.498427 \n",
      "[Epoch 2/300] [Batch 234/292] [D loss: 0.124889, acc:  83%] [G loss: 7.361587, adv: 0.742164, recon: 0.265586, id: 0.116336] time: 0:03:12.715929 \n",
      "[Epoch 2/300] [Batch 235/292] [D loss: 0.142591, acc:  81%] [G loss: 7.475549, adv: 0.952873, recon: 0.251294, id: 0.140509] time: 0:03:12.929169 \n",
      "[Epoch 2/300] [Batch 236/292] [D loss: 0.127547, acc:  88%] [G loss: 7.399269, adv: 1.002949, recon: 0.244949, id: 0.112187] time: 0:03:13.142367 \n",
      "[Epoch 2/300] [Batch 237/292] [D loss: 0.169941, acc:  79%] [G loss: 7.891228, adv: 1.196960, recon: 0.248203, id: 0.142287] time: 0:03:13.329540 \n",
      "[Epoch 2/300] [Batch 238/292] [D loss: 0.130838, acc:  83%] [G loss: 7.016775, adv: 0.896084, recon: 0.235002, id: 0.154622] time: 0:03:13.536421 \n",
      "[Epoch 2/300] [Batch 239/292] [D loss: 0.153729, acc:  78%] [G loss: 5.845657, adv: 0.561926, recon: 0.211802, id: 0.126423] time: 0:03:13.769071 \n",
      "[Epoch 2/300] [Batch 240/292] [D loss: 0.181266, acc:  78%] [G loss: 6.240633, adv: 0.884090, recon: 0.201189, id: 0.119152] time: 0:03:13.990035 \n",
      "[Epoch 2/300] [Batch 241/292] [D loss: 0.127181, acc:  82%] [G loss: 6.265262, adv: 0.954474, recon: 0.193699, id: 0.129228] time: 0:03:14.204350 \n",
      "[Epoch 2/300] [Batch 242/292] [D loss: 0.241047, acc:  56%] [G loss: 6.210847, adv: 0.910080, recon: 0.195269, id: 0.119944] time: 0:03:14.409272 \n",
      "[Epoch 2/300] [Batch 243/292] [D loss: 0.134167, acc:  79%] [G loss: 6.180412, adv: 0.960875, recon: 0.190879, id: 0.113842] time: 0:03:14.642153 \n",
      "[Epoch 2/300] [Batch 244/292] [D loss: 0.098167, acc:  93%] [G loss: 6.991561, adv: 0.971630, recon: 0.232357, id: 0.105712] time: 0:03:14.851210 \n",
      "[Epoch 2/300] [Batch 245/292] [D loss: 0.193280, acc:  70%] [G loss: 6.816732, adv: 1.123746, recon: 0.205618, id: 0.103760] time: 0:03:15.076878 \n",
      "[Epoch 2/300] [Batch 246/292] [D loss: 0.147657, acc:  75%] [G loss: 6.136802, adv: 0.907405, recon: 0.196878, id: 0.114050] time: 0:03:15.301227 \n",
      "[Epoch 2/300] [Batch 247/292] [D loss: 0.156596, acc:  73%] [G loss: 5.959884, adv: 1.027775, recon: 0.177609, id: 0.107638] time: 0:03:15.496975 \n",
      "[Epoch 2/300] [Batch 248/292] [D loss: 0.229595, acc:  63%] [G loss: 6.865672, adv: 1.039544, recon: 0.218686, id: 0.107522] time: 0:03:15.701860 \n",
      "[Epoch 2/300] [Batch 249/292] [D loss: 0.245879, acc:  70%] [G loss: 6.447548, adv: 0.888705, recon: 0.210175, id: 0.097913] time: 0:03:15.907029 \n",
      "[Epoch 2/300] [Batch 250/292] [D loss: 0.171097, acc:  70%] [G loss: 6.468985, adv: 1.207399, recon: 0.184068, id: 0.106898] time: 0:03:16.097246 \n",
      "[Epoch 2/300] [Batch 251/292] [D loss: 0.159523, acc:  81%] [G loss: 5.968532, adv: 1.027256, recon: 0.178840, id: 0.101502] time: 0:03:16.305863 \n",
      "[Epoch 2/300] [Batch 252/292] [D loss: 0.137829, acc:  83%] [G loss: 6.708733, adv: 0.924430, recon: 0.220632, id: 0.127050] time: 0:03:16.487040 \n",
      "[Epoch 2/300] [Batch 253/292] [D loss: 0.201586, acc:  59%] [G loss: 6.085540, adv: 1.086160, recon: 0.178151, id: 0.113405] time: 0:03:16.692300 \n",
      "[Epoch 2/300] [Batch 254/292] [D loss: 0.103633, acc:  87%] [G loss: 6.524682, adv: 1.031665, recon: 0.200665, id: 0.088970] time: 0:03:16.870598 \n",
      "[Epoch 2/300] [Batch 255/292] [D loss: 0.094444, acc:  91%] [G loss: 7.167803, adv: 1.255880, recon: 0.209596, id: 0.105582] time: 0:03:17.058991 \n",
      "[Epoch 2/300] [Batch 256/292] [D loss: 0.089629, acc:  92%] [G loss: 6.249240, adv: 0.963951, recon: 0.195997, id: 0.103216] time: 0:03:17.244956 \n",
      "[Epoch 2/300] [Batch 257/292] [D loss: 0.088077, acc:  96%] [G loss: 6.233150, adv: 1.240762, recon: 0.171158, id: 0.095921] time: 0:03:17.414033 \n",
      "[Epoch 2/300] [Batch 258/292] [D loss: 0.076201, acc:  95%] [G loss: 7.008498, adv: 1.363236, recon: 0.197981, id: 0.111391] time: 0:03:17.632226 \n",
      "[Epoch 2/300] [Batch 259/292] [D loss: 0.153818, acc:  83%] [G loss: 6.889883, adv: 0.466621, recon: 0.273621, id: 0.116061] time: 0:03:17.835712 \n",
      "[Epoch 2/300] [Batch 260/292] [D loss: 0.127702, acc:  88%] [G loss: 7.111598, adv: 1.139276, recon: 0.221420, id: 0.103307] time: 0:03:18.039614 \n",
      "[Epoch 2/300] [Batch 261/292] [D loss: 0.330922, acc:  54%] [G loss: 6.508513, adv: 1.017304, recon: 0.207985, id: 0.106649] time: 0:03:18.256605 \n",
      "[Epoch 2/300] [Batch 262/292] [D loss: 0.186888, acc:  71%] [G loss: 6.608235, adv: 0.916170, recon: 0.218876, id: 0.105477] time: 0:03:18.482923 \n",
      "[Epoch 2/300] [Batch 263/292] [D loss: 0.074077, acc:  95%] [G loss: 7.436903, adv: 1.045912, recon: 0.243120, id: 0.134060] time: 0:03:18.706846 \n",
      "[Epoch 2/300] [Batch 264/292] [D loss: 0.270003, acc:  51%] [G loss: 6.169724, adv: 1.021601, recon: 0.189664, id: 0.095726] time: 0:03:18.902883 \n",
      "[Epoch 2/300] [Batch 265/292] [D loss: 0.070885, acc:  94%] [G loss: 6.899660, adv: 0.873751, recon: 0.231338, id: 0.107447] time: 0:03:19.077043 \n",
      "[Epoch 2/300] [Batch 266/292] [D loss: 0.143853, acc:  82%] [G loss: 6.796411, adv: 1.109605, recon: 0.197842, id: 0.390108] time: 0:03:19.284033 \n",
      "[Epoch 2/300] [Batch 267/292] [D loss: 0.172515, acc:  75%] [G loss: 6.629234, adv: 1.131884, recon: 0.200885, id: 0.106623] time: 0:03:19.479967 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2/300] [Batch 268/292] [D loss: 0.156978, acc:  81%] [G loss: 6.806736, adv: 1.205774, recon: 0.204027, id: 0.112755] time: 0:03:19.688150 \n",
      "[Epoch 2/300] [Batch 269/292] [D loss: 0.354360, acc:  41%] [G loss: 6.669388, adv: 1.360474, recon: 0.183385, id: 0.116009] time: 0:03:19.874045 \n",
      "[Epoch 2/300] [Batch 270/292] [D loss: 0.197717, acc:  68%] [G loss: 6.588047, adv: 1.012542, recon: 0.212729, id: 0.105189] time: 0:03:20.070064 \n",
      "[Epoch 2/300] [Batch 271/292] [D loss: 0.196290, acc:  67%] [G loss: 8.605644, adv: 1.056300, recon: 0.300489, id: 0.168751] time: 0:03:20.274280 \n",
      "[Epoch 2/300] [Batch 272/292] [D loss: 0.080059, acc:  96%] [G loss: 7.839763, adv: 1.064217, recon: 0.266924, id: 0.124561] time: 0:03:20.466064 \n",
      "[Epoch 2/300] [Batch 273/292] [D loss: 0.098088, acc:  92%] [G loss: 7.349564, adv: 1.208187, recon: 0.230507, id: 0.113122] time: 0:03:20.685479 \n",
      "[Epoch 2/300] [Batch 274/292] [D loss: 0.040904, acc:  99%] [G loss: 6.931739, adv: 0.970107, recon: 0.232638, id: 0.125354] time: 0:03:20.909810 \n",
      "[Epoch 2/300] [Batch 275/292] [D loss: 0.028285, acc:  99%] [G loss: 6.151285, adv: 0.979408, recon: 0.195691, id: 0.122435] time: 0:03:21.110831 \n",
      "[Epoch 2/300] [Batch 276/292] [D loss: 0.074756, acc:  95%] [G loss: 6.682555, adv: 1.311804, recon: 0.190647, id: 0.098278] time: 0:03:21.301728 \n",
      "[Epoch 2/300] [Batch 277/292] [D loss: 0.447410, acc:  57%] [G loss: 6.252969, adv: 1.037472, recon: 0.195729, id: 0.105421] time: 0:03:21.480467 \n",
      "[Epoch 2/300] [Batch 278/292] [D loss: 0.124159, acc:  87%] [G loss: 6.477635, adv: 1.020321, recon: 0.200535, id: 0.112383] time: 0:03:21.659848 \n",
      "[Epoch 2/300] [Batch 279/292] [D loss: 0.296508, acc:  45%] [G loss: 9.555502, adv: 1.251048, recon: 0.336692, id: 0.110483] time: 0:03:21.830112 \n",
      "[Epoch 2/300] [Batch 280/292] [D loss: 0.102590, acc:  91%] [G loss: 7.371466, adv: 1.054062, recon: 0.245406, id: 0.099351] time: 0:03:22.044848 \n",
      "[Epoch 2/300] [Batch 281/292] [D loss: 0.070993, acc:  94%] [G loss: 6.984034, adv: 0.889441, recon: 0.242747, id: 0.143054] time: 0:03:22.246369 \n",
      "[Epoch 2/300] [Batch 282/292] [D loss: 0.044675, acc:  98%] [G loss: 6.658141, adv: 1.007449, recon: 0.212662, id: 0.159943] time: 0:03:22.459756 \n",
      "[Epoch 2/300] [Batch 283/292] [D loss: 0.056056, acc:  98%] [G loss: 6.366377, adv: 1.010128, recon: 0.200680, id: 0.121064] time: 0:03:22.666128 \n",
      "[Epoch 2/300] [Batch 284/292] [D loss: 0.090472, acc:  90%] [G loss: 6.014565, adv: 1.112826, recon: 0.173897, id: 0.131938] time: 0:03:22.905460 \n",
      "[Epoch 2/300] [Batch 285/292] [D loss: 0.059352, acc:  98%] [G loss: 6.892078, adv: 1.080879, recon: 0.217892, id: 0.090950] time: 0:03:23.116587 \n",
      "[Epoch 2/300] [Batch 286/292] [D loss: 0.327299, acc:  53%] [G loss: 5.536750, adv: 0.852573, recon: 0.175978, id: 0.136338] time: 0:03:23.320629 \n",
      "[Epoch 2/300] [Batch 287/292] [D loss: 0.166204, acc:  71%] [G loss: 5.480339, adv: 0.851415, recon: 0.175477, id: 0.116759] time: 0:03:23.510564 \n",
      "[Epoch 2/300] [Batch 288/292] [D loss: 0.156385, acc:  78%] [G loss: 5.041197, adv: 0.903944, recon: 0.148055, id: 0.113880] time: 0:03:23.681456 \n",
      "[Epoch 2/300] [Batch 289/292] [D loss: 0.159075, acc:  70%] [G loss: 4.670543, adv: 0.788828, recon: 0.142034, id: 0.107784] time: 0:03:23.862959 \n",
      "[Epoch 2/300] [Batch 290/292] [D loss: 0.148013, acc:  75%] [G loss: 4.984302, adv: 0.792033, recon: 0.157895, id: 0.101465] time: 0:03:24.070529 \n",
      "[Epoch 3/300] [Batch 0/292] [D loss: 0.060418, acc:  98%] [G loss: 5.753561, adv: 0.871480, recon: 0.182772, id: 0.106888] time: 0:03:24.290932 \n",
      "[Epoch 3/300] [Batch 1/292] [D loss: 0.156409, acc:  71%] [G loss: 5.120461, adv: 0.824513, recon: 0.157507, id: 0.108868] time: 0:03:24.779670 \n",
      "[Epoch 3/300] [Batch 2/292] [D loss: 0.165833, acc:  67%] [G loss: 5.316391, adv: 1.009098, recon: 0.152426, id: 0.102932] time: 0:03:24.989656 \n",
      "[Epoch 3/300] [Batch 3/292] [D loss: 0.107486, acc:  86%] [G loss: 6.137175, adv: 1.039058, recon: 0.186695, id: 0.095230] time: 0:03:25.222721 \n",
      "[Epoch 3/300] [Batch 4/292] [D loss: 0.107090, acc:  91%] [G loss: 5.977889, adv: 1.003986, recon: 0.184418, id: 0.107756] time: 0:03:25.439245 \n",
      "[Epoch 3/300] [Batch 5/292] [D loss: 0.069615, acc:  98%] [G loss: 5.716154, adv: 0.933249, recon: 0.176784, id: 0.110501] time: 0:03:25.633840 \n",
      "[Epoch 3/300] [Batch 6/292] [D loss: 0.102151, acc:  90%] [G loss: 5.478479, adv: 1.012961, recon: 0.158371, id: 0.113358] time: 0:03:25.837141 \n",
      "[Epoch 3/300] [Batch 7/292] [D loss: 0.106894, acc:  90%] [G loss: 5.133361, adv: 0.973514, recon: 0.147213, id: 0.098224] time: 0:03:26.046953 \n",
      "[Epoch 3/300] [Batch 8/292] [D loss: 0.211127, acc:  71%] [G loss: 5.514399, adv: 1.063748, recon: 0.156131, id: 0.115808] time: 0:03:26.249250 \n",
      "[Epoch 3/300] [Batch 9/292] [D loss: 0.202005, acc:  68%] [G loss: 5.873705, adv: 1.226809, recon: 0.158207, id: 0.103703] time: 0:03:26.457005 \n",
      "[Epoch 3/300] [Batch 10/292] [D loss: 0.090968, acc:  94%] [G loss: 5.624250, adv: 1.096300, recon: 0.158170, id: 0.106673] time: 0:03:26.650571 \n",
      "[Epoch 3/300] [Batch 11/292] [D loss: 0.107682, acc:  88%] [G loss: 5.854408, adv: 1.277690, recon: 0.152482, id: 0.118923] time: 0:03:26.867066 \n",
      "[Epoch 3/300] [Batch 12/292] [D loss: 0.053795, acc:  97%] [G loss: 6.747288, adv: 1.081357, recon: 0.213197, id: 0.097053] time: 0:03:27.075341 \n",
      "[Epoch 3/300] [Batch 13/292] [D loss: 0.112511, acc:  86%] [G loss: 7.081326, adv: 0.885340, recon: 0.245645, id: 0.110547] time: 0:03:27.270041 \n",
      "[Epoch 3/300] [Batch 14/292] [D loss: 0.073599, acc:  92%] [G loss: 6.309627, adv: 0.974161, recon: 0.198660, id: 0.096703] time: 0:03:27.462219 \n",
      "[Epoch 3/300] [Batch 15/292] [D loss: 0.107966, acc:  82%] [G loss: 5.944533, adv: 0.881010, recon: 0.190380, id: 0.123762] time: 0:03:27.667495 \n",
      "[Epoch 3/300] [Batch 16/292] [D loss: 0.107232, acc:  92%] [G loss: 5.287394, adv: 0.861151, recon: 0.164664, id: 0.102189] time: 0:03:27.864881 \n",
      "[Epoch 3/300] [Batch 17/292] [D loss: 0.080035, acc:  93%] [G loss: 4.603930, adv: 0.899889, recon: 0.127683, id: 0.101745] time: 0:03:28.070356 \n",
      "[Epoch 3/300] [Batch 18/292] [D loss: 0.062299, acc:  96%] [G loss: 5.129683, adv: 0.931623, recon: 0.147899, id: 0.096579] time: 0:03:28.265436 \n",
      "[Epoch 3/300] [Batch 19/292] [D loss: 0.108804, acc:  86%] [G loss: 7.579165, adv: 1.208498, recon: 0.242876, id: 0.115709] time: 0:03:28.474639 \n",
      "[Epoch 3/300] [Batch 20/292] [D loss: 0.279134, acc:  66%] [G loss: 7.395567, adv: 0.835153, recon: 0.264420, id: 0.116877] time: 0:03:28.692048 \n",
      "[Epoch 3/300] [Batch 21/292] [D loss: 0.171181, acc:  74%] [G loss: 6.420998, adv: 0.817352, recon: 0.216345, id: 0.099400] time: 0:03:28.880159 \n",
      "[Epoch 3/300] [Batch 22/292] [D loss: 0.083280, acc:  93%] [G loss: 6.333365, adv: 0.863835, recon: 0.215309, id: 0.090821] time: 0:03:29.073940 \n",
      "[Epoch 3/300] [Batch 23/292] [D loss: 0.081425, acc:  94%] [G loss: 6.349424, adv: 0.925558, recon: 0.208393, id: 0.095052] time: 0:03:29.302318 \n",
      "[Epoch 3/300] [Batch 24/292] [D loss: 0.092040, acc:  92%] [G loss: 6.716871, adv: 1.091133, recon: 0.212044, id: 0.115706] time: 0:03:29.507632 \n",
      "[Epoch 3/300] [Batch 25/292] [D loss: 0.068895, acc:  95%] [G loss: 6.176162, adv: 0.996799, recon: 0.195687, id: 0.088607] time: 0:03:29.683752 \n",
      "[Epoch 3/300] [Batch 26/292] [D loss: 0.093176, acc:  93%] [G loss: 5.870469, adv: 1.008933, recon: 0.179502, id: 0.097596] time: 0:03:29.883623 \n",
      "[Epoch 3/300] [Batch 27/292] [D loss: 0.086336, acc:  90%] [G loss: 6.147638, adv: 0.881891, recon: 0.202142, id: 0.096817] time: 0:03:30.085953 \n",
      "[Epoch 3/300] [Batch 28/292] [D loss: 0.040073, acc:  98%] [G loss: 6.475833, adv: 1.106893, recon: 0.193769, id: 0.113630] time: 0:03:30.314545 \n",
      "[Epoch 3/300] [Batch 29/292] [D loss: 0.146252, acc:  83%] [G loss: 6.029493, adv: 1.275372, recon: 0.161289, id: 0.088749] time: 0:03:30.494087 \n",
      "[Epoch 3/300] [Batch 30/292] [D loss: 0.105741, acc:  89%] [G loss: 5.201787, adv: 0.842805, recon: 0.158814, id: 0.111763] time: 0:03:30.667815 \n",
      "[Epoch 3/300] [Batch 31/292] [D loss: 0.265159, acc:  55%] [G loss: 5.173397, adv: 0.917744, recon: 0.153953, id: 0.125746] time: 0:03:30.836277 \n",
      "[Epoch 3/300] [Batch 32/292] [D loss: 0.164039, acc:  76%] [G loss: 5.021708, adv: 0.880322, recon: 0.148984, id: 0.098988] time: 0:03:31.049082 \n",
      "[Epoch 3/300] [Batch 33/292] [D loss: 0.140976, acc:  80%] [G loss: 4.891424, adv: 0.957398, recon: 0.134089, id: 0.100992] time: 0:03:31.257237 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3/300] [Batch 34/292] [D loss: 0.122007, acc:  85%] [G loss: 4.818871, adv: 1.040717, recon: 0.122991, id: 0.092401] time: 0:03:31.481903 \n",
      "[Epoch 3/300] [Batch 35/292] [D loss: 0.139630, acc:  87%] [G loss: 4.611680, adv: 0.592383, recon: 0.156746, id: 0.097439] time: 0:03:31.680985 \n",
      "[Epoch 3/300] [Batch 36/292] [D loss: 0.254604, acc:  72%] [G loss: 6.351595, adv: 1.182566, recon: 0.185038, id: 0.096255] time: 0:03:31.877066 \n",
      "[Epoch 3/300] [Batch 37/292] [D loss: 0.097803, acc:  90%] [G loss: 5.421885, adv: 0.887528, recon: 0.166097, id: 0.111539] time: 0:03:32.049048 \n",
      "[Epoch 3/300] [Batch 38/292] [D loss: 0.292439, acc:  56%] [G loss: 4.495502, adv: 0.733008, recon: 0.139328, id: 0.092627] time: 0:03:32.257798 \n",
      "[Epoch 3/300] [Batch 39/292] [D loss: 0.157023, acc:  75%] [G loss: 4.863860, adv: 0.759338, recon: 0.152584, id: 0.126937] time: 0:03:32.464306 \n",
      "[Epoch 3/300] [Batch 40/292] [D loss: 0.196226, acc:  64%] [G loss: 5.383166, adv: 0.891166, recon: 0.166935, id: 0.097113] time: 0:03:32.676964 \n",
      "[Epoch 3/300] [Batch 41/292] [D loss: 0.217123, acc:  62%] [G loss: 4.849368, adv: 0.785619, recon: 0.149025, id: 0.112679] time: 0:03:32.883929 \n",
      "[Epoch 3/300] [Batch 42/292] [D loss: 0.163940, acc:  70%] [G loss: 4.909517, adv: 0.739661, recon: 0.159663, id: 0.087009] time: 0:03:33.091585 \n",
      "[Epoch 3/300] [Batch 43/292] [D loss: 0.074875, acc:  96%] [G loss: 4.987167, adv: 0.802109, recon: 0.153475, id: 0.101994] time: 0:03:33.309728 \n",
      "[Epoch 3/300] [Batch 44/292] [D loss: 0.159839, acc:  74%] [G loss: 5.365670, adv: 0.787524, recon: 0.179422, id: 0.105418] time: 0:03:33.501786 \n",
      "[Epoch 3/300] [Batch 45/292] [D loss: 0.113934, acc:  87%] [G loss: 4.849785, adv: 0.846096, recon: 0.142993, id: 0.105794] time: 0:03:33.699152 \n",
      "[Epoch 3/300] [Batch 46/292] [D loss: 0.123581, acc:  82%] [G loss: 4.985039, adv: 0.955853, recon: 0.142913, id: 0.092822] time: 0:03:33.937112 \n",
      "[Epoch 3/300] [Batch 47/292] [D loss: 0.148760, acc:  77%] [G loss: 4.572810, adv: 0.893440, recon: 0.127852, id: 0.079812] time: 0:03:34.149515 \n",
      "[Epoch 3/300] [Batch 48/292] [D loss: 0.203093, acc:  68%] [G loss: 4.974645, adv: 0.884781, recon: 0.147921, id: 0.106317] time: 0:03:34.331505 \n",
      "[Epoch 3/300] [Batch 49/292] [D loss: 0.123979, acc:  84%] [G loss: 4.168785, adv: 0.589541, recon: 0.138772, id: 0.097295] time: 0:03:34.510552 \n",
      "[Epoch 3/300] [Batch 50/292] [D loss: 0.188717, acc:  70%] [G loss: 4.756390, adv: 0.812501, recon: 0.142487, id: 0.117477] time: 0:03:34.721072 \n",
      "[Epoch 3/300] [Batch 51/292] [D loss: 0.114639, acc:  89%] [G loss: 4.948988, adv: 0.883093, recon: 0.146321, id: 0.094327] time: 0:03:34.918372 \n",
      "[Epoch 3/300] [Batch 52/292] [D loss: 0.152475, acc:  81%] [G loss: 4.591472, adv: 0.844742, recon: 0.133153, id: 0.102191] time: 0:03:35.125616 \n",
      "[Epoch 3/300] [Batch 53/292] [D loss: 0.096079, acc:  94%] [G loss: 4.450358, adv: 0.897524, recon: 0.121743, id: 0.086856] time: 0:03:35.335260 \n",
      "[Epoch 3/300] [Batch 54/292] [D loss: 0.264549, acc:  57%] [G loss: 5.127121, adv: 0.869929, recon: 0.158829, id: 0.093266] time: 0:03:35.552935 \n",
      "[Epoch 3/300] [Batch 55/292] [D loss: 0.081298, acc:  93%] [G loss: 5.730324, adv: 0.853476, recon: 0.186096, id: 0.083462] time: 0:03:35.765998 \n",
      "[Epoch 3/300] [Batch 56/292] [D loss: 0.074289, acc:  94%] [G loss: 5.115202, adv: 0.938351, recon: 0.148278, id: 0.089596] time: 0:03:35.968453 \n",
      "[Epoch 3/300] [Batch 57/292] [D loss: 0.155675, acc:  77%] [G loss: 4.639303, adv: 0.921092, recon: 0.127949, id: 0.099655] time: 0:03:36.166999 \n",
      "[Epoch 3/300] [Batch 58/292] [D loss: 0.206226, acc:  67%] [G loss: 4.970336, adv: 0.994607, recon: 0.138211, id: 0.097497] time: 0:03:36.380466 \n",
      "[Epoch 3/300] [Batch 59/292] [D loss: 0.184711, acc:  74%] [G loss: 5.014148, adv: 0.976213, recon: 0.139703, id: 0.102502] time: 0:03:36.586508 \n",
      "[Epoch 3/300] [Batch 60/292] [D loss: 0.216516, acc:  65%] [G loss: 4.881127, adv: 0.782882, recon: 0.153991, id: 0.102872] time: 0:03:36.789072 \n",
      "[Epoch 3/300] [Batch 61/292] [D loss: 0.103024, acc:  91%] [G loss: 4.530760, adv: 0.883924, recon: 0.127069, id: 0.099153] time: 0:03:37.014219 \n",
      "[Epoch 3/300] [Batch 62/292] [D loss: 0.059845, acc:  96%] [G loss: 4.601285, adv: 0.814155, recon: 0.135316, id: 0.086510] time: 0:03:37.226073 \n",
      "[Epoch 3/300] [Batch 63/292] [D loss: 0.138840, acc:  85%] [G loss: 5.029098, adv: 1.084759, recon: 0.128233, id: 0.108318] time: 0:03:37.434467 \n",
      "[Epoch 3/300] [Batch 64/292] [D loss: 0.282905, acc:  71%] [G loss: 4.899311, adv: 0.890757, recon: 0.143063, id: 0.100935] time: 0:03:37.636116 \n",
      "[Epoch 3/300] [Batch 65/292] [D loss: 0.135193, acc:  82%] [G loss: 4.544345, adv: 0.892231, recon: 0.126727, id: 0.098895] time: 0:03:37.824667 \n",
      "[Epoch 3/300] [Batch 66/292] [D loss: 0.111648, acc:  88%] [G loss: 4.202915, adv: 0.798375, recon: 0.116141, id: 0.116930] time: 0:03:38.011648 \n",
      "[Epoch 3/300] [Batch 67/292] [D loss: 0.156818, acc:  74%] [G loss: 4.579579, adv: 0.785487, recon: 0.139939, id: 0.091778] time: 0:03:38.210030 \n",
      "[Epoch 3/300] [Batch 68/292] [D loss: 0.068667, acc:  96%] [G loss: 4.661528, adv: 0.712154, recon: 0.146086, id: 0.089716] time: 0:03:38.410064 \n",
      "[Epoch 3/300] [Batch 69/292] [D loss: 0.174504, acc:  72%] [G loss: 4.765633, adv: 0.860826, recon: 0.140917, id: 0.098327] time: 0:03:38.582023 \n",
      "[Epoch 3/300] [Batch 70/292] [D loss: 0.169654, acc:  78%] [G loss: 4.454458, adv: 0.835045, recon: 0.126721, id: 0.101439] time: 0:03:38.759912 \n",
      "[Epoch 3/300] [Batch 71/292] [D loss: 0.082418, acc:  93%] [G loss: 4.492564, adv: 0.904040, recon: 0.123755, id: 0.092331] time: 0:03:38.975420 \n",
      "[Epoch 3/300] [Batch 72/292] [D loss: 0.112273, acc:  84%] [G loss: 4.653901, adv: 0.977781, recon: 0.121577, id: 0.095211] time: 0:03:39.168563 \n",
      "[Epoch 3/300] [Batch 73/292] [D loss: 0.071252, acc:  94%] [G loss: 4.740366, adv: 0.907823, recon: 0.133103, id: 0.095097] time: 0:03:39.377440 \n",
      "[Epoch 3/300] [Batch 74/292] [D loss: 0.036063, acc:  99%] [G loss: 4.974283, adv: 0.970813, recon: 0.137653, id: 0.097559] time: 0:03:39.585060 \n",
      "[Epoch 3/300] [Batch 75/292] [D loss: 0.064578, acc:  96%] [G loss: 4.708946, adv: 0.980635, recon: 0.123050, id: 0.087464] time: 0:03:39.797033 \n",
      "[Epoch 3/300] [Batch 76/292] [D loss: 0.152295, acc:  77%] [G loss: 4.742528, adv: 0.912938, recon: 0.132589, id: 0.102124] time: 0:03:40.020347 \n",
      "[Epoch 3/300] [Batch 77/292] [D loss: 0.154961, acc:  77%] [G loss: 4.369260, adv: 0.853139, recon: 0.120942, id: 0.097701] time: 0:03:40.230641 \n",
      "[Epoch 3/300] [Batch 78/292] [D loss: 0.104972, acc:  88%] [G loss: 5.075766, adv: 1.059753, recon: 0.135098, id: 0.097347] time: 0:03:40.439102 \n",
      "[Epoch 3/300] [Batch 79/292] [D loss: 0.285373, acc:  58%] [G loss: 6.090299, adv: 1.081821, recon: 0.185021, id: 0.093096] time: 0:03:40.636436 \n",
      "[Epoch 3/300] [Batch 80/292] [D loss: 0.184103, acc:  69%] [G loss: 5.203218, adv: 0.781819, recon: 0.164913, id: 0.188852] time: 0:03:40.844701 \n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    gan = CycleGAN()\n",
    "    gan.train(epochs=300, batch_size=1, sample_interval=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nvidia/anaconda3/lib/python3.6/site-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n",
      "/home/nvidia/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "g_AB = load_model(\"gen_model/299_200_model.h5\",custom_objects={'InstanceNormalization':InstanceNormalization})\n",
    "file_dir = \"PatchImage\"\n",
    "path = \"gen_img/\"\n",
    "\n",
    "def imread(path):\n",
    "    return scipy.misc.imread(path, mode='RGB').astype(np.float)\n",
    "\n",
    "for root, dirs, files in os.walk(file_dir): \n",
    "    for file in files:\n",
    "        patch_a = []\n",
    "        file_path = os.path.join(root, file)\n",
    "        img = imread(file_path)\n",
    "        patch_a.append(img)\n",
    "        final_img = np.array(patch_a)/127.5 - 1.\n",
    "        gen_A = g_AB.predict(final_img)\n",
    "        gen_pic_A = image.array_to_img((gen_A[0]+1) * 127.5, scale=False)\n",
    "        gen_pic_A.save(path+file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
